%\documentclass{article}
\documentclass[3p,computermodern,10pt]{elsarticle}
\input{packages}
\input{commands}
\input{commands_cwst}
%\input{title}
%\begin{frontmatter}
%\title{Continuous Least-Squares Petrov-Galerkin for Model Reduction}
%\author{Eric J. Parish}
\begin{document}
\begin{frontmatter}

%\title{Model Reduction via \methodName}
%\title{Windowed least--squares reduced-order models for dynamical systems}
\title{Windowed least-squares model reduction for dynamical systems}
%\title{The windowed least-squares framework for model reduction of dynamical systems}

\author[a]{Eric J. Parish and Kevin T. Carlberg}
%\ead{ejparis@sandia.gov}

\address[a]{Sandia National Laboratories,  Livermore, CA}
\begin{abstract}
This work proposes a \methodNameLower\ (\methodAcronym) approach for model reduction
	of dynamical systems. The proposed approach sequentially minimizes the
	time-continuous full-order-model residual within a low-dimensional space--time trial
	subspace over time windows. The approach comprises a generalization 
of existing model reduction approaches, as particular instances of
  the methodology recover Galerkin,
	least-squares Petrov--Galerkin (LSPG), and space–time LSPG projection. In
	addition, the approach
	addresses key deficiencies in existing model reduction
	techniques, e.g., the dependence of LSPG and space--time LSPG projection on the
	time discretization and the exponential growth in time exhibited by \textit{a posteriori}
	error bounds for both Galerkin and LSPG projection.  We consider two types of
	space--time trial
	subspaces within the proposed approach: one that reduces only the spatial dimension of the full-order model, and one that reduces both the spatial and temporal dimensions of the full-order model. For each type of trial
	subspace, we consider two different solution techniques: direct (i.e.,
	discretize then optimize) and indirect (i.e., optimize then discretize).
	Numerical experiments conducted using trial subspaces characterized by spatial
	dimension reduction demonstrate that the \methodAcronym\
	approach can yield more accurate solutions with lower space--time residuals than
	Galerkin and LSPG projection. 

\end{abstract}
\end{frontmatter}


%\maketitle
\section{Introduction}

Simulating parameterized dynamical systems arises in many applications across
science and engineering. In many contexts, executing a dynamical-system
simulation at a single parameter instance---which entails the numerical
integration of a system of ordinary differential equations (ODEs)---incurs an
extremely large computational cost.  This occurs, for example, when the
state-space dimension is large (e.g., due to fine spatial resolution when
discretizing a partial differential equation) and/or when the number of time
instances is large (e.g., due to time-step limitations incurred by stability
or accuracy considerations).  When the application is time critical or many
query in nature, analysts must replace such large-scale parameterized
dynamical-system models (which we refer to as the full-order model) with a low-cost approximation that makes the
application tractable.


Projection-based reduced-order models (ROMs) comprise one such approximation
strategy. First, these techniques execute a computationally expensive
\textit{offline} stage that computes a low-dimensional \textit{trial subspace} on
which the dynamical-system state can be well approximated (e.g., by computing
state ``snapshots'' at different time and parameter instances, by solving
Lyapunov equations). Second, these methods execute an inexpensive
\textit{online} stage during which they compute approximations to the
dynamical-system trajectory that reside on this trial subspace via, e.g., 
projection of the full-order model or residual minimization. 
 
Model reduction for linear-time-invariant systems (and other well structured
dynamical systems) is quite mature
\cite{wilcox_benner_rev,moore,roberts,GugercinIRKA}, as system-theoretic properties (e.g., controllability,
observability, asymptotic stability, $\mathcal H_2$-optimality) can be readily
quantified and accounted for; this often results in
reduced-order models that inherit such important properties.  The primary
challenge in
developing reduced-order models for general nonlinear dynamical systems is
that such properties are difficult to assess quantitatively. As a result, it
is challenging to develop reduced-order models that preserve important
dynamical-system properties, which often results in methods that yield
trajectories that are
inaccurate, unstable, or violate physical properties.  To address this, researchers have pursued several directions that
aim to imbue reduced-order models for nonlinear
dynamical systems with properties that can improve robustness and accuracy.
These
efforts include residual-minimization approaches that equip the ROM solution with a notion of optimality~\cite{carlberg_lspg,carlberg_gnat,legresley_1,legresley_2,legresley_3,bui_resmin_steady,bui_unsteady,rovas_thesis,carlberg_thesis,bui_thesis,l1};
space--time approaches that lead to error bounds that grow slowly in time~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC,benner_st};
``energy-based'' inner
products that ensure non-increasing entropy in the ROM
solution~\cite{rowley_pod_energyproj,Kalashnikova_sand2014,chan2019entropy};
basis-adaptation methods that improve the ROM's accuracy \textit{a
posteriori}~\cite{carlberg_hadaptation,adeim_peherstorfer,etter2019online}, stabilizing subspace rotations that account for truncated modes \textit{a priori}~\cite{basis_rotation}, structure-preserving
methods that enforce conservation \cite{carlberg_conservative_rom} or
(port-)Hamiltonian/Lagrangian structure
\cite{LALL2003304,carlberg2012spd,structurePreserveBeattie,chaturantabut2016structure,farhat2014dimensional} in the ROM; and subgrid-scale
modeling methods that aim to improve accuracy by addressing the closure
problem~\cite{san_iliescu_geostrophic,iliescu_pod_eddyviscosity,iliescu_vms_pod_ns,Bergmann_pod_vms,iliescu_ciazzo_residual_rom,Wang_ROM_thesis,wentland_apg,Wang:269133,San2018}.
We note that these techniques are often not mutually exclusive.
%Residual-minimizing projection methods are the most relevant to the current work and comprise the focus of the following review.
Residual-minimizing and space--time approaches are the most relevant classes of
methods for the current work and comprise the focus of the following review.


Residual-minimization methods in model reduction compute the solution within a
low-dimensional trial subspace that minimizes the full-order-model residual.\footnote{While we focus our review on residual-minimization approaches in the context of model reduction, we note that these approaches are intimately related to least-squares finite element methods~\cite{bochev_leastsquares,bochev_lsfem_book}.}
Researchers have developed such residual-minimizing model reduction methods
for both static systems (i.e., systems without time-dependence)
\cite{legresley_1,legresley_2,legresley_3,bui_resmin_steady,rovas_thesis,carlberg_thesis,bui_thesis}
and dynamical
systems~\cite{bui_thesis,bui_unsteady,carlberg_thesis,carlberg_gnat,carlberg_lspg,carlberg_lspg_v_galerkin}.
In the latter category,
Refs.~\cite{bui_thesis,bui_unsteady,carlberg_thesis,carlberg_gnat,carlberg_lspg}
formulated the residual minimization problem for dynamical systems by
sequentially minimizing the \textit{time-discrete} full-order-model residual
(i.e., the residual arising after applying time discretization) at each time
instance on the time-discretization grid. This formulation is often referred
to as the \textit{least--squares Petrov--Galerkin} (LSPG) method.
Ref.~\cite{carlberg_lspg_v_galerkin} performed detailed analyses of this
formulation and examined its connections with Galerkin projection. Critically,
this work demonstrated that (1) under certain conditions, LSPG can be cast as a 
Petrov--Galerkin projection applied to the time-continuous
full-order-model residual, and (2) LSPG and Galerkin projection are equivalent
in the limit as the time step goes to zero (i.e., Galerkin projection 
minimizes the time-instantaneous full-order-model residual). Numerous
numerical experiments have demonstrated that LSPG often yields more
accurate and stable solutions than Galerkin~\cite{bui_thesis,carlberg_lspg_v_galerkin,carlberg_gnat,carlberg_thesis,parish_apg}.
The common intuitive explanation for this improved performance is that, by
minimizing the full-order-model residual over a finite time window (rather
than time instantaneously), LSPG computes solutions that are more
accurate over a larger part of the trajectory as compared to Galerkin.

However, LSPG has several notable shortcomings. First, LSPG
exhibits a complex dependence on the time discretization. In
particular, changing
the time step ($\Delta t$) modifies both the
time window over which LSPG minimizes the residual as well as the
time-discretization error of the full-order model on which LSPG is based. 
As LSPG and Galerkin projection are equivalent in the
limit of $\Delta t \rightarrow 0$, the accuracy of LSPG approaches the (sometimes poor)
accuracy of Galerkin as the time step shrinks.
For too-large a time step the accuracy of LSPG also degrades. It is unclear if this is due to the time-discretization error associated with enlarging the time step, or rather if it is due to the size of the window the residual is being minimized over.
As a
consequence, LSPG often yields the smallest error for an intermediate value of the
time step (see, e.g., Ref.~\cite[Figure 9]{carlberg_lspg_v_galerkin}); there
is no known way to compute this optimal time step \textit{a priori}.
Second, as the LSPG approach performs sequential residual minimization in
time, its \textit{a posteriori} error bounds grow exponentially in time
\cite{carlberg_lspg_v_galerkin}, and it is not equipped with any notion of
optimality over the entire time domain of interest. As a result, LSPG is not
equipped with \textit{a priori} guarantees of accuracy or stability, even for
linear time-invariant systems~\cite{bui_thesis}.

Researchers have pursued the development of space--time residual-minimization approaches~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC} to
address the issues incurred by sequential residual minimization in time.
Existing space--time approaches differ from the classic LSPG and Galerkin approaches in (1) the definition of the space--time trial subspace and (2) the definition of the residual minimization problem. 
First, space--time approaches leverage a space--time trial basis that characterizes both the spatial \textit{and} temporal dependence of the state, classic ``spatial" model reduction approaches such as LSPG and Galerkin leverage only a spatial trial basis that characterizes the spatial dependence of the state. 
%First, space--time approaches leverage a space--time trial 
%subspace that associates with a space--time projection operator; classic ``spatial" model reduction approaches such as LSPG and Galerkin leverage a space--time trial subspace that can be associated with a spatial projection operator only. 
Second, space--time residual minimization approaches compute the entire space--time trajectory of the state (within the low-dimensional space--time trial subspace) that minimizes the full-order-model residual over the entire time domain; Galerkin and LSPG sequentially compute instances of the state that either minimize the full-order model instantaneously (Galerkin) or over a time step (LSPG). 
The result of these differences is that space--time approaches yield a system of algebraic equations defined
over all space and time, whose solution comprises a vector of
(space--time) generalized coordinates; on the other hand, spatial-projection-only approaches 
generally associate with systems of ODEs whose solutions
comprise \textit{time-dependent} vectors of (spatial) generalized
coordinates.

%Space--time residual-minimization approaches compute the space--time
%trajectory within a low-dimensional space--time trial trial space that associates with space--time projection (spanned by
%a small number of space--time basis vectors) that minimizes the full-order-model
%residual over the entire time domain.
%We note that methods leveraging a trial subspace that associates with space--time 
%projection yields a fundamentally different model than methods that leverage a trial subspace that 
%associates with spatial projection only; in particular space--time
%projection associates with an algebraic system of equations defined
%over all space and time, whose solution comprises a \text{fixed} vector of
%(space--time) generalized coordinates; on the other hand, spatial projection only  
%generally associates with a system of ODEs whose solution
%comprises a \textit{time-dependent} vector of (spatial) generalized
%coordinates.
%We note that space--time residual minimization yields a fundamentlaly
%different model than sequential residual minimization in time. In particular, space--time
%residual minimization associates with an algebraic system of equations defined
%over all space and time, whose solution comprises a \text{fixed} vector of
%(space--time) generalized coordinates; on the other hand, sequential residual
%minimization in time (often) associates with a system of ODEs whose solution
%comprises a \textit{time-dependent} vector of (spatial) generalized
%coordinates.

Space--time residual minimization approaches minimize the FOM residual over all of space and time and, as a result, yield models that are equipped with a notion of space--time optimality and \textit{a priori} error bounds that grow more slowly
in time. Further, space--time approaches reduce both the spatial and temporal dimensions of the full-order model, and thus promise cost savings over spatial-projection-only approaches.
% and the resulting \textit{trajectory} exhibits a notion of
%optimality; this implies that the solution exhibits a notion of optimality
%over the entire time domain. 
%\EP{Come back to this, this also has to do with residual minimization}
However, space--time techniques also suffer from several
limitations. First, the computational cost of solving the algebraic system arising from space--time
approaches scales cubically with the number of space--time degrees of freedom;
in contrast, the computational cost incurred by standard
spatial-projection-based ROMs is linear in the number of temporal degrees of
freedom, as the attendant solvers can leverage the lower-block triangular
structure of the system arising from the sequential nature of time
evolution. As a result, solving the algebraic systems arising from space--time
projection is generally intractable without applying hyper-reduction in time
\cite{choi_stlspg,constantine_strom}. Second, space--time residual
minimization precludes future
state prediction, as these methods employ space--time basis vectors defined over the
entire time domain of interest, which must have been included in the training
simulations.

The objectives of this work are to overcome the shortcomings of existing
residual-minimizing methods, and to provide a unifying framework from which
existing methods can be assessed. In essence, the proposed \textit{\methodNameLower\
(\methodAcronym)} \approachKwd\ sequentially minimizes the FOM
residual over a sequence of arbitrarily defined
time windows. The method is characterized by three notable aspects.  First, the method
minimizes the \textit{time-continuous}  residual (i.e., that associated with
the full-order model ODE). By adopting a time-continuous viewpoint, the
formulation decouples the underlying temporal discretization scheme from the
residual-minimization problem, thus addressing a key deficiency of both LSPG
and space--time LSPG. Critically, time-continuous residual minimization also exposes
two different solution methods:  a \textit{discretize-then-optimize}
(i.e., direct) method, and an \textit{optimize-then-discretize} (i.e.,
indirect) method. Second, the method sequentially minimizes
the residual over arbitrarily defined \textit{time windows} rather than
sequentially minimizing the residual over time steps (as in LSPG)
or over the entire time domain (as in space--time residual-minimization
methods). This equips the method with additionally flexibility that enables it
to explore more fine-grained tradeoffs between computational cost and error.
Finally, \methodAcronym\ is formulated for two kinds of space--time trial
subspaces: one that associates with spatial dimension reduction (as employed by traditional spatial-projection-based methods), and one that associates with space--time dimension reduction (as employed by space--time methods).
The above attributes allow the \methodAcronym\ approach to be viewed as a
generalization of existing model reduction methods, as 
Galerkin, LSPG, and space--time LSPG projection correspond to specific
instances of the formulation.
Figure~\ref{fig:flowchart} depicts how the proposed \methodAcronym\ method
provides a unifying framework from which these existing approaches can be derived.

The \methodAcronym\ approach can be viewed as a hybrid space--time method and displays 
commonalities with several related efforts. 
First, Ref.~\cite{bui_thesis} briefly formulated a space--time least-squares ROM
and connected this formulation with optimal control; specifically it mentioned the
optimize-then-discretize vs.\ discretize-then-optimize approaches. This work
did not fully develop this approach, eschewing it 
for 
sequential residual minimization in time (i.e., LSPG). The present work thus
formally develops and extends several of the concepts put forth
in Ref.~\cite{bui_thesis}. Next, Ref.~\cite{constantine_strom} developed a
space--time residual minimization formulation for model interpolation.  The
present work distinguishes itself from Ref.~\cite{constantine_strom} in that
(1) this work also considers trial subspaces characterized by spatial dimension reduction only, and (2) we
minimize the time-continuous FOM residual over arbitrary time
windows. We note that,
similar to the current work, Ref.~\cite{constantine_strom} employs
minimization of the time-continuous FOM residual as a starting point; as such,
this work shares some thematic similarities with
Ref.~\cite{constantine_strom}.  Additionally, Ref.~\cite{choi_stlspg} develops a
space--time extension of LSPG projection that minimizes the time-discrete
FOM residual over the entire time domain.  The present work distinguishes
itself from Ref.~\cite{choi_stlspg} in that (1) this work minimizes the
time-continuous FOM residual, (2) this work minimizes this residual over
arbitrary time windows, and (3) this work also considers trial subspaces
associated with spatial dimension reduction only. Lastly, we note that the windowing approach proposed here  
displays commonalities with space--time methods employing space--time slabs, e.g., Refs.\cite{HUGHES1988339,PIRONNEAU1992117,HUGHES1996217,HUGHES1989173}. 
In these approaches, the space--time domain is typically decomposed into a series of space--time slabs. A system of algebraic equations is then formed over each slab 
via, e.g., the discontinuous Galerkin method, and the space--time method operates by sequentially marching through these space--time slabs. From this perspective, the proposed windowing approach is best viewed as a space--time method in where each window comprises multiple time-slabs. The time-slabs within each window experience a two-way coupling to each other, by virtue of the residual minimization statement being defined over the entire window, and thus one can no longer sequentially march over the time-slabs. 


\begin{figure} 
\begin{centering} 
\includegraphics[trim={0.1cm 0cm 1cm 0cm},clip,width=0.99\textwidth]{diagram.pdf} 
\caption{Relationship diagram for the \methodAcronym\ approach for model
	reduction.} 
\label{fig:flowchart} 
\end{centering} 
\end{figure}



 In summary, specific contributions of this work include:
\begin{enumerate}
%\item The development of a time-continuous residual minimization framework for reduced-order models.
\item The \methodNameLower\ (\methodAcronym) approach for dynamical-system
	model reduction. The approach sequentially minimizes the time-continuous
		full-order-model residual over arbitrary time windows.
%\item Support of two space--time trial subspaces: one that reduces the spatial
%	dimension of the FOM only, and one that reduces both the spatial and
%		temporal dimensions of the FOM. 
\item Support of two space--time trial subspaces: one that associates with spatial dimension reduction and 
    one that associates with spatial and temporal dimension reduction.
The former case is of particular
		interest in the \methodAcronym\  context, as the stationary conditions are
		derived via the Euler--Lagrange equations and comprise a coupled two-point
		Hamiltonian boundary value problem containing a forward and backward
		system. The forward system, which is forced by an auxiliary costate,
		evolves the (spatial) generalized coordinates of the ROM in time. The
		backward system, which is forced by the time-continuous FOM residual
		evaluated about the ROM state, governs the dynamics of the costate. 
	\item Derivation of two solution techniques:
		\textit{discretize then optimize} (i.e., direct) and
		\textit{optimize then discretize} (i.e., indirect). 
	\item Remarks and derivations of conditions under which the
		\methodAcronym\ approach recovers Galerkin, LSPG, and space--time LSPG.
\item Error analysis of the \methodAcronym\ approach using
	trial subspaces associated with spatial dimension reduction.
	This analysis demonstrates that, over a given window, the \methodAcronymROMs\ error 
		is bounded \textit{a priori} by a combination of the
		error at the start of the window and the integrated FOM ODE residual
		evaluated at the FOM state projected onto the trial subspace. 
%As a result,
%		the error bound exhibits exponential growth in the number of time windows
%		(not total time instances), yielding smaller error-bound growth in thime than
%		Galerkin or LSPG projection. \KTC{Check this}
\item Numerical experiments for 
	trial subspaces associated with spatial dimension reduction, which demonstrate two key findings:
\begin{itemize}
\item Minimizing the residual over a larger time window leads to more stable solutions 
with lower space--time residuals norms.
\item Minimizing the residual over a larger time window does not necessarily
	lead to a more accurate trajectory (as measured in the space--time
		$\elltwo$-norm of the solution). Instead, minimizing the residual over an
		intermediate-sized time window leads to the smallest trajectory error.
\end{itemize}
\end{enumerate}

The paper proceeds as follows: Section~\ref{sec:math}
outlines the mathematical setting for the full-order model, along with Galerkin, LSPG, and
space--time LSPG projection. Section~\ref{sec:tclspg} outlines the proposed \methodAcronym\
\approachKwd. Section~\ref{sec:numerical_techniques}
outlines numerical techniques for solving \methodAcronymROMs, including both direct and
indirect methods. Section~\ref{sec:analysis} provides
equivalence conditions and error analysis for \methodAcronymROMs.
Section~\ref{sec:numerical_experiments} presents numerical experiments.
Section~\ref{sec:conclude} provides conclusions and perspectives.
We denote vector-valued functions with italicized bold symbols (e.g., $\boldsymbol
x$), vectors with standard bold symbols (e.g., $\mathbf{x}$), 
matrices with capital bold symbols (e.g., $\mathbf{X} \equiv \begin{bmatrix}
\mathbf{x}_1 & \cdots & \mathbf{x}_r\end{bmatrix}$), and spaces with
calligraphic symbols (e.g., $\mathcal{X}$). We additionally denote differentiation of a time-dependent 
function with respect to time with the $\dot\ $ operator.
%Model reduction of nonlinear dynamical systems is a growing research area.
%The least-squares Petrov-Galerkin (LSPG) approach has gained popularity as a model reduction technique for nonlinear dynamical systems. Derived at the fully discrete level, the LSPG approach solves a least-squares problem that minimizes the fully discrete residual of the reduced-order model at each time-step. LSPG has been shown to be an effective formulation for nonlinear model reduction on complex problems of interest.
%\begin{comment} 
\section{Mathematical formulation}\label{sec:math}
	We begin by providing the formulation for the full-order model,
	followed by a description of standard model reduction methods
	classified according to the type of trial subspace they employ.
	\subsection{Full-order model}\label{sec;FOM}
We consider the full-order model to be a dynamical system expressed as a
	system of ordinary differential equations (ODEs)
\begin{equation}\label{eq:FOM}
 \stateFOMDotArg{t}  = \velocity(\stateFOMArg{}{t},t), \qquad \stateFOM(0) =
	\stateFOMIC,\qquad t \in [0,T],
\end{equation}
where  $\stateFOM: [0,T]
	\rightarrow  \RR{\fomdim}$ with $\stateFOM: \timeDummy \mapsto
	\stateFOM(\timeDummy) $ and $\stateFOMDot \equiv {d \stateFOM}/{d\tau}$
	denotes the state implicitly defined as the solution to initial value
	problem~\eqref{eq:FOM}, 
$T \in \RRplus$ denotes the final time, 
 $\stateFOMIC \in \mathbb{R}^{\fomdim}$ denotes the initial condition,
	and $\velocity: \mathbb{R}^{\fomdim} \times [0,T] \rightarrow
	\mathbb{R}^{\fomdim}$ with $(\stateyDiscrete,\timeDummy) \mapsto
	\velocity(\stateyDiscrete,\timeDummy)$ denotes the velocity, which is possibly
	nonlinear in its first argument. For subsequent exposition, we introduce
	$\timeSpace$ to denote the set of (sufficiently smooth) real-valued functions acting on the time
	domain (i.e., $\timeSpace = \{f\,|\,f:[0,T]\rightarrow\RR{}\}$); the
	state can be expressed equivalently as $\stateFOM \in \RR{\fomdim} \otimes
	\timeSpace $.  We refer to the initial value problem defined in
	Eq.~\eqref{eq:FOM} as the ``full-order model" (FOM) ODE. We note that
	although the problem of interest described in the introduction corresponds
	to a parameterized dynamical system, we suppress dependence of the FOM ODE
	\eqref{eq:FOM} on such parameters for notational convenience, as this work
	focuses on devising a model reduction approach applicable to a specific parameter instance.

Directly solving the FOM ODE~\eqref{eq:FOM} is computationally expensive if
	either the state-space dimension $\fomdim$ is large, or if the time-interval
	length $T$ is large relative to the time step required to numerically
	integrate Eq.~\eqref{eq:FOM}. For time-critical or many-query applications,
	it is essential to replace the FOM ODE~\eqref{eq:FOM} with a strategy that
	enables an approximate trajectory to be computed at lower computational
	cost. Projection-based ROMs constitute one such promising approach. 

\subsection{Reduced-order models}
Projection-based ROMs generate approximate solutions to the FOM
	ODE~\eqref{eq:FOM} by approximating the state in a low-dimensional trial
	subspace. Two types of space--time trial subspaces are commonly used for
	this purpose:\footnote{For both spatial and space--time ROMs of dynamical systems, all trial subspaces are, strictly speaking, space--time subspaces.} 
\begin{enumerate} 
	\item \textit{Subspaces that reduce only the spatial dimension of the full-order
		model (\spatialAcronym)}\footnote{In this work, we refer to the ``state-space" dimension of an ordinary differential equation as the ``spatial" dimension. We use this notation due to the common application of projection-based ROMs to semi-discretized partial differential equations. We emphasize, however, that the presented approach is applicable to ordinary differential equations that do not associate with some type of spatial discretization, i.e., as in molecular dynamics.}
. These trial subspaces are characterized by a spatial projection operator, associate with a basis that represents the spatial dependence of the state, and are employed in classic model reduction approaches, e.g., Galerkin and LSPG. %These 
%	trial subspaces are commonly used
%		in model-reduction methods based on spatial projection followed by time
%		integration (e.g., the Galerkin and LSPG approaches).
	\item \textit{Subspaces that reduce both the spatial and temporal dimensions of the full-order
		model (\spaceTimeAcronym)}.
These trial subspaces are characterized by a space--time projection operator, associate with a basis that represents the spatial and temporal dependence of the state, and are employed in space--time 
model reduction approaches (e.g., space--time Galerkin~\cite{benner_st}, space--time LSPG~\cite{choi_stlspg}). 
% These trial subspaces are commonly employed in
%		model-reduction methods based on space--time projection (e.g., space--time
%		LSPG).
\end{enumerate}
 We now describe these two types of space--time trial subspaces and their
	application to the Galerkin, LSPG, and space--time LSPG approaches. 

\subsection{\spatialAcronym\ trial subspaces}
At a given time instance 
$t\in[0,T]$,
\spatialAcronym\ trial subspaces approximate the FOM ODE solution
	as $\approxstate(t)\approx\stateFOM(t)$, which is enforced to reside in an
	affine spatial trial subspace of dimension $K\ll\fomdim$ such that
	$\approxstate(t)\in
	\stateIntercept+\trialspace
\subseteq\RR{\fomdim}$, where $\dim(\trialspace) = K$
and $\stateIntercept \in \mathbb{R}^{\fomdim}$ denotes the reference state, which
	is often taken to be the initial condition (i.e., $\stateIntercept = \stateFOMIC$).
Here, the trial subspace
$\trialspace$ 
is spanned by an orthogonal basis such that
$ \trialspace= \Range{\basismat}$
with 
$ \basismat \equiv \begin{bmatrix}  \basisvec_1  & \cdots &  \basisvec_K \end{bmatrix}
	\in \RRStar{\romdim}{\fomdim}$, where $\RRStar{\romdim}{\fomdim}$ denotes the compact Stiefel manifold (i.e.,  $
	\RRStar{\romdim}{\fomdim}\defeq
	\{ \mathbf{X} \in \RR{\fomdim
	\times \romdim}\, \big|\, \mathbf{X}^T \mathbf{X} = \mathbf{I} \}$).
The basis vectors $\basisvec_i$, $i=1,\ldots,K$ are typically constructed
using state snapshots, e.g., via
proper orthogonal decomposition (POD)~\cite{berkooz_turbulence_pod}, the reduced-basis method~\cite{rb_1,rb_2,rb_3,NgocCuong2005,Rozza2008}. 
Thus, at any time instance $t\in[0,T]$, ROMs that employ the  \spatialAcronym\
trial subspace approximate the FOM ODE solution as
\begin{equation}\label{eq:affine_trialspace}
\stateFOMArg{}{t} \approx \approxstate(t) = \basisspace \genstateArg{}{t} + \stateIntercept,
\end{equation}
where $\genstate \in \RR{\romdim} \otimes \timeSpace$ with
$\genstate:\timeDummy\mapsto\genstate(\timeDummy)$
denotes the generalized
coordinates. From the space--time perspective, this is equivalent to approximating the
	FOM ODE solution trajectory $\stateFOM\in\RR{N}\otimes\timeSpace$ with 
	$\approxstate\in \stspaceS$, where
\begin{equation}\label{eq:spatial_subspace}
\begin{split}
& \stspaceS \defeq \trialspace \otimes \timeSpace +
	\stateIntercept\otimes\onesFunction\subseteq\RR{N}\otimes\timeSpace,
\end{split}
\end{equation}
with $\onesFunction\in\timeSpace$ defined as
$\onesFunction:\timeDummy\mapsto 1$.
	 
Substituting the approximation~\eqref{eq:affine_trialspace} into the FOM ODE~\eqref{eq:FOM} and performing orthogonal
$\elltwo$-projection of the initial condition onto the trial subspace yields
the overdetermined system of ODEs
\begin{equation}\label{eq:g_truncation}
\basisspace \genstateDotArg{}{t} = \velocity(\basisspace
\genstateArg{}{t} + \stateIntercept,t ), \qquad \genstate(0) = \genstateIC,
	\qquad t \in [0,T],
\end{equation}
where $\genstateDot\equiv {d \genstate}/{d\tau}$.
Because Eq.~\eqref{eq:g_truncation} is overdetermined, a solution may not
exist. Typically, either \textit{Galerkin} or \textit{least-squares
Petrov--Galerkin} projection is employed to reduce the number of equations
such that a unique solution exists. We now describe these two methods.

\subsubsection{Galerkin projection}
The Galerkin approach reduces the number of equations in
Eq.~\eqref{eq:g_truncation} by enforcing orthogonality of the 
residual to the spatial trial subspace in the (semi-)inner product induced by the 
positive (semi-)definite $\fomdim\times\fomdim$ matrix $\stweightingMatArg{}\equiv \stweightingMatOneTArg{ }
\stweightingMatOneArg{ }$ (commonly set to
$\stweightingMatArg{}=\mathbf{I}$), i.e.,
\begin{equation}\label{eq:g_truncation2}
\genstateGalerkinDotArg{}{t} = \massArgnt{n}^{-1} \basisspace^T \stweightingMatArg{} \velocity(\basisspace
\genstateGalerkinArg{}{t} + \stateIntercept,t), \qquad \genstateGalerkin(0) = \genstateIC, \qquad t \in [0,T],
\end{equation}
where $\massArgnt{ } \equiv \basisspace^T \stweightingMatArg{ } \basisspace$
denotes the $K\times K$ positive definite mass matrix.
As demonstrated in Ref.~\cite{carlberg_lspg_v_galerkin}, the Galerkin approach
can be viewed alternatively as a residual-minimization
method, as the Galerkin ODE~\eqref{eq:g_truncation2} is equivalent
to
\begin{equation}\label{eq:GalOptimal}
\genstateGalerkinDotArg{}{t} = \underset{ \genstateyDiscrete \in \RR{\romdim}
	}{\text{arg\,min}} \| \basisspace \hat{\stateyDiscrete} -
	\velocity(\basisspace \genstateGalerkinArg{}{t} + \stateIntercept,t)
	\|_{\stweightingMatArg{}}^2, \qquad \genstate(0) = \genstateIC,
	\qquad t \in [0,T],
%\hat{\mathbf{v}}, %\qquad \genstateGalerkin(0) = \basisspace^T(\stateFOMIC -\stateIntercept), \qquad t \in [0,T].
\end{equation}
where $\|\mathbf{x}\|_{\stweightingMatArg{}}\equiv
\sqrt{\mathbf{x}^T\stweightingMatArg{}\mathbf{x}}$.
Thus, the computed velocity $\genstateGalerkinDotArg{}{t}$ minimizes the
FOM ODE residual evaluated at the state $\basisspace
\genstateGalerkin(t) + \stateIntercept$ and time instance $t$ over the spatial trial
subspace $\trialspace$.
\subsubsection{Least-squares Petrov--Galerkin projection}
Despite its time-instantaneous residual-minimization optimality property 
\eqref{eq:GalOptimal}, the Galerkin approach can yield inaccurate solutions,
particularly if the velocity is not self-adjoint or
is nonlinear. 
Least-squares Petrov--Galerkin (LSPG)
~\cite{carlberg_lspg_v_galerkin,carlberg_thesis,carlberg_gnat,bui_unsteady,bui_thesis}
was developed
as an alternative method that exhibits several advantages over the Galerkin approach.
Rather than minimize the (time-continuous) FOM ODE residual at a time instance (as in Galerkin), LSPG minimizes
the (time-discrete) FOM O$\Delta$E residual 
(i.e., the residual arising after applying time discretization to the FOM ODE)
over a time step.
We now describe the LSPG approach in the case of linear multistep methods;
Ref.~\cite{carlberg_lspg_v_galerkin} also presents LSPG for Runge--Kutta
schemes. 

Without loss of generality, we introduce a uniform time
grid characterized by time step $\Delta t$ and time instances
$t^n = n\Delta
t$, $n=0,\ldots,N_t$.
Applying a linear multistep method to discretize the FOM ODE \eqref{eq:FOM}
with this time grid
yields the FOM O$\Delta$E, which computes the sequence of discrete
solutions
$\stateFOMDiscreteArg{n}( \approx \stateFOM(t^n))$, $n=1,\ldots,N_t$
as the implicit solution to the system of algebraic equations
\begin{equation}\label{eq:lms}
\residLMSArg{n}
	(\stateFOMDiscreteArg{n};\stateFOMDiscreteArg{n-1},\ldots,\stateFOMDiscreteArg{n-k^n})
	= \bz,\qquad n=1,\ldots,N_t,
\end{equation}
with the initial condition $\stateFOMDiscreteArg{0} = \stateFOMIC$. In the above, $k^n$ denotes the number of steps employed by the scheme at the $n$th
time instance and 
$\residLMSArg{n}$ denotes the FOM O$\Delta$E residual defined as
\begin{align*}
\residLMSArg{n} &: (\stateyDiscreteArgnt{n};\stateyDiscreteArgnt{n-1},\ldots,\stateyDiscreteArgnt{n-k^n}) \mapsto  \frac{1}{\Delta t} \sum_{j=0}^{k^n} \alpha^n_j \stateyDiscreteArgnt{n-j} -  \sum_{j=0}^{k^n} \beta^n_j \velocity(\stateyDiscreteArgnt{n-j},t^{n-j}),
\\
&: \RR{\fomdim} \otimes \RR{k^n + 1} \rightarrow \RR{\fomdim}.
\end{align*} 
Here, $\alpha^n_j,\beta^n_j\in\RR{}$, $j=0,\ldots,k^n$ are coefficients
that define the linear multistep method at the $n$th time instance.

At each time instance on the time grid, LSPG substitutes the \spatialAcronym\ trial subspace approximation
\eqref{eq:affine_trialspace} into the FOM O$\Delta$E~\eqref{eq:lms} and
minimizes the residual, i.e., LSPG sequentially computes the solutions
$\approxstateLSPG^n \approx \stateFOMDiscreteArg{n}$, $n=1,\ldots,
\ntimeSteps$ that satisfy
\begin{equation*}
\approxstateLSPG^n = \underset{\stateyDiscrete \in \trialspace + \stateIntercept }{\text{arg\,min}}|| \lspgWeightingArg{\stateyDiscrete}\residLMSArg{n} (\stateyDiscrete; \approxstateLSPG^{n-1},\ldots,\approxstateLSPG^{n-k^n}) ||_2^2, \qquad n = 1,\ldots,\ntimeSteps,
\end{equation*}
where 
$\lspgWeightingArg{\cdot} \in \RR{\nsamples \times \fomdim}$, with $\romdim
\le \nsamples \le \fomdim$, is a weighting matrix that can be used, e.g., to
enable hyper-reduction by requiring it to have a small number of nonzero
columns. 

As described in the introduction, although numerical experiments have
demonstrated that LSPG often yields more accurate and stable
solutions than Galerkin~\cite{bui_thesis,carlberg_lspg_v_galerkin,carlberg_gnat,carlberg_thesis,parish_apg},
LSPG still suffers from several shortcomings. In particular, LSPG suffers from its complex
dependence on the time discretization, exponentially growing error bounds, and
lack of optimality for the trajectory defined over the entire time domain.

\subsection{\spaceTimeAcronym\ trial spaces and space--time ROMs}

Space--time projection methods that employ \spaceTimeAcronym\ trial
spaces~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC,benner_st,bui_thesis}
aim to overcome the latter two shortcomings of LSPG. Because these methods employ \spaceTimeAcronym\ trial
spaces, they reduce both the spatial and temporal dimensions of the full-order
model; further, they yield error bounds that grow more slowly in time and
their trajectories exhibit an optimality property over the entire time domain. 

\spaceTimeAcronym\ trial subspaces approximate the FOM ODE solution
trajectory
	$\stateFOM\in\RR{N}\otimes \timeSpace$ with an approximation that resides in an
	affine space--time trial subspace of dimension $\stdim\ll\fomdim$, i.e., 
	$\approxstate\in \stspaceST$ with $\dim(\stspaceST) =\stdim $, where
\begin{equation}\label{eq:sttrialspace_def}
 \stspaceST \defeq 
	\Range{\stbasis} + 
	\stateFOMIC\otimes\onesFunction
	\subseteq \RR{N} \otimes \timeSpace.
\end{equation}
%and $\stbasis\in\RR{N \times \stdim}\otimes \timeSpace$ with 
Here $\stbasis \in \RR{\fomdim \times \stdim} \otimes \timeSpace$, with $\stbasis:\timeDummy\mapsto\stbasis(\timeDummy)$ and $\stbasis(0) =
\boldsymbol 0$
denotes the space--time trial basis. 
Thus, at any time instance $t\in[0,T]$, ROMs that employ the
\spaceTimeAcronym\ trial subspace approximate the FOM ODE solution as
\begin{equation}\label{eq:stapprox1}
 \stateFOMArg{}{t} \approx \approxstateArg{}{t}  = \stbasisArg{}{t} \stgenstate + \stateFOMIC,
\end{equation}
where $\stgenstateArg{} \in \RR{\stdim}$ denotes the space--time generalized coordinates. 
Critically, comparing the approximations arising from \spatialAcronym\ and
\spaceTimeAcronym\ trial subspaces in Eqs.~\eqref{eq:affine_trialspace} and \eqref{eq:stapprox1}, respectively,
highlights that the former approximation associates with time-dependent
generalized coordinates, while the latter approximation associates with a
time-dependent basis matrix.

Substituting the approximation~\eqref{eq:stapprox1} into the FOM
ODE~\eqref{eq:FOM} yields
\begin{equation}\label{eq:st_fom}
\stbasisDotArg{}{t} \stgenstate =  \velocity(\stbasisArg{}{t} \stgenstate +
	\stateFOMIC,t) , \qquad t \in [0,T],
\end{equation}
where 
$\stbasisDot\equiv d\stbasis/d\timeDummy$. We note that the initial conditions
are automatically satisfied from the definition of the \spaceTimeAcronym\
trial subspace.

Space--time methods reduce the number of equations in
\eqref{eq:st_fom} to ensure a unique solution exists.
We now outline one such method: space--time least-squares Petrov--Galerkin (ST-LSPG)
~\cite{choi_stlspg}. 
While the space--time Galerkin method~\cite{benner_st} is another alternative, it does not
associate with any residual-minimization principle, and thus we do not discuss
it further.
\subsubsection{Space--time LSPG projection} 
Analogously to LSPG, space--time LSPG~\cite{choi_stlspg}
minimizes the (time-discrete) FOM O$\Delta$E residual, but does so
using the \spaceTimeAcronym\ subspace and simultaneously minimizes this
residual over all $N_t$ time instances.
We first introduce the full space--time FOM O$\Delta$E residual for linear
multistep methods as
\begin{align*}
\residST_{} & \vcentcolon (\stateyDiscreteArg{1}, \ldots
	,\stateyDiscreteArg{N_t};\stateFOMIC) \mapsto \begin{bmatrix}
\residLMSArg{1}(\stateyDiscreteArg{1}; \stateFOMIC) 
%\frac{\stateyDiscreteArg{N_t} - \stateyDiscreteArg{N_t-1}  }{\Delta t} -  \velocity(\stateyDiscreteArg{N_t}) \\
\\ 
%\frac{\stateyDiscreteArg{N_t-1} - \stateFOMDiscreteArg{N_t-2}  }{\Delta t} -  \velocity(\stateyDiscreteArg{N_t-1}) \\
\vdots \\
%\frac{\stateyDiscreteArg{1} - \stateFOMIC }{\Delta t} -  \velocity(\stateyDiscreteArg{1}) \\
\residLMSArg{N_t}(\stateyDiscreteArg{N_t};\stateyDiscreteArg{N_t-1},\ldots,\stateyDiscreteArg{N_t - k^{N_t}}) \end{bmatrix} , \\
& \vcentcolon \RR{\fomdim} \otimes \RR{N_t+1} \rightarrow \RR{\fomdim N_t},
\end{align*}
and define the counterpart function acting on space--time generalized
coordinates:
\begin{align*}
\genresidST_{\text{}} & \vcentcolon (\stgenstatey ;\stateFOMIC) \mapsto \begin{bmatrix}
%\frac{\stateyDiscreteArg{N_t} - \stateyDiscreteArg{N_t-1}  }{\Delta t} -  \velocity(\stateyDiscreteArg{N_t}) \\
\residLMSArg{1}\bigg( \stbasisArg{}{t^1}\stgenstatey + \stateIntercept; \stateFOMIC \bigg)
\\ 
\vdots \\
\residLMSArg{N_t}\bigg( \stbasisArg{}{t^{N_t}}\stgenstatey + \stateIntercept; \stbasisArg{}{t^{N_t-1}}\stgenstatey + \stateIntercept ,\ldots,  \stbasisArg{}{t^{N_t-k^{N_t}}}\stgenstatey + \stateIntercept \bigg)  \\ 
\end{bmatrix} , \\
& \vcentcolon \RR{\stdim} \times \RR{N} \rightarrow \RR{\fomdim N_t}. 
\end{align*}
ST-LSPG computes the space--time generalized coordinates that
minimize the space--time FOM O$\Delta$E residual:
\begin{equation}\label{eq:stlspg}
	\stgenstate_\text{ST-LSPG} = \underset{ \stgenstatey\in\RR{\stdim} }{\text{arg\,min}}|| \lspgWeightingSTArg{\stgenstatey}  \genresidST_{\text{}}(\stgenstatey;\stateFOMIC) ||_2^2, 
\end{equation}
where $\lspgWeightingSTArg{\cdot} \in \RR{n_{st} \times N N_t}$, with $\stdim
\le n_{st} \le N N_t$, is a space--time weighting matrix that can be chosen,
e.g., to enable hyper-reduction.

ST-LSPG overcomes two of the primary shortcomings of LSPG. In particular, it
leads to error bounds that grow sub-quadratically in time rather than
exponentially in time, and it generates entire trajectories that associate
with an optimality property over the entire time domain \cite{choi_stlspg}.
However, it is subject to several challenges. First, the computational cost of
solving Eq.~\eqref{eq:stlspg} scales cubically with the number of space--time
degrees of freedom $\stdim$. This cost is due to the fact that ST-LSPG yields 
dense systems that do not expose any natural mechanism for exploiting
the sequential nature of time evolution. Second, it is unclear how these
methods can be employed for future state prediction, as the space--time trial
basis $\stbasis$ must be defined over the entire time interval of interest
$[0,T]$. Third, ST-LSPG is still strongly tied to the time discretization
employed for the full-order model, as it minimizes the (time-discrete) FOM
O$\Delta$E residual over all time instances.
  
\subsection{Outstanding challenges}
This work seeks to overcome the limitations of existing residual-minimizing
model reduction methods, and to provide a unifying framework from which
existing methods can be assessed. Specifically, we look to overcome the
complex dependence of LSPG on the time discretization,
exponential time growth of the error bounds for Galerkin and LSPG,
the cubic dependence of the computational cost of ST-LSPG on the number of
degrees of freedom, and the lack of ability for ST-LSPG to perform prediction
in time. We now describe the proposed \methodNameLower\ \approachKwd\ for this
purpose. 
%\end{comment}
%\input{formulation_3}

%\input{space_time}
%\input{timecont_lspg}
\input{section3/tclsrm_formulation}
\input{section4/numerical_methods}
\input{analysis}
\input{numerical_experiments}



\section{Conclusions}\label{sec:conclude}
This paper proposed the windowed least-squares (\methodAcronym) \approachKwd\ for model reduction  of dynamical systems. The approach sequentially minimizes the
	time-continuous full-order-model residual within a low-dimensional space--time trial
	subspace over time windows. The approach was formulated for two types of trial subspaces: one that reduces only the spatial dimension of the full-order model, and one that reduces both the spatial and temporal dimensions of the full-order model. For each type of trial
	subspace, we outlined two different solution techniques: direct (i.e.,
	discretize then optimize) and indirect (i.e., optimize then discretize). We showed that particular instances of the approach recover Galerkin,
	least-squares Petrov--Galerkin (LSPG), and space–time LSPG projection.  
The case of \spatialAcronym\ trial subspaces is of particular interest in the \methodAcronym\ context: it was shown that indirect methods comprise solving a coupled two-point 
Hamiltonian boundary value problem.
The forward system, which is forced by an auxiliary costate,
		evolves the (spatial) generalized coordinates of the ROM in time. The
		backward system, which is forced by the time-continuous FOM residual
		evaluated about the ROM state, governs the dynamics of the costate. 

Across three numerical experiments conducted on the compressible Euler equations, compressible Navier--Stokes equations, and parameterized shallow water equations, we observed WLS with S-reduction trial subspaces to yield improvements over the Galerkin and LSPG approaches. The first numerical experiment, in where the Sod shock tube was examined, demonstrated that \methodAcronymROMs\ minimizing the residual over larger time windows yielded solutions with lower space--time residuals. Increasing the window size over which the residual was minimized, however, did not necessarily decrease the solution error in the $\elltwo$-norm; we observed this to occur over an intermediary window size. We additionally observed that the  
\methodAcronym\ \approachKwd\ overcomes the time-discretization sensitivity that LSPG is subject to. The second numerical experiment, which examined collocated ROMs of a supersonic compressible cavity flow featuring a two-dimensional shock boundary layer interaction, demonstrated the utility of the \methodAcronym\ formulation on a more complex flow. In this experiment, \methodAcronymROMs\ yielded accurate predictions at a $10$x reduction in cost as compared to the FOM. The WLS ROMs additionally displayed convergence with respect to an increasing basis dimension. LSPG ROMs, on the other hand, failed to produce an accurate prediction for the quantity-of-interest for every basis dimension considered. Additionally, LSPG did not display convergence with increasing basis dimension. Increasing the window size over which the residual was minimized again led to a lower space--time residual, but not necessarily a lower error in the $\elltwo$-norm. Lastly, the final numerical experiment demonstrated the utility of \methodAcronym\ on a parameterized system of equations. When predicting at a novel parameter instance, we observed WLS to again yield accurate solutions at a $10$x reduction in computational cost. For the same training set, the LSPG ROMs, as well as a surrogate model based on multivariate linear regression, failed to yield accurate solutions.   

%In the case of \spatialAcronym\ trial subspaces, \methodAcronym\ displays improved performance over Galerkin and LSPG projection at the expense of an increased computational cost. This increase in computational cost is due to the fact that forming and/or solving the nonlinear minimization problem becomes more difficult as the time window grows. To minimize this increase in cost, future work will target the development of new solution techniques tailored to \methodAcronym.
 
The principal challenge encountered in the \methodAcronym\ formulation is the computational cost: increasing the window size over which the residual is minimized leads to a higher computational cost. In the context of the direct solution approach, this increased cost is due to the increased expense of forming and solving the least-squares problem associated with larger window sizes. In the context of indirect methods, this increased cost is a result of the increased number of iterations of the forward backward sweep method. While numerical experiments demonstrated that the increase in computational cost is mild, future work will target the development of new solution techniques tailored for the \methodAcronym\ approach.

 

%The second subspace we consider arrises when the decoder, $\decoder$, is linear and time-varying, while the generalized coordinates are time-stationary. This is the case in space-time discretizations. In this case one has,
%$$\state(t) \approx \approxstate(t)  =  \decoder(\genstate(t),t) = \basisst(t) \genstate + \state_0 $$
%where $\genstate \in \mathbb{R}^{\romstdim}$ are the time-stationary (space-time) generalized coordinates and the time-continuous basis is given by $\basisst \in \mathbb{R}^{romdim \times \romstdim} \times [0,T]$. The Euler-Lagrange equations are given by,
%\begin{equation}\label{eq:euler_lagrange_st}
% \bigg[ \dot{\basisst}^T \stweightingMat^T  - \basisst^T \big[\frac{\partial \velocity}{\partial \state} \big]^T \stweightingMat^T \bigg]  \bigg( \dot{\basisst} \genstate -    \velocity \bigg) = \mathbf{0}, \qquad \state(t=0) = \state_0\end{equation}
%Note that the end-point boundary condition in the Euler-Lagrange equations is automatically satisfied.

%\subsubsection{Solution techniques}
%Equation~\ref{eq:euler_lagrange_st} consists of a nonlinear algebraic system to be solved at each time-instance, $t$. To obtain solutions, we must employ a temporal discretization. Here, we propose using a variational discretization with tensor products, e.g. finite elements. Let,
%$$\basisst \equiv \basisspace \otimes \basistime \subseteq \basisspace \otimes \mathcal{T},$$
%where $\mathcal{T}$ is the finite elements space. Similarly, let
%$$\genstate = \genstate_s \otimes \genstate_t$$

\section{Acknowledgments}
The authors thank Yukiko Shimizu, Patrick Blonigan, and Francesco Rizzi for numerous conversations from which this work benefited. 
E.\ Parish acknowledges an appointment to the Sandia National Laboratories'
John von Neumann Postdoctoral Research Fellowship in Computational Science. 
This work was partially sponsored by Sandia's Advanced Simulation and
Computing (ASC) Verification and Validation (V\&V) Project/Task
\#103723/05.30.02.  This paper describes objective technical results and
analysis. Any subjective views or opinions that might be expressed in the
paper do not necessarily represent the views of the U.S.\ Department of Energy
or the United States Government.  Sandia National Laboratories is a
multimission laboratory managed and operated by National Technology and
Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell
International, Inc., for the U.S.\  Department of Energy's National Nuclear
Security Administration under contract DE-NA-0003525.

\begin{appendices}
\section{Gauss--Newton method with frozen Jacobians}\label{appendix:gnupdate}
One of the primary costs incurred by both the windowed least-squares approach (solved via the direct method), and the least-squares Petrov--Galerkin approach, is the computation of the Jacobian in the Gauss--Newton algorithm. For instance, in the second numerical example, for WLS ROMs employing a sample mesh of size $10170$ with $K=156$ basis vectors and a window size of $\Delta T = 0.025$ ($=5\Delta T$), computing the Jacobian over a time window via automatic differentiation incurs a computational cost that is $250$x greater than computing the residual over the window. In the third numerical example, for a sample mesh of size $1752$, a basis size of dimension $K = 83$, and a window size of $\Delta T = 2.0$ ($=100\Delta T$), computing the Jacobian over a time window via automatic differentiation incurs a computational cost that is $50$x greater than computing the residual. 

We expect that the computational cost associated with computing the Jacobian can be reduced by employing, for example, frozen Jacobians, analytic Jacobians, matrix-free Jacobian transpose methods~\cite{doi:10.2514/6.2016-0833}. While we leave a full investigation into these methods as a topic for future work, here we examine the use of a Gauss--Newton method employing frozen Jacobians. The main thesis behind a Gauss--Newton method employing frozen Jacobians is that approximate gradients may suffice for convergence of the nonlinear least-squares problem. The algorithm operates similarly to a standard Gauss--Newton method, but differs in that the Jacobian matrix is only updated ever $n_{\text{update}}$ iterations. Algorithm~\ref{alg:colloc_gn_frozen} outlines the algorithm.  
  
\begin{algorithm}
\caption{\spatialAcronym\ trial subspace: algorithm for the direct solution technique with the Gauss--Newton method with frozen Jacobians and a linear multistep method over the $n$th window}
\label{alg:colloc_gn_frozen}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{tolerance, $\epsilon$; initial guess, $\genstateGuessDiscreteArg{n,1}{0},\ldots,\genstateGuessDiscreteArg{n,\nstepsArg{n}}{0}$; global iteration number, $m$; Jacobian update frequency, $n_{\text{update}}$; initial guess to residual-Jacobian $\boldsymbol J$}%; initial condition $\genstateDiscreteArg{n,0}$}
\Output{Solution to least squares problem, $\genstatecollocMatSlabArg{n}$} 
\textbf{Online Steps}: \\
$\text{converged} \leftarrow \text{false}$ \Comment{Set convergence checker} \\
$\genstatecollocMatSlabArg{n}_0 \leftarrow \unroll(\genstateGuessDiscreteArg{n,1}{0},\ldots,\genstateGuessDiscreteArg{n,\nstepsArg{n}}{0})$ \Comment{Assemble generalized coordinates over window} \\
$k \leftarrow 0$ \Comment{Set counter}\\
\While{\text{not converged}}
{
%\For{$i=1,\hdots,\nstepsArg{n}$}{
%  \For{$j=1,\hdots,\ncollocArg{n}{i}$}{
%Compute: $\approxstateArgnt{n,i}  =  \basisspace  \genstateDiscreteArg{n,i} + \stateIntercept$ 
%\Comment{Compute state} \\
%Compute: $\velocity(\basisspace \genstateDiscreteArg{n,i} + \stateIntercept ) $ \Comment{Compute velocity at each time-step instance}\\
%Compute: $\residLMSArg{n,i}(\basisspace \genstateDiscreteArg{n,i} + \stateIntercept, \ldots , \basisspace \genstateDiscreteArg{n,i - k^n(i)} + \stateIntercept) $  \Comment{Compute residual} \\
%}
$\mathbf{r} \leftarrow \residLMSSlabArg{n}(\genstatecollocMatSlabArg{n}_k)$ \Comment{Compute weighted residual over window} \\
\uIf{ $m \% n_{\text{update}} == 0$}{
$\mathbf{J} \leftarrow  
\frac{\partial \residLMSSlabArg{n}}{\partial \genstatecollocMatySlabArg{}}(\genstatecollocMatSlabArg{n}_k)$ 
\Comment{Compute weighted residual-Jacobian over window} \\
}
%Compute: $[\jacobianSlabArg{n}]^T \jacobianSlabArg{n}$ \Comment{Compute system matrix for the normal equations} \\
%Compute: $[\jacobianSlabArg{n}]^T \residLMSSlabArg{n}(\genstatecollocMatSlabArg{n}_k)$ \Comment{compute RHS for normal equations} \\
\uIf{ $\norm{ \mathbf{J}^T\mathbf{r}  } \le \epsilon$ }{
{\text{converged} $\leftarrow$ \text{true}}  \Comment{Check and set convergence based on gradient norm} \\
Return: $\genstatecollocMatSlabArg{n} = \genstatecollocMatSlabArg{n}_{k+1} $ \Comment{Return converged solution}\\
}
\Else{
 Compute  $\Delta  \genstatecollocMatSlabArg{n} $ that minimizes $\norm{ \mathbf{J} \Delta \genstatecollocMatSlabArg{n} +\mathbf{r}}^2$ \Comment{Solve the linear least-squares problem} \\
$\alpha \leftarrow \text{linesearch}(\Delta  \genstatecollocMatSlabArg{n} ,\genstatecollocMatSlabArg{n}_k ) $ \Comment{Compute $\alpha$ based on a line search, or set to 1}\\
$\genstatecollocMatSlabArg{n}_{k+1} \leftarrow \genstatecollocMatSlabArg{n}_k + \alpha \Delta \genstatecollocMatSlabArg{n}$ \Comment{Update guess to the state} \\
}
$k\leftarrow k+1$ \\
$m\leftarrow m+1$
}
\end{algorithm}

The final numerical experiment considered in this work presents results for ROMs employing the Gauss--Newton method with frozen Jacobians. Here, we investigate the sensitivity of the ROMs to the parameter controlling the Jacobian update frequency, $n_{\text{update}}$. To this end we consider the case outlined in Section~\ref{sec:swe_results}, in where WLS ROMs of varying window sizes and a basis of dimension $K=83$ are used to solve the shallow water equations for the parameter instance $(\mu_1, \mu_2) = (7.5,0.16)$. Figure~\ref{fig:rom_metrics_swe_updatefreq} presents performance metrics for the various WLS ROMs, solved via the Gauss--Newton method with frozen Jacobians, as a function of the window size. We note that, in Algorithm~\ref{alg:colloc_gn_frozen}, we use the Jacobian over the previous window as our initial guess to the Jacobian. Results are shown for $n_{\text{update}} = 1,2,5,10,20$. We observe that, for all instances of $n_{\text{update}}$, WLS ROMs solved via Gauss--Newton with frozen Jacobians yield results that are similar to those of the WLS ROMs solved via the standard Gauss--Newton algorithm.  

\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{figs/swe/swe_windowSize_vs_error_updateFreq_K83.pdf}
\caption{Relative time-integrated $\elltwo$-error.}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{figs/swe/swe_windowSize_vs_residual_updateFreq_K83.pdf}
\caption{Time-integrated residual norm.}
\end{subfigure}
\caption{Time-integrated performance metrics of the LSPG and WLS ROMs as a function of window size. We examine ROMs solved via the Gauss--Newton method with frozen Jacobians for different update frequencies, $n_{\text{update}}$} 
\label{fig:rom_metrics_swe_updatefreq}
\end{center}
\end{figure}

Next, Figure~\ref{fig:rom_walltimes_swe_updatefreq} reports the relative wall times of the various WLS ROMs with respect to the FOM. We make several interesting observations. We first observe that, for the values of $n_{\text{update}}$ considered, higher values of $n_{\text{update}}$ yield lower wall times for WLS with smaller window sizes. For WLS with larger window sizes, larger values of $n_{\text{update}}$ yield lower wall times until $n_{\text{update}}$ reaches a critical value. After this value has been exceeded, we do not observe a further reduction in cost. For example, WLS with $\Delta T = 10.0$ incurs the same computational cost for $n_{\text{update}} = 5$, as it does for $n_{\text{update}} = 10$ and $n_{\text{update}} = 20$. Similarly, WLS with $\Delta T = 5.0$ incurs the same computational cost for $n_{\text{update}}= 10$ as it does for $n_{\text{update}} = 20$. Across all cases, employing a frozen Jacobian algorithm leads to between a $2$x and $4$x decrease in computational cost. This speedup is obtained without compromising the solution accuracy, as shown in Figure~\ref{fig:rom_metrics_swe_updatefreq}. 
\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{figs/swe/swe_windowSize_vs_walltime_updateFreq_K83.pdf}
\caption{Relative time-integrated $\elltwo$-error.}
\end{subfigure}
%\begin{subfigure}[t]{0.49\textwidth}
%\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{figs/swe/swe_windowSize_vs_walltimeLspg2_updateFreq_K83.pdf}
%\caption{Time-integrated residual norm.}
%\end{subfigure}
\caption{Relative wall times of WLS ROMs solved via the Gauss--Newton method with frozen Jacobians. Results are presented for various Jacobian update frequencies, $n_{\text{update}}$} 
\label{fig:rom_walltimes_swe_updatefreq}
\end{center}
\end{figure}


\section{Proper orthogonal decomposition}
Algorithm~\ref{alg:pod} presents the algorithm for computing the trial basis via proper orthogonal decomposition.
In the final numerical experiment, trial bases are constructed for a set of varying input parameters. Algorithm~\ref{alg:pod_tlp} describes the construction of the POD bases for this case.
\begin{algorithm}
\caption{Algorithm for generating POD basis.}
\label{alg:pod}
\textbf{Input:} Number of time-steps between snapshots $N_{\text{skip}}$; intercept $\stateInterceptArg{}$; basis dimension $K$; weighting matrix, $\mathbf{T}$ \; 
%\newline
\textbf{Output:} POD Basis $\basisspace \in \{ \mathbf{X} \in \RR{\fomdim \times \romdim} | \mathbf{X}^T\mathbf{T} \mathbf{X} = \mathbf{I} \} $ \;
%\newline
\textbf{Steps:}
\begin{enumerate}
    \item Solve FOM O$\Delta$E and collect solutions into snapshot matrix
$$\mathbf{S}(N_{\text{skip}}) \defeq \begin{bmatrix} \stateFOMDiscreteArg{0} - \stateInterceptArg{} & \stateFOMDiscreteArg{N_{\text{skip}}} - \stateInterceptArg{} & \cdots & \stateFOMDiscreteArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}} - \stateInterceptArg{} \end{bmatrix}$$
    \item Compute the thin singular value decomposition, $$ \sqrt{\mathbf{T}}^T\mathbf{S} (N_{\text{skip}}) = \mathbf{U \Sigma Z}^T,$$
    where $\sqrt{\mathbf{T}}$ is the Cholesky decomposition of $\mathbf{T}$, i.e., $\sqrt{\mathbf{T}} \sqrt{\mathbf{T}}^T = \mathbf{T}$
    \item Truncate left singular vectors and to form the basis, $\basisspace \equiv \begin{bmatrix} [\sqrt{\mathbf{T}}^T]^{-1}  \mathbf{u}_1 & \cdots & [\sqrt{\mathbf{T}}^T]^{-1} \mathbf{u}_K \end{bmatrix}$
\end{enumerate}
\end{algorithm}
\begin{algorithm}
\caption{Algorithm for generating POD basis over multiple parameter instances}
\label{alg:pod_tlp}
\textbf{Input:} Number of time-steps between snapshots $N_{\text{skip}}$; intercept $\stateInterceptArg{}$; parameter training set $\mathcal{D}_{\text{train}}$; basis dimension $K$; weighting matrix, $\mathbf{T}$ \;
%\newline
\textbf{Output:} POD Basis $\basisspace \in \{ \mathbf{X} \in \RR{\fomdim \times \romdim} | \mathbf{X}^T\mathbf{T} \mathbf{X} = \mathbf{I} \} $ \;
%\newline
\textbf{Steps:}
\begin{enumerate}
    \item Execute Algorithm~\ref{alg:pod}, with the following modification to step 1: Solve the FOM O$\Delta$E for $\params \in \mathcal{D}_{\text{train}}$ and collect solutions into snapshot matrix
\small{$$\mathbf{S}(N_{\text{skip}}) \defeq \begin{bmatrix} \stateFOMDiscreteParamArg{0}{\param_1} - \stateInterceptArg{} & \cdots & \stateFOMDiscreteParamArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}}{\param_1}- \stateInterceptArg{} & \cdots & \stateFOMDiscreteParamArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}}{\param_{n_{\text{train}}}}- \stateInterceptArg{} \end{bmatrix}$$}
\end{enumerate}
\end{algorithm}


\section{Selection of sampling points}
To construct the sampling point matrix used for hyper-reduction in the second numerical experiment, we employ q-sampling~\cite{qdeim_drmac} and the sample 
mesh concept~\cite{carlberg_gnat}. Algorithm~\ref{alg:qdeim} outlines the steps used in the second numerical experiment to compute the sampling points. In the final numerical experiment, trial bases are constructed for a set of varying input parameters. Algorithm~\ref{alg:qdeim_tlp} outlines the steps used for this case.


\begin{algorithm}
\caption{Algorithm for generating the sampling matrix through q-sampling.}
\label{alg:qdeim}
\textbf{Input:} Number of time-steps between snapshots $N_{\text{skip}}$, number of primal sampling points, $n_s$, weighting  matrix, $\mathbf{T}$\; 
%\newline
\textbf{Output:} Weighting matrix $\stweightingMatArg{} \equiv \stweightingMatOneTArg{} \stweightingMatOneArg{} \in \{0,1\}^{N \times N}$ \;
%\newline
\textbf{Steps:}
\begin{enumerate}
    \item Solve FOM O$\Delta$E and collect solution snapshots 
%$$\mathbf{F}(N_{\text{skip}}) \defeq \begin{bmatrix} \velocity (\stateFOMDiscreteArg{0})  & \velocity( \stateFOMDiscreteArg{N_{\text{skip}}}) & \cdots & \velocity( \stateFOMDiscreteArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}} ) \end{bmatrix}$$
$$\mathbf{F}(N_{\text{skip}}) \defeq \begin{bmatrix} \stateFOMDiscreteArg{0} & \stateFOMDiscreteArg{N_{\text{skip}}}  & \cdots & \stateFOMDiscreteArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}} \end{bmatrix}$$
    \item Compute the thin singular value decomposition, $$ \sqrt{\mathbf{T}}^T \mathbf{F} (N_{\text{skip}}) = \mathbf{U \Sigma Z}^T,$$
    where $\mathbf{U} \equiv \begin{bmatrix} \mathbf{u}_1 & \cdots & \mathbf{u}_{\text{floor}(N_t/N_{\text{skip}})}\end{bmatrix}$
    \item Set  $\basisspace \equiv \begin{bmatrix} [\sqrt{\mathbf{T}}^T]^{-1}  \mathbf{u}_1 & \cdots & [\sqrt{\mathbf{T}}^T]^{-1} \mathbf{u}_{\text{floor}(N_t/N_{\text{skip}}}\end{bmatrix}$
    \item Compute the QR factorization of  $\mathbf{\basisspace}^T$ with column pivoting,
    \begin{equation*}
        \mathbf{\basisspace}^T \mathbf{P}^* = \mathbf{QR}
    \end{equation*}
    with $\mathbf{P}^* \equiv \begin{bmatrix} \mathbf{p}_1 & \cdots & \mathbf{p}_{\text{floor}(N_t/N_{\text{skip}})} \end{bmatrix}$, $\mathbf{p}_i \in \{0,1\}^N$. 
    \item Select the first $n_s$ columns of $\mathbf{P}^*$ to form the sampling point matrix $\stweightingMatOneTArg{} \in \{0,1\}^{N \times n_s}$. 
    \item Augment the sampling point matrix, $\stweightingMatOneArg{}$, with additional columns such that all unknowns are computed at the mesh cells selected by Step 3. 
\end{enumerate}


\end{algorithm}
\begin{algorithm}
\caption{Algorithm for generating the sampling matrix through q-sampling.}
\label{alg:qdeim_tlp}
\textbf{Input:} Number of time-steps between snapshots $N_{\text{skip}}$, number of primal sampling points, $n_s$, parameter training set $\mathcal{D}_{\text{train}}$, weighting  matrix, $\mathbf{T}$\;
%\newline
\textbf{Output:} Weighting matrix $\stweightingMatArg{} \equiv \stweightingMatOneTArg{} \stweightingMatOneArg{} \in \{0,1\}^{N \times N}$ \;
%\newline
\textbf{Steps:}
\begin{enumerate}
    \item Execute Algorithm~\ref{alg:qdeim}, with the following modification to step 1: Solve the FOM O$\Delta$E for $\params \in \mathcal{D}_{\text{train}}$ and collect solutions into snapshot matrix
\small{$$\mathbf{F}(N_{\text{skip}}) \defeq \begin{bmatrix} \stateFOMDiscreteParamArg{0}{\param_1}  & \cdots & \stateFOMDiscreteParamArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}}{\param_1} & \cdots & \stateFOMDiscreteParamArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}}{\param_{n_{\text{train}}}} \end{bmatrix}$$}

\end{enumerate}


\end{algorithm}



\section{Derivation of the Euler--Lagrange equations}\label{appendix:eulerlagrange}
This section details the derivation of the Euler--Lagrange equations. To this end, we consider the generic functional of the form,
\begin{equation}\label{eq:el_gen}
\mathcal{J}: (\statey,\stateyDot) \mapsto  \int_a^b \integrand(\statey(t),\stateyDot(t),t)  dt,
\end{equation}
where $\integrand: \RR{M}  \times \RR{M}  \times[a,b] \rightarrow  \RRplus$
	(for arbitrary $M$) with $\integrand:
	(\stateyDiscrete,\stateyDotDiscrete,\timeDummy) \mapsto
	\integrand(\stateyDiscrete,\stateyDotDiscrete,\timeDummy)$. We now introduce
	the function $\statez: [a,b] \rightarrow \RR{M}$ with $\statez: \timeDummy \mapsto \statez(\timeDummy)$ along with $\statezDot \equiv d \statez /d\timeDummy$. We define this function to be a stationary point of~\eqref{eq:el_gen} (with $\statez$ being the first argument and $\statezDot$ being the second argument) subject to the boundary condition
	$\statez(a) = \statez_a$. 
We additionally introduce an arbitrary function $\variation : [a,b] \rightarrow \RR{M}$
with the boundary condition $\variationArg{a} = \bz$ and define a variation
from the stationary point by
\begin{align*}
\statezBar  &: (\tau,\delta) \mapsto \statez(\tau) + \delta \variation(\tau),\\
&: [a,b] \times \RR{} \rightarrow \RR{M}.
\end{align*}
We note that $\statezBar$ satisfies the same boundary condition $\statez$ since $\variationArg{a} = \bz$.
We define a new function that is equivalent to the function~\eqref{eq:el_gen} evaluated at $\statezBar$ in the first argument and 
$\statezDotBar \equiv d\statezBar / d\timeDummy$ in the second argument,
$$
\mathcal{J}_{\delta}: \delta \mapsto \int_a^b \integrand(\statezBar(t,\delta),\statezDotBar(t,\delta) ,t)  dt.
$$
%$$
%\mathcal{J}(\epsilon) = \int_a^b \integrand(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t)  dt.
%$$
The objective is to now find $\overline{\statez}$ that makes
$\mathcal{J}_{\delta}$ stationary. This can be done by differentiating with
respect to $\delta$ and setting the result to zero, i.e., % Noting that the extermum of Eq.~\ref{eq:el_gen} occurs when $\state = \state^*$, and thus when $\epsilon = 0$, we have,
\begin{equation}\label{eq:stationaryOne}
\frac{d}{d\delta} \big( \mathcal{J}_{\delta} \big)=  0.
\end{equation}
%or equivalently,
%$$
% \int_a^b \frac{d}{d \delta} \big(\integrand(\statezBar(t,\delta),\statezDotBar(t,\delta) ,t)\big)  dt = 0
%$$
%\int_a^b \frac{d}{d\epsilon} \bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon),t \big)\bigg] dt=  0.
%$$
%As $\statez$ is a stationary point of $\mathcal{J}$, it then directly follows that,
%\begin{equation}\label{eq:el1_appendix}
%\bigg[ \frac{d \mathcal{J}}{d \epsilon} (\statezBar(t;0),\statezDotBar(t;0) \bigg]= 0.
%\end{equation}
%This fact can be used to derive the Euler--Lagrange equations. Evaluating Eq.~\eqref{eq:el1_appendix},
%$$ \int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) \big) \bigg] = 0. $$
Using the chain rule,
$$ 
\frac{d}{d\delta} \big( \mathcal{J}_{\delta} \big)(\epsilon)= 
\int_a^b \bigg[ \frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) ,t\big) \frac{\partial \statezBar }{\partial \delta}(t,\epsilon)  + \frac{\partial \integrand}{\partial \stateyDotDiscrete } \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) ,t \big) \frac{\partial \statezDotBar}{\partial \delta} (t,\epsilon)\bigg]dt . $$
%$$\int_a^b \bigg[ \frac{\partial \integrand  }{\partial \statezBar } \frac{\partial \statezBar }{\partial \epsilon}\big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) \big)  + \frac{\partial \integrand}{\partial \statezDotBar }\frac{\partial \statezDotBar}{\partial \epsilon} \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) \big)\bigg]dt = 0. $$
%Setting $\epsilon = 0$,
%$$\int_a^b \bigg[ \frac{\partial \integrand  }{\partial \statezBar } \frac{\partial \statezBar }{\partial \epsilon}\big(\statezBar(t,0),\statezDotBar(t,0) \big)  + \frac{\partial \integrand}{\partial \statezDotBar }\frac{\partial \statezDotBar}{\partial \epsilon} \big(\statezBar(t,0),\statezDotBar(t,0) \big)\bigg]dt = 0. $$
Noting that
$$\frac{\partial \statezBar}{\partial \delta} (t,\cdot)= \variation(t), \qquad \frac{\partial \statezDotBar }{\partial \delta}(t,\cdot)= \variationDot(t),$$
where $\variationDot \equiv d \variation / d\timeDummy$, 
we have
$$
%\int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \bigg] dt 
\frac{d}{d\delta} \big( \mathcal{J}_{\delta} \big)(\epsilon)
= \int_a^b \bigg[ \frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon),t \big) \variation(t)  + \frac{\partial \integrand}{\partial \stateyDotDiscrete } \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) ,t \big) \variationDot(t) \bigg]dt. $$
%$$\int_a^b \bigg[\frac{\partial \integrand  }{\partial \overline{\genstate}} \variation + \frac{\partial \integrand}{\partial {\overline{\genstateDot}}}\variationDot \bigg]_{\epsilon = 0}dt = 0.$$
We integrate the second term by parts,
\begin{multline}\label{eq:gradient}
%	\begin{split}
%\int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon),t \big) \bigg] dt 
\frac{d}{d\delta} \big( \mathcal{J}_{\delta} \big)(\epsilon)
	=  \int_a^b  \bigg[\frac{\partial \integrand  }{\partial \stateyDiscrete }
	\big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) , t \big)
	\variation(t) - \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial
	\stateyDotDiscrete }  \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon)
	,t \big) \bigg) \variation(t) \bigg]dt + \\ 
\frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statezBar(b,\epsilon),\statezDotBar(b,\epsilon) ,b \big) \variation(b) , 
%	\end{split}
\end{multline}
where we have used $\variationArg{a} = \bz$.
Substituting Eq.~\eqref{eq:gradient} in the stationarity condition~\eqref{eq:stationaryOne} yields
\begin{equation}\label{eq:statOne}
  \int_a^b  \bigg[\frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon),t \big) \variation(t) - \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial \stateyDotDiscrete }  \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon),t \big) \bigg) \variation(t) \bigg]dt + \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statezBar(b,\epsilon),\statezDotBar(b,\epsilon),b \big) \variation(b)  = 0. 
\end{equation}
By construction, $\statez(\cdot) \equiv \statezBar(\cdot;0)$ and $\statezDot(\cdot) \equiv \statezDotBar(\cdot;0)$, comprise a stationary
point; thus, setting $\epsilon = 0$ in Eq.~\eqref{eq:statOne} yields
\begin{equation}\label{eq:euler_lagrange_analysis}
  \int_a^b  \bigg[\frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statez(t),\statezDot(t),t \big) \variation(t) - \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial \stateyDotDiscrete }  \big(\statez(t),\statezDot(t),t \big) \bigg) \variation(t) \bigg]dt + \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statez(b),\statezDot(b) ,b\big) \variation(b)  = 0. 
\end{equation}
%$$  \int_a^b \bigg[\frac{\partial \integrand  }{\partial \overline{\genstate}} \variation  - \frac{d}{dt}\frac{\partial \integrand}{\partial {\overline{\genstateDot}}}\variation \bigg]_{\epsilon = 0}dt + \bigg[\bigg[ \frac{\partial \integrand}{\partial {\overline{\genstateDot}}} \variation \bigg]_a^b\bigg]_{\epsilon = 0} = 0. $$
%At $\epsilon = 0$ we have $\overline{\genstate} = \genstate$ and, as $\variationArg{a} = 0$, we obtain,
%$$ \int_a^b \bigg(\frac{\partial \integrand  }{\partial \genstate} \variation  - \frac{\partial \integrand}{\partial {\genstateDot}}\variation \bigg)dt +  \frac{\partial \integrand}{\partial {\genstateDot}} \variation \bigg|_{t=b} = 0.$$
As $\variation$ is an arbitrary function, this equality requires
%\begin{equation}\label{eq:euler_lagrange_appendix}
%\bigg[ \frac{\partial \integrand  }{\partial \genstate} \bigg]^T  - \bigg[\frac{\partial \integrand}{\partial {\genstateDot}}\bigg]^T = \boldsymbol 0, 
%\end{equation}
\begin{equation}\label{eq:euler_lagrange_appendix}
\begin{split}
\bigg[ &\frac{\partial \integrand  }{\partial \stateyDiscrete }
	\big(\statez(t),\statezDot(t) ,t\big) \bigg]^T - \bigg[ \frac{d}{dt}\bigg(
	\frac{\partial \integrand}{\partial \stateyDotDiscrete }
	\big(\statez(t),\statezDot(t),t \big) \bigg) \bigg]^T = \bz , \\
&\statez(a) = \statez_a, \hspace{0.65 in} \bigg[ \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statez(b),\statezDot(b),b \big) \bigg]^T = \bz.
\end{split}
\end{equation}
%\genstate(a) = \genstate_a, \qquad \bigg[ \frac{\partial \integrand}{\partial {\genstateDot}} \bigg]^T_{t=b} = \bz.
%\KTC{I removed "subject to" as this is only used in an optimization-problem
%statement. Also, I don't understand the $z(a) = z_a$ conditions, or how that
%derives obviously from above. Can you clarify? Also note the inconsistent use
%of $a$, $b$, and $0$, $T$.} \EP{Yes, sorry about the [a,b], I changed this recently and obviously missed things. As for the BC, 
%the requirement that $z(a)=z_a$ comes in first from the implicit definition of $z$, which is required to satisfy this BC. Then, in defining the variation, 
%we explicitly force the variation to be zero at the IC such that all possible functions satisfy the BC. We don't, however, make any such requirement on the end point 
%condition, $\eta(b)$ can be anything. This yields the endpoint boundary condition on the gradient (i.e., second term in Eq 56.). If we had a fixed final boundary condition, then the second term in Eq. 56 would be replaced 
%with this end point condition.} 
Equation~\eqref{eq:euler_lagrange_appendix} is known as the Euler--Lagrange
equation. It states that, for ($\statez$,$\statezDot$) to define a stationary point of $\mathcal{J}$, then they must 
satisfy~\eqref{eq:euler_lagrange_appendix}. It is emphasized
that~\eqref{eq:euler_lagrange_appendix} is a necessary condition on ($\statez,\statezDot$)
to make $\mathcal{J}$ stationary, but it is not a sufficient condition. It is
additionally noted that~\eqref{eq:euler_lagrange_appendix} provides a
stationary point of $\mathcal{J}$, but the resulting stationary point could be a local minima, local maxima, or saddle point.

\section{Evaluation of gradients in the Euler--Lagrange equations for \methodAcronym\ with \spatialAcronym\ trial subspaces}\label{appendix:vector_calc}
We now derive the specific form of the Euler--Lagrange equations for the \methodAcronym\ formulation with \spatialAcronym\ trial subspaces. Without loss of generality, we present the derivation for a single window $t \in [0,T]$ with a constant basis $\basisspace$ and weighting matrix $\stweightingMat$.  
To obtain the specific form of the Euler--Lagrange equations for the \methodAcronym\ formulation, we need to evaluate the gradients in~\eqref{eq:euler_lagrange_appendix} for the integrand
\begin{align*}\label{eq:integrand_apx}
 \minintegrand & \vcentcolon
(\genstateyDiscreteArgnt{}, \genstateyDiscreteDotArgnt{} ,\timeDummy) \mapsto \frac{1}{2} \big[
\basisspace \genstateyDiscreteDotArgnt{} - \velocity(\basisspace \genstateyDiscreteArgnt{}
+ \stateIntercept,\timeDummy) \big]^T \stweightingMatArg{} \big[
\basisspace \genstateyDiscreteDotArgnt{}  - \velocity(\basisspace \genstateyDiscreteArgnt{} +
\stateIntercept,\timeDummy) \big], \\ & 
\vcentcolon \RR{\romdim} \times \RR{\romdim} \times [0,T]
 \rightarrow \RR{} .  
\end{align*}
To evaluate the gradients, we first expand $\minintegrand$:
\begin{align*}
 \minintegrand(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy)  &= \frac{1}{2} \big[ \basisspace \genstateyDiscreteDotArgnt{}  - \velocity(\basisspace \genstateyDiscreteArgnt{} + \stateIntercept,\timeDummy) \big]^T \stweightingMatArg{} \big[ \basisspace \genstateyDiscreteDotArgnt{} - \velocity(\veloargsromy) \big] \\ 
 &= \frac{1}{2}\big[\basisspace \genstateyDiscreteDotArgnt{}  \big]^T  \stweightingMatArg{}  \big[\basisspace \genstateyDiscreteDotArgnt{} \big]  - \frac{1}{2}\big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{}  \big[\basisspace \genstateyDiscreteDotArgnt{}\big] - \frac{1}{2} \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  \\ &+ \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big].
\end{align*}
Since $\stweightingMat$ is symmetric,
\begin{equation}\label{eq:int_expand}
 \minintegrand(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy)  = \frac{1}{2}\big[\basisspace \genstateyDiscreteDotArgnt{}  \big]^T  \stweightingMatArg{}  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]  -  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  + \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big].
\end{equation}
For notational purposes, we write the above as,
$$  \minintegrand(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy) =  \minintegrand_1(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy) +  \minintegrand_2(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy) +  \minintegrand_3(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy),$$
where
\begin{align*}
&\minintegrand_1(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy) =  \frac{1}{2}\big[\basisspace \genstateyDiscreteDotArgnt{}  \big]^T  \stweightingMatArg{}  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big], \\
&   \minintegrand_2(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy) = -  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big], \\
&  \minintegrand_3(\genstateyDiscrete,\genstateyDiscreteDot,\timeDummy) = \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big] . 
\end{align*}
Constructing the Euler--Lagrange equations for this functional $\minintegrand$ requires evaluating the derivatives $\frac{\partial \minintegrand}{\partial \genstateyDiscrete}$ and $\frac{\partial \minintegrand}{\partial \genstateyDiscreteDot}$. We start by evaluating $\frac{\partial \minintegrand}{\partial \genstateyDiscrete}$ and go term by term.

Starting with $\minintegrand_1(\genstateyDiscrete,\genstateyDiscreteDot)$, we see,
$$\frac{\partial \minintegrand_1 }{\partial \genstateyDiscrete} = \boldsymbol 0,$$
where it is noted that $\mathcal{I}_1$ only depends on $\genstateyDiscreteDot$. Working with the second term:
\begin{align*}
\frac{\partial \minintegrand_2}{\partial \genstateyDiscrete}  &= -\frac{\partial}{\partial \genstateyDiscrete} \bigg( \big[ \basisspace \genstateyDiscreteDot \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big] \bigg) \\ 
&= - \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{} \frac{\partial}{\partial \genstateyDiscreteArgnt{}} \bigg( \big[ \velocity(\veloargsromy) \big]  \bigg)\\
 &= -\big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{}   \frac{\partial \velocity}{\partial \stateyDiscrete}\frac{\partial \stateyDiscrete}{\partial \genstateyDiscrete} \\
& = - \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace \\
&= -[\genstateyDiscreteDotArgnt{}]^T \basisspace^T \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete}  \basisspace,
\end{align*}
where we have suppressed the arguments of the Jacobian for simplicity; e.g., formally 
$$\frac{\partial \velocity}{\partial \stateyDiscrete} : (\statewDiscrete,\timeDummy) \mapsto \frac{ \partial \velocity }{ \partial \stateyDiscrete} (\statewDiscrete, \timeDummy).$$ 
For $\minintegrand_3$,
\begin{align}\label{eq:i3sol}
\frac{\partial  \minintegrand_3}{\partial \genstateyDiscrete}  &= \frac{1}{2} \frac{\partial }{\partial \genstateyDiscrete} \bigg( \big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{}\big[ \velocity(\veloargsromy) \big] \bigg) \\  
&=  [\velocity(\veloargsromy) ]^T \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \frac{\partial \stateyDiscrete}{\partial \genstateyDiscrete} \\
 &=   [\velocity(\veloargsromy) ]^T  \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
\end{align}
This gives the final expression,
$$
 \frac{\partial \minintegrand}{\partial \boldsymbol \genstateyDiscrete} = - \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace +  [\velocity(\veloargsromy) ]^T \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
$$%
We now evaluate $\frac{\partial \minintegrand}{\partial \genstateyDiscreteDotArgnt{}}$ and again go term by term. Starting with $\minintegrand_1$,
\begin{align*}
\frac{\partial \minintegrand_1}{\partial \genstateyDiscreteDotArgnt{}} &=
\frac{1}{2} \frac{\partial}{\partial \genstateyDiscreteDotArgnt{} }\bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]  \stweightingMatArg{}  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]  \bigg) \\
&= [\genstateyDiscreteDotArgnt{}]^T \basisspace^T \stweightingMatArg{} \basisspace.
\end{align*}
Now working with the second term:
\begin{align*}
\frac{\partial \minintegrand_2}{\partial \genstateyDiscreteDotArgnt{} } &=
\frac{\partial}{\partial \genstateyDiscreteDotArgnt{}  } \bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  \bigg)\\
 &= \frac{\partial}{\partial \genstateyDiscreteDotArgnt{} } \bigg( [\genstateyDiscreteDotArgnt{}]^T \bigg) \basisspace^T  \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big] \\ 
 &= \bigg[  \basisspace^T  \stweightingMatArg{}  \velocity(\veloargsromy )   \bigg]^T \\
 &= [\velocity(\veloargsromy) ]^T \stweightingMatArg{} \basisspace.
\end{align*}
Finally, for the last term,
\begin{align*}
\frac{\partial \minintegrand_3}{\partial \genstateyDiscreteDotArgnt{} } &= \boldsymbol 0,
\end{align*}
where it is noted that $\mathcal{I}_3$ only depends on $\genstateyDiscrete$. We thus have,
$$\frac{\partial \minintegrand}{\partial \genstateyDiscreteDot } =   [\genstateyDiscreteDotArgnt{}]^T \basisspace^T \stweightingMatArg{} \basisspace -  [\velocity(\veloargsromy ) ]^T \stweightingMatArg{} \basisspace.$$
Combining all terms and evaluating at $(\genstateArg{}{t},\genstateDotArg{}{t},t)$,
\begin{multline*}
 \frac{\partial \minintegrand}{\partial \genstateyDiscrete}(\genstateArg{}{t},\genstateDotArg{}{t},t)  - \frac{d}{dt} \bigg[ \frac{\partial \minintegrand}{\partial \genstateyDiscreteDotArgnt{}} (\genstateArg{}{t},\genstateDotArg{}{t},t) \bigg] =  - \big[ \basisspace \genstateDotArg{}{t}  \big]^T \stweightingMatArg{} \big[  \frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsrom) \big] \basisspace + \\  [\velocity(\veloargsrom) ]^T \stweightingMatArg{} \big[ \frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsrom) \big] \basisspace - \frac{d}{dt} \bigg[  [\genstateDotArg{}{t} ]^T  \basisspace^T \stweightingMatArg{} \basisspace - [\velocity(\veloargsrom) ]^T \stweightingMatArg{} \basisspace \bigg]  = \boldsymbol 0.
\end{multline*}
To put this in a more recognizable form, we can pull out the common factor in the first two terms,
\begin{multline*}
- \bigg( \big[ \basisspace \genstateDotArg{}{t} \big]^T  -  [\velocity(\veloargsrom)] ^T \bigg) \stweightingMatArg{}
 \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \basisspace -  \\
\frac{d}{dt} \bigg[  [\genstateDotArg{}{t}]^T \basisspace^T \stweightingMatArg{} \basisspace -  
[\velocity(\veloargsrom) ]^T \stweightingMatArg{} \basisspace  \bigg] = \boldsymbol 0.
\end{multline*}
Taking the transpose to put into the common column major format,
$$ -  \basisspace^T \big[ \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom)  \big]^T \stweightingMatArg{} \bigg( \big[ \basisspace \genstateDotArg{}{t}\big]  -  \velocity(\veloargsrom)  \bigg) -  \frac{d}{dt} \bigg[  \basisspace^T \stweightingMatArg{} \basisspace \genstateDotArg{}{t}  - \basisspace^T \stweightingMatArg{} \velocity(\veloargsrom)   \bigg] = \bz.
 $$
We now factor the second term,
$$ -  \basisspace^T \big[ \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom)  \big]^T \stweightingMatArg{ } \bigg(  \basisspace \genstateDotArg{ }{t}  -  \velocity(\veloargsrom) \bigg) -  \basisspace^T \stweightingMatArg{ } \frac{d}{dt} \bigg[   \basisspace \genstateDotArg{}{t}  - \velocity(\veloargsrom) \bigg] = \bz. $$
Gathering terms and multiplying by negative one, the final form of the Euler--Lagrange equations are obtained,
\begin{equation}\label{eq:el_secondorder}
 \bigg[\basisspace^T \big[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \big]^T \stweightingMatArg{} + \basisspace^T \stweightingMatArg{} \frac{d}{dt} \bigg] \bigg(  \basisspace \genstateDotArg{}{t} -  \velocity(\veloargsrom) \bigg) = \bz.
\end{equation} 
This is the \methodAcronym-ROM. Note that this is a second order equation and can be written as two separate first order equations. Defining the ``costate" as,
\begin{equation*}
%\adjoint^n(t) \defeq \basisspace^T \stweightingMatArg{n} \basisspace \genstateDotArg{n}{t}  -  \basisspace^T \stweightingMatArg{n} \velocity(\veloargsromn) ,
\adjointArgnt{}  : \timeDummy \mapsto \genstateDotArg{}{\timeDummy}  -  \mass^{-1} \basisspace^T \stweightingMatArg{} \velocity(\basisspace \genstateArg{}{t} + \stateIntercept, \timeDummy) ,
\end{equation*}
we can manipulate Eq.~\eqref{eq:el_secondorder} as follows: %First, to enable hyper-reduction later on, we leverage the fact that $\stweightingMat^2 = \stweightingMat$,
%$$
% \bigg[\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn)\bigg]^T \stweightingMat^n \stweightingMat^n + \basisspace^T \stweightingMat^n \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}^n   -  \velocity(\veloargsromn) \bigg) = \bz.
%$$
First, we add and subtract the first term multiplied by $\basisspace [\massArgnt{n}]^{-1}\basisspace^T \stweightingMat$, 
\begin{multline*} 
\bigg[\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \bigg]^T \stweightingMatArg{} \bigg( \mathbf{I} - \basisspace [\massArgnt{}]^{-1} \basisspace^T\stweightingMatArg{} + \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \bigg)  + \basisspace^T \stweightingMatArg{}  \frac{d}{dt} \bigg] \\ 
\bigg(  \basisspace \genstateDotArg{}{t}   -  \velocity(\veloargsrom) \bigg) = \bz.
\end{multline*}
Pulling out the term multiplied by the positive portion of $\basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}$,
\begin{multline*} 
\bigg[\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} ( \veloargsrom) \bigg]^T \stweightingMatArg{}\bigg( \mathbf{I} - \basisspace [\massArgnt{}]^{-1}  \basisspace^T  \stweightingMatArg{} \bigg)  + \\ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsrom) \bigg]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}  \basisspace^T \stweightingMatArg{} +   \basisspace^T \stweightingMatArg{} \frac{d}{dt} \bigg] \bigg(  \basisspace \genstateDotArg{}{t}   -  \velocity(\veloargsrom) \bigg) = \bz.
\end{multline*}
Splitting into two separate terms, 
\begin{multline*}
\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \bigg]^T \stweightingMatArg{}\bigg( \mathbf{I} - \basisspace [\massArgnt{}]^{-1}  \basisspace^T \stweightingMatArg{} \bigg)  \bigg(  \basisspace \genstateDotArg{}{t}   -  \velocity(\veloargsrom) \bigg)  + \\  
\bigg[ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \bigg]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} +   \basisspace^T \stweightingMatArg{} \frac{d}{dt} \bigg] \bigg(  \basisspace \genstateDotArg{}{t}  -  \velocity(\veloargsrom) \bigg) = \bz.
\end{multline*}
Pulling $\mass^{-1} \basisspace^T \stweightingMatArg{}$ inside the parenthesis on the second term,
\begin{multline*}
\basisspace^T \bigg[\frac{\partial \velocity}{\partial \state^n}(\veloargsrom) \bigg]^T \stweightingMatArg{}\bigg( \mathbf{I} - \basisspace[\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \bigg)  \bigg(  \basisspace \genstateDotArg{}{t}   -  \velocity(\veloargsrom) \bigg)  + \\  
\bigg[ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \bigg]^T \stweightingMatArg{} \basisspace  +   \mass \frac{d}{dt} \bigg] \bigg( \genstateDotArg{}{t}  - \mass^{-1} \basisspace^T \stweightingMatArg{}  \velocity(\veloargsrom) \bigg) = \bz.
\end{multline*}
By definition, the term inside the parenthesis of the second term is $\adjointArg{}{t}$,
\begin{multline*}
\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \bigg]^T \stweightingMatArg{}\bigg( \mathbf{I} - \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \bigg)  \bigg(  \basisspace \genstateDotArg{}{t}    -  \velocity(\veloargsrom) \bigg)  +   \\ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \bigg]^T \stweightingMatArg{} \basisspace \adjointArg{}{t}  +  \massArgnt{} \frac{d}{dt} \adjointArg{}{t}= \bz.
\end{multline*}
Re-arranging,
\begin{multline*}
\mass \frac{d }{dt}\adjointArg{}{t} + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} ( \veloargsrom ) \bigg]^T \stweightingMatArg{} \basisspace  \adjointArg{}{t}  \\
= - \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom)\bigg]^T \stweightingMatArg{}\bigg( \mathbf{I} - \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \bigg)  \bigg(  \basisspace \genstateDotArg{}{t}   -  \velocity(\veloargsrom) \bigg).
\end{multline*}
We thus get the splitting
%Eq.~\ref{eq:clspg_2ord} can be split as,
%\begin{equation}
\begin{align*}\label{eq:lspg_continuous_appendix}
& \mass \frac{d}{dt}\genstateArg{}{t}  -  \basisspace^T \stweightingMatArg{} \velocity(\veloargsrom) =  \mass \adjointArg{}{t},\\
&\mass \frac{d}{dt} \adjointArg{}{t}  + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsrom) \bigg]^T \stweightingMatArg{} \basisspace \adjointArg{}{t} = \\
& -\basisspace^T \big[ \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsrom) \big] ^T \stweightingMatArg{}  \bigg( \mathbf{I} -   \basisspace [\massArgnt{}]^{-1} \basisspace^T   \stweightingMatArg{} \bigg)\bigg( \basisspace \genstateDotArg{}{t}   -   \velocity(\veloargsrom) \bigg) . 
\end{align*}
\section{Evaluation of gradients for optimal control formulation}\label{appendix:optimal_control}
When formulated as an optimal control problem of Lagrange type, the gradients of the Hamiltonian with respect to the state, controller, and costate 
need to be evaluated. This section details this evaluation. The case derivation is presented for the case with one window, for notational simplicity.

The Pontryagin Maximum Principle leverages the following Hamiltonian,
\begin{align*}
\hamiltonian \; &: \;  (\genstateyDiscreteArgnt{},\adjointDiscreteDumArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \mapsto 
 \adjointDiscreteDumArgnt{T} \bigg[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{} \bigg] +  \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \\
&: \; \RR{\romdim} \times \RR{\romdim} \times \RR{\romdim} \times [0,T] \rightarrow \RR{},
\end{align*} 
where,
\begin{align*}
 \objectiveControlArg{} &:  (\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy)
\mapsto \frac{1}{2} \bigg[ \basisspace \bigg(  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{} \bigg) -
\velocity(\veloargsromy) \bigg]^T
\stweightingMatArg{}  \\ & \hspace{1.5 in}\bigg[ \basisspace \bigg(
[\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{}
\bigg) - \velocity( \veloargsromy ) \bigg]
, \nonumber \\ & : \RR{\romdim} \times \RR{\romdim} \times [0,T] \rightarrow \RR{}.
\end{align*} 
As described in Section~\ref{sec:optimal_control}, to derive the stationary conditions of the \methodAcronym\ objective function, we require evaluating the following gradients,
$ \frac{\partial \hamiltonianArg{}}{\partial \adjointDiscreteDumArgnt{}{}}, \;   \frac{\partial \hamiltonianArg{}}{\partial \genstateyDiscreteArgnt{}{}}$, and $\frac{\partial \hamiltonianArg{}}{\partial \controllerDiscreteDumArgnt{}{}}.$  Starting with $\frac{\partial \hamiltonianArg{}}{\partial \adjointDiscreteDumArgnt{}{}}$, we have,
$$
\bigg[\frac{\partial \hamiltonianArg{}}{\partial \adjointDiscreteDumArgnt{}{}}\bigg]^T = [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{} .
$$
Next, we address $ \frac{\partial \hamiltonianArg{}}{\partial \genstateyArgnt{}{}}$. 
\begin{align*}
  \frac{\partial \hamiltonianArg{}}{\partial \genstateyDiscreteArgnt{}{}} &= \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}} \bigg[  \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{} \big] +  \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg] \\
 &= \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}} \bigg[  \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsromy
) + [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{} \big] \bigg]+  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg] \\
 &=   \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}} \bigg( \velocity(\veloargsromy)\bigg) \big] +  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg] \\
 &=   \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}  \frac{\partial \velocity}{\partial \stateyDiscrete} \frac{\partial \stateyDiscreteArgnt{}{}}{\partial  \genstateyDiscreteArgnt{}{} } \big] +  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg] \\
 &=   \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace  \big] +  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg]. 
\end{align*}
To evaluate $\frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg]$, we leverage the previous result~\eqref{eq:int_expand} and insert $\genstateyDiscreteDotArgnt{} =  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDiscreteDumArg{}{t} .$ This leads to the expression for the expanded Lagrangian,
\begin{multline*}
 \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) = \frac{1}{2}\big[\basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy)   + \basisspace [\massArgnt{}]^{-1}\controllerDiscreteDumArg{}{t} \big]^T  \stweightingMatArg{}  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy)   + \basisspace [\massArgnt{}]^{-1}\controllerDiscreteDumArg{}{t}\big] \\  -  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy)  + \basisspace [\massArgnt{}]^{-1}\controllerDiscreteDumArg{}{t} \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  + \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big].
\end{multline*}
Again for notational purposes, we split this into three terms,
$$
\objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy)  = 
\objectiveControlArg{}_1(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy)  + 
\objectiveControlArg{}_2(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy)  + 
\objectiveControlArg{}_3(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) ,$$
where
\begin{align*}
& \objectiveControlArg{}_1(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) =  \frac{1}{2}\big[\basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy)  + \basisspace [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{}{}  \big]^T  \stweightingMatArg{}  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy)   +  \basisspace [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{}{} \big],\\
& \objectiveControlArg{}_2(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) = -  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy)  + \basisspace [\massArgnt{}]^{-1} \controllerDiscreteDumArgnt{}\big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big] ,\\
& \objectiveControlArg{}_3(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) = \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big] .
\end{align*}
Evaluating the first term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_1 }{\partial \genstateyDiscreteArgnt{}{}} &= 
\frac{1}{2} \frac{\partial }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\velocity(\veloargsromy)]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) \big] \bigg) +  \\
& \qquad \frac{\partial }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) \big] \bigg) + \\
& \qquad \frac{1}{2} \frac{\partial }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \controllerDiscreteDumArgnt{}\bigg) 
,\\ 
&= 
[\velocity(\veloargsromy)]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}  \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace + 
[\controllerDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}   \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{} \frac{\partial \velocity}{\partial \statey} \basisspace , \\ 
%&= 
%[\velocity(\basisspace \genstateyArgnt{} + \stateIntercept)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  
% \frac{\partial \velocity}{\partial \statey} \basisspace + 
%[\controllerArgnt{n}]^T [\massArgnt{n}]^{-1} \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
%\stweightingMatArg{n} \frac{\partial \velocity}{\partial \statey} \basisspace , \\  
&= 
\bigg( [\velocity(\veloargsromy)]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}  + [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \bigg)\basisspace^T  \stweightingMatArg{}   \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace ,\\
&= 
[\genstateyDiscreteDotArgnt{} ]^T \basisspace^T  \stweightingMatArg{}   \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace , \\
&= 
[\basisspace \genstateyDiscreteDotArgnt{} ]^T   \stweightingMatArg{}   \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace .
\end{align*}
Now evaluating the second term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_2 }{\partial \genstateyDiscreteArgnt{}{}} &= 
-\frac{\partial  }{\partial \genstateyDiscreteArgnt{}{}}   \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  - \frac{\partial  }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T[\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  \bigg)  \\&= 
-\frac{\partial  }{\partial \genstateyDiscreteArgnt{}{}}    [\velocity(\veloargsromy)]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T 
 \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  - 
  [\controllerDiscreteDumArgnt{}]^T[\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace 
\\
&= 
- 2[\velocity(\veloargsromy)]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}  
 \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace - 
 [\controllerDiscreteDumArgnt{}]^T[\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace, \\
&= - \bigg[ 2[\velocity(\veloargsromy)]^T  \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}  + [\controllerDiscreteDumArgnt{}]^T  [\massArgnt{}]^{-1} \bigg] \basisspace^T \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace ,\\
&= -\bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} ]^T  + \velocity(\veloargsromy)^T  \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T \bigg) \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace. 
\end{align*}
Next, we can use the result~\eqref{eq:i3sol} to have,
$$ \frac{\partial  \objectiveControlArg{}_3 }{\partial \genstateyDiscreteArgnt{}{}} =
  [\velocity(\veloargsromy) ]^T  \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
$$
Thus we have
\begin{align*}
\frac{\partial \objectiveControlArg{}}{\partial \genstateyDiscreteArgnt{}} &= 
[\basisspace \genstateyDiscreteDotArgnt{} ]^T   \stweightingMatArg{}   \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace -
\bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} ]^T  + \velocity(\veloargsromy)]^T  \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T  \bigg) \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace + 
 [\velocity(\veloargsromy) ]^T  \stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace, \\
&= 
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)  \stweightingMatArg{}   \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace -
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)  \stweightingMatArg{}
\frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace, \\ 
&= 
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)   \bigg( \stweightingMatArg{} \basisspace  [\massArgnt{}]^{-1}\basisspace^T - \mathbf{I} \bigg)
\stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace, 
\end{align*}
such that,
\begin{equation*}
\frac{\partial \hamiltonianArg{}}{\partial \genstateyDiscreteArgnt{}{}} = 
  \adjointDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace  \big]  + 
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)   \bigg( \stweightingMatArg{} \basisspace  [\massArgnt{}]^{-1}\basisspace^T - \mathbf{I} \bigg)
\stweightingMatArg{} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
\end{equation*}
Equivalently we write
\begin{equation*}
\bigg[ \frac{\partial \hamiltonianArg{}}{\partial \genstateyDiscreteArgnt{}{}} \bigg]^T =  \basisspace^T \big[  \frac{\partial \velocity}{\partial \stateyDiscrete} \big]^T \stweightingMatArg{}  \basisspace [\massArgnt{}]^{-1}  \adjointDiscreteDumArgnt{} + 
 \basisspace^T \big[  \frac{\partial \velocity}{\partial \stateyDiscrete} \big]^T 
  \stweightingMatArg{} \bigg(  \basisspace  [\massArgnt{}]^{-1}\basisspace^T  \stweightingMatArg{}- \mathbf{I} \bigg)
 \bigg( \basisspace \genstateyDiscreteDotArgnt{} - \velocity(\veloargsromy) \bigg). 
\end{equation*}
Next, we evaluate $\frac{\partial \hamiltonianArg{}}{\partial \controllerDumArgnt{}}$:
\begin{align*}
  \frac{\partial \hamiltonianArg{}}{\partial \controllerDumArgnt{}{}} &= \frac{\partial}{\partial \controllerDumArgnt{}{}} \bigg[  \adjointDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDumArgnt{} \big] +  \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg] \\
 &= \frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}} \bigg[  \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsromy) + [\massArgnt{}]^{-1}\controllerDiscreteDumArgnt{} \big] \bigg]+  \frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg] \\
 &= [\adjointDiscreteDumArgnt{}]^T[\massArgnt{}]^{-1} + \frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg]. 
\end{align*}
To evaluate $\frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDummy) \bigg]$, we again 
go term by term. Starting with the first term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_1 }{\partial \genstateyDiscreteArgnt{}{}} &= 
\frac{1}{2} \frac{\partial }{\partial \controllerDiscreteDumArgnt{}} \bigg( [\velocity(\veloargsromy)]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) \big] \bigg) + \\
& \qquad \frac{\partial }{\partial \controllerDiscreteDumArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) \big] \bigg) + \\
& \qquad \frac{1}{2} \frac{\partial }{\partial \controllerDiscreteDumArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \controllerDiscreteDumArgnt{}\bigg) \\
&= \bigg(  [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{}  \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T                      
\stweightingMatArg{}  \velocity(\veloargsromy) \big] \bigg)^T +   [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} \\
&= \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}\basisspace^T  \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} +   [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}. 
\end{align*}
Moving on to the second term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_2 }{\partial \controllerDiscreteDumArgnt{}{}} &= 
-\frac{\partial  }{\partial \controllerDiscreteDumArgnt{}{}}   \big[ \basisspace  [\massArgnt{}]^{-1}\basisspace^T
\stweightingMatArg{}  \velocity(\veloargsromy) \big]^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  - \frac{\partial  }{\partial \controllerDiscreteDumArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T[\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \big[ \velocity(\veloargsromy) \big]  \bigg)  \\
&=- \bigg[ [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \velocity(\basisspace \genstateyDiscrete + \stateIntercept) \bigg]^T \\
&= - [\velocity(\veloargsromy) ]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}. 
\end{align*}
For the third term we have simply,
$$ \frac{\partial  \objectiveControlArg{}_3 }{\partial \controllerDiscreteDumArgnt{}{}} = \bz .$$
Thus,
\begin{align*}
\frac{\partial \hamiltonianArg{}}{\partial \controllerDiscreteDumArgnt{}} &= 
[\adjointDiscreteDumArgnt{}]^T[\massArgnt{}]^{-1}  +  \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}\basisspace^T  \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} + \\
& \qquad   [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \basisspace^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1} - [\velocity(\veloargsromy) ]^T \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}, \\
& = \bigg( [\adjointDiscreteDumArgnt{}]^T - [\velocity(\veloargsromy) ]^T \stweightingMatArg{} \basisspace \bigg) [\massArgnt{}]^{-1}  + \bigg(  \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{} \basisspace + [\controllerDiscreteDumArgnt{}]^T   \bigg) [\massArgnt{}]^{-1}\basisspace^T  \stweightingMatArg{} \basisspace [\massArgnt{}]^{-1}  \\
& = \bigg( [\adjointDiscreteDumArgnt{}]^T - [\velocity(\veloargsromy) ]^T \stweightingMatArg{} \basisspace \bigg) [\massArgnt{}]^{-1}  + \bigg(  \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{} \basisspace + [\controllerDiscreteDumArgnt{}]^T   \bigg) [\massArgnt{}]^{-1}  \\
& =  [\adjointDiscreteDumArgnt{}]^T  [\massArgnt{}]^{-1}  +  [\controllerDiscreteDumArgnt{}]^T   [\massArgnt{}]^{-1}  
\end{align*}
Or, equivalently,
\begin{equation*}
\frac{\partial \hamiltonianArg{}}{\partial \controllerDiscreteDumArgnt{}} = 
  [\massArgnt{}]^{-1} [\adjointDiscreteDumArgnt{} +  \controllerDiscreteDumArgnt{}]. 
\end{equation*}
Evaluating at $(\genstateArg{}{t},\genstateDotArg{}{t},t$), the gradients in the Pontryagin Maximum Principle yield
\begin{align*}%\label{eq:hamiltonian_sysa}
&\frac{d}{dt}\genstateArg{}{t} =  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\veloargsrom) + [\massArgnt{}]^{-1}\controllerArg{}{t}\\ 
&\frac{d }{dt} \adjointArg{}{t}  + \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{}  \basisspace [\massArgnt{}]^{-1}  \adjointArg{}{t} = 
 - \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{} 
\bigg(  \basisspace  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{} - \mathbf{I} \bigg)
 \bigg( \basisspace \genstateDotArg{}{t} - \velocity(\basisspace \genstateArg{}{t} + \stateIntercept) \bigg) \\ 
& \adjointArg{}{t}= -\controllerArg{}{t}.
\end{align*}
This can be written equivalently as
\begin{align*}%\label{eq:hamiltonian_sys}
&\frac{d}{dt}\genstateArg{}{t} =  [\massArgnt{}]^{-1}\basisspace^T \stweightingMatArg{}\velocity(\basisspace \genstateArg{}{t} + \stateIntercept) + [\massArgnt{}]^{-1}\controllerArg{}{t} \\ 
&\frac{d}{dt} \controllerArg{}{t}  + \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{}  \basisspace [\massArgnt{}]^{-1}  \controllerArg{}{t} = 
 - \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{} 
\bigg( \mathbf{I} -   \basisspace  [\massArgnt{}]^{-1}\basisspace^T \bigg)
\stweightingMatArg{} \bigg( \basisspace \genstateDotArg{}{t} - \velocity(\basisspace \genstateArg{}{t} + \stateIntercept) \bigg). 
\end{align*}


\begin{comment}

====================
To do this, first write the above as,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T + \basisspace \basisspace^T \bigg)  \stweightingMat^T + \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
Re-arranging gives,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg]  +  \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T  \basisspace   +  \frac{d}{dt} \bigg] \bigg( \basisspace^T \stweightingMat \basisspace \dot{\genstate}   -  \basisspace^T \stweightingMat  \velocity \bigg) = 0. $$
Defining the adjoint as,
\begin{equation}
\basisspace^T \stweightingMat \basisspace \frac{d \genstate}{dt}  -  \basisspace^T \stweightingMat \velocity =  \boldsymbol \lambda , \qquad \genstate(t=0) = \genstate_0,
%\basisspace \frac{d \genstate}{dt}  -   \velocity =  \adjoint  , \qquad \genstate(t=0) = \genstate_0
\end{equation}
we then get the auxiliary adjoint equation,
\begin{equation}
%\basisspace^T \stweightingMat^T \frac{d}{dt} \adjoint  + \basisspace^T \frac{\partial \velocity}{\partial \genstate}^T \stweightingMat^T \adjoint = 0, \qquad \basisspace^T \stweightingMat \adjoint(t=T) = 0
 \frac{d}{dt} \adjoint  + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \state} \bigg]^T \basisspace \adjoint = -\bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg] , \qquad \adjoint(t=T) = 0.
\end{equation}
\end{comment}
\begin{comment}
=================================

%We start $ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate}$. We go term by term and note the useful link: \url{https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions}
%First term
\begin{enumerate}
\item First term: Only depends on $\dot{\genstate}$, term is zero
\item Second term: $$\frac{\partial}{\partial \genstate} \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)=  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \frac{\partial}{\partial \genstate} \bigg( \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg)$$
$$ = \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat   \frac{\partial \boldsymbol f(\state)}{\partial \state}\frac{\partial \state}{\partial \genstate} $$
$$ =  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace $$
%$${\color{red} \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big] \bigg) = \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \bigg) \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \basisspace.$$
%$${\color{red} ?  = \big[ \frac{\partial \velocity}{\partial \state}\frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
%$${\color{red} ?  = \big[ \basisspace \frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
\item Third term: 
$$\frac{1}{2} \frac{\partial }{\partial \genstate} \bigg( \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)  =  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ =   \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisspace$$
\end{enumerate}
This gives us,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} = - \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace +  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace $$


Now we work on $\frac{\partial \minintegrand}{\partial \dot{ \genstate } }$:
\begin{enumerate}
\item First term:
$$\frac{1}{2} \frac{\partial}{\partial \dot{\genstate}}\bigg( \big[\dot{\genstate}^T  \basisspace^T \big]  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  \bigg) =  \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace$$
\item Second term:
$$\frac{\partial}{\partial \dot{\genstate} } \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg) = \frac{\partial}{\partial \dot{\genstate} } \bigg( \dot{\genstate}^T \bigg) \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  $$
$$ = \bigg[  \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg]^T $$
$$ = \velocity^T \stweightingMat^T \basisspace$$
\end{enumerate}
This gives,
$$\frac{\partial \minintegrand}{\partial \dot{ \genstate } } =   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace -  \velocity^T \stweightingMat^T \basisspace.$$
%\end{comment}

\begin{comment}
Let $\stweightingMat = \mathbf{I}$,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} - \frac{d}{dt} \bigg[ \frac{\partial \boldsymbol F}{\partial \dot{\genstate}} \bigg] =   \bigg( -\big[ \basisspace \dot{\genstate} \big]^T  +  \velocity^T \bigg) \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T  - \velocity^T \basisspace \bigg] $$
Transposing,
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[   \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[ \basisspace^T \basisspace  \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ =    -\basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( \basisspace \dot{\genstate}  -  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ = -\bigg[ \basisspace^T \frac{d}{dt} + \basisspace^T \big[ \frac{\partial \velocity}{\partial \state} \big]^T  \bigg] \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$ 
Adjoint style
$$ =   \basisspace^T  \frac{d }{dt}\lambda  +   \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \lambda  $$
$$ \basisspace  \dot{\genstate}  -\velocity = \lambda$$
%Gathering terms



\end{comment}
%\begin{comment}

\begin{comment}

\subsection{Time-varying spacetime subspace}
We have $\state(t)= \basisst(t) \genstate + \state_0$. Note that, in this case, the generalized coordinates are stationary in time, $\dot{\genstate} = \mathbf{0}$. Thus we only need to compute $\frac{\partial \minintegrand}{\partial \genstate}$. Going term by term:

\begin{enumerate}
\item First term: 
$$\frac{1}{2} \frac{\partial}{\partial \genstate} \bigg(\big[\frac{\partial \decoder}{\partial t}  \big]^T \stweightingMat  \big[\frac{\partial \decoder}{\partial t} \big]\bigg) = 
\frac{\partial}{\partial \genstate} \bigg(\big[\frac{\partial \basisst}{\partial t} \genstate \big]^T \stweightingMat  \big[\frac{\partial \basisst}{\partial t} \genstate \big]\bigg) $$
$$
= \genstate^T \dot{\basisst}^T \stweightingMat \dot{ \basisst }
$$
\item Second term: $$\frac{\partial}{\partial \genstate} \bigg( \big[ \frac{\partial \decoder}{\partial t} \big]^T \stweightingMat \big[ \velocity(\decoder,t;\param) \big] \bigg)= \frac{\partial}{\partial \genstate} \bigg( \big[ \frac{\partial \basisst}{\partial t} \genstate  \big]^T \stweightingMat  \big[ \velocity(\decoder,t;\param) \big]  \bigg)$$
$$ = \big[ \frac{\partial \basisst}{\partial t} \genstate  \big]^T \stweightingMat \frac{\partial}{\partial \genstate}\bigg( \big[ \velocity(\decoder,t;\param) \big] \bigg) +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T \frac{\partial}{\partial \genstate} \bigg( \big[ \frac{\partial \basisst}{\partial t} \genstate  \big] \bigg)$$
$$ = \big[ \dot{\basisst} \genstate  \big]^T \stweightingMat \frac{\partial \velocity}{\partial \decoder} \frac{\partial \decoder}{\partial \genstate} +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T  \dot{ \basisst}$$
$$ = \big[ \dot{\basisst} \genstate  \big]^T \stweightingMat \frac{\partial \velocity}{\partial \decoder} \basisst +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T  \dot{ \basisst}$$
\item Third term: 
$$\frac{1}{2} \frac{\partial }{\partial \genstate} \bigg( \big[\velocity(\decoder,t;\param) \big]^T \stweightingMat \big[ \velocity(\decoder,t;\param) \big] \bigg)  =  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ =   \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisst$$
Putting it all together we get,
$$ \genstate^T \dot{\basisst}^T \stweightingMat \dot{\basisst} - \bigg(  \big[ \dot{\basisst} \genstate  \big]^T \stweightingMat \frac{\partial \velocity}{\partial \decoder} \basisst +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T  \dot{ \basisst} \bigg) + \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisst = \mathbf{0}$$
Transposing, we obtain the Euler--Lagrange equations for a space-time basis
$$ \basisst^T \stweightingMat^T \dot{\basisst} \genstate - \bigg( \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \dot{\basisst} \genstate + \dot{\basisst}^T \stweightingMat \velocity  \bigg) + \basisst^T  \big[ \frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \velocity  = \mathbf{0}, \qquad \state(t=0) = \state_0.$$
Note that the additional boundary condition is automatically satisfied.
$$ \dot{\basisst}^T \stweightingMat^T \bigg( \dot{\basisst} \genstate -    \velocity \bigg)   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \dot{\basisst} \genstate  + \basisst^T  \big[ \frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \velocity  = \mathbf{0}, \qquad \state(t=0) = \state_0.$$
$$ \dot{\basisst}^T \stweightingMat^T \bigg( \dot{\basisst} \genstate -    \velocity \bigg)   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \bigg( \dot{\basisst} \genstate  -  \velocity \bigg) = \mathbf{0} $$
$$ \bigg[ \dot{\basisst}^T \stweightingMat^T  - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \bigg]  \bigg( \dot{\basisst} \genstate -    \velocity \bigg) = \mathbf{0}$$

Note that we can write this as two systems. To put it in the best form, write the above as,
$$ \dot{\basisst}^T \bigg( \mathbf{I} -  \basisst \basisst^T  +  \basisst \basisst^T \bigg)  \stweightingMat^T \bigg( \dot{\basisst} \genstate -    \velocity \bigg)   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \bigg( \mathbf{I} -  \basisst \basisst^T  +  \basisst \basisst^T \bigg)\stweightingMat^T \bigg( \dot{\basisst} \genstate  -  \velocity \bigg) = \mathbf{0} $$
Defining,
$$  \basisst^T \stweightingMat^T \dot{\basisst} \genstate  -  \basisst^T \stweightingMat^T \velocity = \boldsymbol \lambda,$$
we have,
$$ \bigg[ \dot{\basisst}^T   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \bigg] \basisst \adjoint  = -\bigg[\dot{\basisst}^T   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \bigg]  \bigg( \mathbf{I} -  \basisst \basisst^T  \bigg)\stweightingMat^T \bigg( \dot{\basisst} \genstate  -  \velocity \bigg)$$

%The above can be written as two systems,
%$$  \dot{\basisst}^T \stweightingMat^T \dot{\basisst} \genstate - \dot{\basisst}^T \stweightingMat \velocity  = \boldsymbol \lambda $$
%$$  \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \dot{\basisst} \genstate  - \basisst^T  \big[ \frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \velocity  = \lambda$$


\end{enumerate}
\end{comment}

\begin{comment}
\textbf{Linear subspace with full temporal support}:
For a linear subspace with full temporal support, we have $\state = \basisspace \genstate + \state_0$
$ \frac{\partial \state}{\partial \genstate} = \basisspace.$
We have,
$$\minintegrand = \frac{1}{2} \big[ \basisspace \dot{\genstate} \big]^T  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  -  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] 
  + \frac{1}{2} \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]$$
Write this as,
$$\minintegrand = \minintegrand_1 + \minintegrand_2 + \minintegrand_3 $$

%other form
%$${\color{red} = \big[ \basisspace \dot{\genstate} \big]^T  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  - 2 \big[ \velocity(\state(\genstate),t;\param) \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big] + \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] }$$

First we compute $ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate}$. We go term by term and note the useful link: \url{https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions}
%First term
\begin{enumerate}
\item First term: Only depends on $\dot{\genstate}$, term is zero
\item Second term: $$\frac{\partial}{\partial \genstate} \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)=  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \frac{\partial}{\partial \genstate} \bigg( \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg)$$
$$ = \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat   \frac{\partial \boldsymbol f(\state)}{\partial \state}\frac{\partial \state}{\partial \genstate} $$
$$ =  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace $$
%$${\color{red} \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big] \bigg) = \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \bigg) \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \basisspace.$$
%$${\color{red} ?  = \big[ \frac{\partial \velocity}{\partial \state}\frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
%$${\color{red} ?  = \big[ \basisspace \frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
\item Third term: 
$$\frac{1}{2} \frac{\partial }{\partial \genstate} \bigg( \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)  =  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ =   \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisspace$$
\end{enumerate}
This gives us,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} = - \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace +  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace $$
Now we work on $\frac{\partial \minintegrand}{\partial \dot{ \genstate } }$:
\begin{enumerate}
\item First term:
$$\frac{1}{2} \frac{\partial}{\partial \dot{\genstate}}\bigg( \big[\dot{\genstate}^T  \basisspace^T \big]  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  \bigg) =  \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace$$
\item Second term:
$$\frac{\partial}{\partial \dot{\genstate} } \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg) = \frac{\partial}{\partial \dot{\genstate} } \bigg( \dot{\genstate}^T \bigg) \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  $$
$$ = \bigg[  \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg]^T $$
$$ = \velocity^T \stweightingMat^T \basisspace$$
\end{enumerate}
This gives,
$$\frac{\partial \minintegrand}{\partial \dot{ \genstate } } =   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace -  \velocity^T \stweightingMat^T \basisspace.$$
Putting it all together, the Euler--Lagrange equation reads,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} - \frac{d}{dt} \bigg[ \frac{\partial \minintegrand}{\partial \dot{\genstate}} \bigg] =  - \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace +  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace - \velocity^T \stweightingMat^T \basisspace \bigg]  = 0$$
Factoring the first term,
$$ - \bigg( \big[ \basisspace \dot{\genstate} \big]^T  -  \velocity^T \bigg) \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace -  \velocity^T \stweightingMat^T \basisspace  \bigg] = 0$$
Taking the transpose,
$$ -  \basisspace^T \frac{\partial \velocity}{\partial \state}^T \stweightingMat^T \bigg( \big[ \basisspace \dot{\genstate} \big]  -  \velocity \bigg) -  \frac{d}{dt} \bigg[  \basisspace^T \stweightingMat^T \basisspace \dot{\genstate} - \basisspace^T \stweightingMat \velocity  \bigg] = 0.
 $$
Noting that $\stweightingMat = \stweightingMat^T$, we factor the second term,
$$ -  \basisspace^T \frac{\partial \velocity}{\partial \state}^T \stweightingMat^T \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) -  \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg[   \basisspace \dot{\genstate} - \velocity  \bigg] = 0. $$
Gathering terms and multiplying by negative one,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \stweightingMat^T + \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
%Equivalently,
%$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T  \basisspace \basisspace^T \stweightingMat^T + \basisspace^T  \basisspace \basisspace^T \stweightingMat^T  \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
%Pulling in the $\stweightingMat^T \basisspace^T$,
%$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \basisspace  +  \frac{d}{dt} \bigg] \bigg(  \basisspace^T \stweightingMat \basisspace \dot{\genstate}   -  \basisspace^T \stweightingMat \velocity \bigg) = 0. $$

This is the time-continuous LSPG ROM. Note that this is a second order equation and can be written as two separate first order equations. To do this, first write the above as,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T + \basisspace \basisspace^T \bigg)  \stweightingMat^T + \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
Re-arranging gives,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg]  +  \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T  \basisspace   +  \frac{d}{dt} \bigg] \bigg( \basisspace^T \stweightingMat \basisspace \dot{\genstate}   -  \basisspace^T \stweightingMat  \velocity \bigg) = 0. $$
Defining the adjoint as,
\begin{equation}
\basisspace^T \stweightingMat \basisspace \frac{d \genstate}{dt}  -  \basisspace^T \stweightingMat \velocity =  \boldsymbol \lambda , \qquad \genstate(t=0) = \genstate_0,
%\basisspace \frac{d \genstate}{dt}  -   \velocity =  \adjoint  , \qquad \genstate(t=0) = \genstate_0
\end{equation}
we then get the auxiliary adjoint equation,
\begin{equation}
%\basisspace^T \stweightingMat^T \frac{d}{dt} \adjoint  + \basisspace^T \frac{\partial \velocity}{\partial \genstate}^T \stweightingMat^T \adjoint = 0, \qquad \basisspace^T \stweightingMat \adjoint(t=T) = 0
 \frac{d}{dt} \adjoint  + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \state} \bigg]^T \basisspace \adjoint = -\bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg] , \qquad \adjoint(t=T) = 0.
\end{equation}

\end{comment}

\begin{comment}
Let $\stweightingMat = \mathbf{I}$,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} - \frac{d}{dt} \bigg[ \frac{\partial \boldsymbol F}{\partial \dot{\genstate}} \bigg] =   \bigg( -\big[ \basisspace \dot{\genstate} \big]^T  +  \velocity^T \bigg) \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T  - \velocity^T \basisspace \bigg] $$
Transposing,
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[   \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[ \basisspace^T \basisspace  \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ =    -\basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( \basisspace \dot{\genstate}  -  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ = -\bigg[ \basisspace^T \frac{d}{dt} + \basisspace^T \big[ \frac{\partial \velocity}{\partial \state} \big]^T  \bigg] \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$ 
Adjoint style
$$ =   \basisspace^T  \frac{d }{dt}\lambda  +   \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \lambda  $$
$$ \basisspace  \dot{\genstate}  -\velocity = \lambda$$
%Gathering terms
\end{comment}



\end{appendices}

\bibliographystyle{siam}
\bibliography{refs}
\end{document}


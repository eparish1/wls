%\documentclass{article}
\documentclass[3p,computermodern,10pt]{elsarticle}
\input{packages}
\input{commands}
\input{commands_cwst}
%\input{title}
%\begin{frontmatter}
%\title{Continuous Least-Squares Petrov-Galerkin for Model Reduction}
%\author{Eric J. Parish}
\begin{document}
\begin{frontmatter}

%\title{Model Reduction via \methodName}
%\title{Windowed least--squares reduced-order models for dynamical systems}
\title{Windowed least-squares model reduction for dynamical systems}
%\title{The windowed least-squares framework for model reduction of dynamical systems}

\author[a]{Eric J. Parish and Kevin T. Carlberg}
%\ead{ejparis@sandia.gov}

\address[a]{Sandia National Laboratories,  Livermore, CA}
\begin{abstract}
This work proposes a \methodNameLower\ (\methodAcronym) model-reduction
	formulation for dynamical systems. The proposed approach sequentially minimizes the
	time-continuous full-order-model residual over a low-dimensional space--time trial
	subspace defined over an arbitrarily defined time window. The approach comprises a generalization 
of existing model reduction approaches, as particular instances of
  the methodology associate with Galerkin,
	least-squares Petrov--Galerkin (LSPG), and space–time LSPG projection. In
	addition, the approach
	addresses key deficiencies in existing model-reduction
	techniques, e.g., the dependence of (space--time) LSPG projection on the
	time discretization and the exponential growth in time exhibited by \textit{a posteriori}
	error bounds for Galerkin and LSPG projection.  We consider two types of
	space--time trial
	subspaces within the proposed framework: one that reduces only the spatial
	dimension of the full-order model, and one that reduces both the spatial and
	temporal dimensions of the full-order model.  For each type of trial
	subspace, we consider two different solution techniques: direct (i.e.,
	discretize-then-optimize) and indirect (i.e., optimize-then-discretize).
	Numerical experiments conducted using the trial subspaces characterized by spatial
	dimension reduction demonstrate that the \methodAcronym\
	approach can yield more accurate solutions that better satisfy the governing equations than
	Galerkin and LSPG projection. 

\begin{comment}
This work proposes a \methodNameLower\ (\methodAcronym) model-reduction
	formulation for dynamical systems.  
The proposed approach minimizes the
	time-continuous full-order-model residual over a low-dimensional 
	trial subspace across a sequence of time windows. % that decompose the time
We consider two types of trial subspaces
	within this framework:
	one that reduces the spatial dimension of the full-order model, and one that
	reduces both the spatial and temporal dimensions of the full-order model. 
We consider two methods to solve the residual-minimization problems: direct
	(i.e., discretize-then-optimize) and indirect (i.e.,
	optimize-then-discretize).
The approach addresses key deficiencies in a variety 
of existing model-reduction approaches; e.g., the time-step and time-scheme dependence of least-squares Petrov--Galerkin (LSPG), the exponentially growing error bounds present in Galerkin and LSPG, and the offline cost of space--time LSPG. 
Various limiting cases of the proposed method yield existing reduced-order modeling techniques, namely Galerkin, Least-Squares Petrov–Galerkin, and Space–Time Least-Squares Petrov– Galerkin.
Numerical experiments on the compressible Euler and Navier--Stokes equations demonstrate that, at an increased computational cost, the \methodAcronym\ formulation can yield more accurate and physically relevant solutions than the Galerkin and Least-Squares Petrov--Galerkin approaches. 
\end{comment}
\end{abstract}
\end{frontmatter}


%\maketitle
\section{Introduction}

Simulating parameterized dynamical systems arises in many applications across
science and engineering, including design, control, uncertainty
quantification, and epidemiology, for example. In many contexts, executing a dynamical-system
simulation at a single parameter instance---which entails the numerical
integration of a system of ordinary differential equations (ODEs)---incurs an
extremely large computational cost.  This occurs, for example, when the
state-space dimension is large (e.g., due to fine spatial resolution when
discretizing a partial differential equation) and/or when the number of time
instances is large (e.g., due to time-step limitations incurred by stability
or accuracy considerations).  When the application is time critical or many
query in nature, analysts must replace such large-scale parameterized
dynamical-system models (which we refer to as the full-order model) with a low-cost approximation that makes the
application tractable.


Projection-based reduced-order models (ROMs) comprise one such approximation
strategy. First, these techniques execute a computationally expensive
\textit{offline} stage that computes a low-dimensional \textit{trial subspace} on
which the dynamical-system state can be well approximated (e.g., by computing
state ``snapshots'' at different time and parameter instances, by solving
Lyapunov equations). Second, these methods execute an inexpensive
\textit{online} stage during which they compute approximations to the
dynamical-system trajectory that reside on this trial subspace via a
projection process.
 
Model reduction for linear-time-invariant systems (and other well structured
dynamical systems) is quite mature
\cite{wilcox_benner_rev,moore,roberts,GugercinIRKA}, as system-theoretic properties (e.g., controllability,
observability, asymptotic stability, $\mathcal H_2$-optimality) can be readily
quantified and accounted for in the projection process; this often results in
reduced-order models that inherit such important properties.  The primary
challenge in
developing reduced-order models for general nonlinear dynamical systems is
that such properties are difficult to assess quantitatively. As a result, it
is difficult to develop reduced-order models that preserve important
dynamical-system properties, which often results in methods that yield
trajectories that are
inaccurate, unstable, or violate physical properties.  To address this, researchers have pursued several directions that
aim to imbue reduced-order models for nonlinear
dynamical systems with properties that can improve robustness and accuracy.
These
efforts include residual-minimizing
projection that equips the ROM solution with a notion of optimality~\cite{carlberg_lspg,carlberg_gnat,legresley_1,legresley_2,legresley_3,bui_resmin_steady,bui_unsteady,rovas_thesis,carlberg_thesis,bui_thesis,l1};
space--time
projection that leads to error bounds that grow slowly in time~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC,benner_st};
``energy-based'' inner
products that ensure non-increasing entropy in the ROM
solution~\cite{rowley_pod_energyproj,Kalashnikova_sand2014,chan2019entropy};
basis-adaptation methods that improve the ROM's accuracy \textit{a
posteriori}~\cite{carlberg_hadaptation,adeim_peherstorfer,etter2019online}, stabilizing subspace rotations that account for truncated modes \textit{a priori}~\cite{basis_rotation}, structure-preserving
methods that enforce conservation \cite{carlberg_conservative_rom} or
(port-)Hamiltonian/Lagrangian structure
\cite{LALL2003304,carlberg2012spd,structurePreserveBeattie,chaturantabut2016structure,farhat2014dimensional} in the ROM; and subgrid-scale
modeling methods that aim to improve accuracy by addressing the closure
problem~\cite{san_iliescu_geostrophic,iliescu_pod_eddyviscosity,iliescu_vms_pod_ns,Bergmann_pod_vms,iliescu_ciazzo_residual_rom,Wang_ROM_thesis,wentland_apg,Wang:269133,San2018}.
We note that these techniques are often not mutually exclusive.
%Residual-minimizing projection methods are the most relevant to the current work and comprise the focus of the following review.
Residual-minimizing and space--time projection are the most relevant classes of
methods for the current work and comprise the focus of the following review.


Residual-minimization methods in model reduction compute the solution within a
low-dimensional trial subspace that minimizes the full-order-model residual.
Researchers have developed such residual-minimizing model-reduction methods
for both static systems (i.e., systems without time-dependence)
\cite{legresley_1,legresley_2,legresley_3,bui_resmin_steady,rovas_thesis,carlberg_thesis,bui_thesis}
and dynamical
systems~\cite{bui_thesis,bui_unsteady,carlberg_thesis,carlberg_gnat,carlberg_lspg,carlberg_lspg_v_galerkin}.
In the latter category,
Refs.~\cite{bui_thesis,bui_unsteady,carlberg_thesis,carlberg_gnat,carlberg_lspg}
formulated the residual minimization problem for dynamical systems by
sequentially minimizing the \textit{time-discrete} full-order-model residual
(i.e., the residual arising after applying time discretization) at each time
instance on the time-discretization grid. This formulation is often referred
to as the \textit{least--squares Petrov--Galerkin} (LSPG) method.
Ref.~\cite{carlberg_lspg_v_galerkin} performed detailed analyses of this
formulation and examined its connections with Galerkin projection. Critically,
this work demonstrated that (1) under certain conditions, LSPG can be derived
from a Petrov--Galerkin projection executed on the time-continuous
full-order-model residual, and (2) LSPG and Galerkin projection are equivalent
in the limit as the time step goes to zero (i.e., Galerkin projection
minimizes the time-instantaneous full-order-model residual). Numerous
numerical experiments have demonstrated that LSPG projection often yields more
accurate and stable solutions than Galerkin
projection~\cite{bui_thesis,carlberg_lspg_v_galerkin,carlberg_gnat,carlberg_thesis,parish_apg}.
The common intuitive explanation for this improved performance is that, by
minimizing the full-order-model residual over a finite time window (rather
than time instantaneously), LSPG projection computes solutions that are more
accurate over a larger part of the trajectory as compared to Galerkin projection.

However, the LSPG approach has several notable shortcomings. First, LSPG
performance exhibits a complex dependence on the time discretization. In
particular, changing
the time step modifies both the
time window over which LSPG minimizes the residual as well as the
time-discretization error of the full-order model on which LSPG is based. 
In particular, because LSPG and Galerkin projection are equivalent in the
limit as the time step goes to zero, LSPG accuracy approaches the (poor)
accuracy of Galerkin as the time step shrinks, but this also has the effect of
reducing the time-dicretization error of the full-order model.
As a
consequence, LSPG often yields the smallest error for an intermediate value of the
time step (see, e.g., Ref.~\cite[Figure 9]{carlberg_lspg_v_galerkin}); there
is no known way to compute this optimal time step \textit{a priori}.
Second, as the LSPG approach performs sequential residual minimization in
time, its \textit{a posteriori} error bounds grow exponentially in time
\cite{carlberg_lspg_v_galerkin}, and it is not equipped with any notion of
optimality over the entire time domain of interest. As a result, LSPG is not
equipped with \textit{a priori} guarantees of accuracy or stability, even for
linear time-invariant systems~\cite{bui_thesis}.

Researchers have pursued the development of space--time
residual-minimization
projection~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC} to
address the issues incurred by sequential residual minimization in time.
Space--time residual-minimization approaches compute the space--time
trajectory within a low-dimensional space--time trial subspace (spanned by
a small number of space--time basis vectors) that minimizes the full-order-model
residual over the entire time domain.
We note that space--time residual minimization yields a fundamentlaly
different model than sequential residual minimization in time. In particular, space--time
residual minimization associates with an algebraic system of equations defined
over all space and time, whose solution comprises a \text{fixed} vector of
(space--time) generalized coordinate; on the other hand, sequential residual
minimization in time (often) associates with a system of ODEs whose solution
comprises a \textit{time-dependent} vector of (spatial) generalized
coordinates.
As a result, space--time residual minimization yields models that are equipped with \textit{a priori} error bounds that grow more slowly
in time, and the resulting \textit{trajectory} exhibits a notion of
optimality; this implies that the solution exhibits a notion of optimality
over the entire time domain. However, these techniques also suffer from several
limitations. First, the computational cost of solving the algebraic system arising from space--time
projection scales cubically with the number of space--time degrees of freedom;
in contrast, the computational cost incurred by standard
spatial-projection-based ROMs is linear in the number of temporal degrees of
freedom, as the attendant solvers can leverage the lower-block triangular
structure of the system arising from the sequential nature of time
evolution. As a result, solving the algebraic systems arising from space--time
projection is generally intractable without applying hyper-reduction in time
\cite{choi_stlspg,constantine_strom}. Second, space--time residual
minimization precludes future
state prediction, as these methods employ space--time basis vectors defined over the
entire time domain of interest, which must have been including in the training
simulations.

The objectives of this work are to overcome the shortcomings of existing
residual-minimizing methods, and to provide a unifying framework from which
existing methods can be assessed. In essence, the proposed  \textit{\methodNameLower\
(\methodAcronym)} method sequentially minimizes the FOM
residual over a sequence of arbitrarily defined
time windows. The method is characterized by three notable aspects.  First, the method
minimizes the \textit{time-continuous}  residual (i.e., that associated with
the full-order model ODE). By adopting a time-continuous viewpoint, the
formulation decouples the underlying temporal discretization scheme from the
residual-minimization problem, thus addressing a key deficiency of both LSPG
and space--time LSPG projection. Critically, time-continuous residual minimization also exposes
two different solution methods:  a \textit{discretize-then-optimize}
(i.e., direct) method, and an \textit{optimize-then-discretize} (i.e.,
indirect) method. Second, the method sequentially minimizes
the residual over arbitrarily defined \textit{time windows} rather than
sequentially minimizing the residual over time steps (as in LSPG projection)
or over the entire time domain (as in space--time residual-minimization
methods). This equips the method with additionally flexibility that enables it
to explore more fine-grained tradeoffs between computational cost and error.
Finally, \methodAcronym\ is formulated for two kinds of space--time trial
subspaces: one that reduces only the spatial dimension of the full-order
model (as employed by traditional spatial-projection-based methods), and one that reduces both the spatial and temporal dimensions of the
full-order model (as employed by space--time residual-minimization methods).
The above attributes allow that \methodAcronym\ method to be viewed as a
generalization of existing model-reduction methods, as 
Galerkin, LSPG, and space--time LSPG projection correspond to specific
instances of the formulation.
Figure~\ref{fig:flowchart} depicts how the proposed \methodAcronym\ method
provides a unifying framework from which these existing approaches can be derived.

The \methodAcronym\ approach can be viewed as a hybrid space--time method and displays 
commonalities with several related efforts. 
First, Ref.~\cite{bui_thesis} tersely formulated a space--time least-squares ROM
and connected this formulation with optimal control; specifically it mentioned the
optimize-then-discretize vs.\ discretize-then-optimize approaches. This work
did not fully develop this approach, eschewing it 
for 
sequential residual minimization in time (i.e., LSPG). The present work thus
formally develops and extends several of the concepts put forth
in Ref.~\cite{bui_thesis}. Next, Ref.~\cite{constantine_strom} developed a
space--time residual minimization formulation for model interpolation.  The
present work distinguishes itself from Ref.~\cite{constantine_strom} in that
(1) this work also considers trial subspaces characterized by a reduction of
the spatial dimension of the FOM only, and (2) we
minimize the time-continuous FOM residual over arbitrary time
windows. We note that,
similar to the current work, Ref.~\cite{constantine_strom} employs
minimization of the time-continuous FOM residual as a starting point; as such,
this work shares some thematic similarities with
Ref.~\cite{constantine_strom}.  Lastly, Ref.~\cite{choi_stlspg} develops a
space--time extension of the LSPG approach that minimizes the time-discrete
FOM residual over the entire time domain.  The present work distinguishes
itself from Ref.~\cite{choi_stlspg} in that (1) this work minimizes the
time-continuous FOM residual, (2) this work minimizes this residual over
arbitrary time windows, and (3) this work considers trial subspaces
characterized by a reduction of the spatial dimension of the FOM only.

\begin{figure} 
\begin{centering} 
\includegraphics[trim={0.1cm 0cm 1cm 0cm},clip,width=0.99\textwidth]{diagram.pdf} 
\caption{Relationship diagram for the \methodAcronym\ framework for model
	reduction. \KTC{Inconsistent capitalization. I recommend using (1)
	``Limiting case'', (2) Space--time LSPG. Also, can we make this more specific? In particular, can
	we write specific equations (e.g., ``$W = T$ in Eq.~(10)'') for the limiting
	cases?}} 
\label{fig:flowchart} 
\end{centering} 
\end{figure}



 In summary, specific contributions of this work include:
\begin{enumerate}
%\item The development of a time-continuous residual minimization framework for reduced-order models.
\item The \methodNameLower\ (\methodAcronym) framework for dynamical-system
	model reduction. The approach sequentially minimizes the time-continuous
		full-order model residual over arbitrary time windows.
\item Support of two space--time trial subspaces: one that reduces the spatial
	dimension of the FOM only, and one that reduces both the spatial and
		temporal dimensions of the FOM. The former case is of particular
		interest in the \methodAcronym\  contex, as the stationary conditions are
		derived via the Euler--Lagrange equations and comprise a coupled two-point
		Hamiltonian boundary value problem containing a forward and backward
		system. The forward system, which is forced by an auxiliary costate,
		evolves the (spatial) generalized coordinates of the ROM in time. The
		backward system, which is forced by the time-continuous FOM residual
		evaluated about the ROM state, governs the dynamics of the costate. 
	\item Derivation of two solution techniques:
		\textit{discretize-then-optimize} (i.e., direct) and
		\textit{optimize-then-discretize} (i.e., indirect). 
	\item Derivations of conditions under which the
		\methodAcronym\ framework recovers Galerkin, LSPG, and space--time LSPG
		projection.
\item Error analysis of the \methodAcronym\ framework using
	trial subspaces characterized by a reduction of
the spatial dimension of the FOM only.
	This analysis demonstrates that the \methodAcronymROMs\ error 
		is bounded \textit{a priori} by a combination of the
		error at the start of the window and the integrated FOM ODE residual
		evaluated at the FOM state projected onto the trial subspace. As a result,
		the error bound exhibits exponential growth in the number of time windows
		(not total time instances), yielding smaller error-bound growth in thime than
		Galerkin or LSPG projection. \KTC{Check this}
\item Numerical experiments for 
	trial subspaces characterized by a reduction of
the spatial dimension of the FOM only, which demonstrate two key findings:
\begin{itemize}
\item Minimizing the residual over a larger time window leads to more stable
	and physically relevant solutions \KTC{What is `physically relevant'?} as
		well as smaller space--time residual norms. 
\item Minimizing the residual over a larger time window does not necessarily
	lead to a more accurate trajectory (as measured in the space--time
		$\elltwo$-norm of the solution). Instead, minimizing the residual over an
		intermediate-sized time window leads to the smallest trajectory error.
\end{itemize}
\end{enumerate}

The paper proceeds as follows: Section~\ref{sec:math}
outlines the mathematical setting for the full-order model, along with the Galerkin, LSPG, and
space--time LSPG projection. Section~\ref{sec:tclspg} outlines the proposed \methodAcronym\
framework. Section~\ref{sec:numerical_techniques}
outlines numerical techniques for solving \methodAcronymROMs, including both direct and
indirect methods. Section~\ref{sec:analysis} provides
equivalence conditions and error analysis for \methodAcronymROMs.
Section~\ref{sec:numerical_experiments} presents numerical experiments.
Section~\ref{sec:conclude} provides conclusions and perspectives.
We denote vector-valued functions with italicized bold symbols (e.g., $\boldsymbol
x$), vectors with standard bold symbols (e.g., $\mathbf{x}$), 
matrices with capital bold symbols (e.g., $\mathbf{X} \equiv \begin{bmatrix}
\mathbf{x}_1 & \cdots & \mathbf{x}_r\end{bmatrix}$), and spaces with
calligraphic symbols (e.g., $\mathcal{X}$). We additionally denote differentiation of a time-dependent 
function with respect to with the $\dot\ $ operator.
%Model reduction of nonlinear dynamical systems is a growing research area.
%The least-squares Petrov-Galerkin (LSPG) approach has gained popularity as a model reduction technique for nonlinear dynamical systems. Derived at the fully discrete level, the LSPG approach solves a least-squares problem that minimizes the fully discrete residual of the reduced-order model at each time-step. LSPG has been shown to be an effective formulation for nonlinear model reduction on complex problems of interest.
%\begin{comment} 
\section{Mathematical formulation}\label{sec:math}
	We begin by providing the formulation for the full-order model,
	followed by a description of standard model-reduction methods
	classified according to the type of trial subspace they employ.
	\subsection{Full-order model}\label{sec;FOM}
We consider the full-order model to be a dynamical system expressed as a
	system of ordinary differential equations (ODEs)
\begin{equation}\label{eq:FOM}
 \stateFOMDotArg{t}  = \velocity(\stateFOMArg{}{t},t), \qquad \stateFOM(0) =
	\stateFOMIC,\qquad t \in [0,T]
\end{equation}
where  $\stateFOM: [0,T]
	\rightarrow  \RR{\fomdim}$ with $\stateFOM: \timeDummy \mapsto
	\stateFOM(\timeDummy) $ and $\stateFOMDot \equiv {d \stateFOM}/{d\tau}$
	denotes the state implicitly defined as the solution to initial value
	problem~\eqref{eq:FOM}, 
$T \in \mathbb{R}^+$ denotes the final time, 
 $\stateFOMIC \in \mathbb{R}^{\fomdim}$ denotes the initial condition,
	and $\velocity: \mathbb{R}^{\fomdim} \times [0,T] \rightarrow
	\mathbb{R}^{\fomdim}$ with $(\stateyDiscrete,\timeDummy) \mapsto
	\velocity(\stateyDiscrete,\timeDummy)$ denotes the velocity, which is possibly
	nonlinear in its first argument. For subsequent exposition, we introduce
	$\timeSpace$ to denote the set of real-valued functions acting on the time
	domain (i.e., $\timeSpace = \{f\,|\,f:[0,T]\rightarrow\RR{}\}$); the
	state can be expressed equivalently as $\stateFOM \in \RR{\fomdim} \otimes
	\timeSpace $.  We refer to the initial value problem defined in
	Eq.~\eqref{eq:FOM} as the ``full-order model" (FOM) ODE. We note that
	although the problem of interest described in the introduction corresponds
	to a parameterized dynamical system, we suppress dependence of the FOM ODE
	\eqref{eq:FOM} on such parameters for notational convenience, as this work
	focuses on devising a projection applicable to a specific parameter instance.

Directly solving the FOM ODE~\eqref{eq:FOM} is computationally expensive if
	either the state-space dimension $\fomdim$ is large, or if the time-interval
	length $T$ is large relative to the time step required to numerically
	integrate Eq.~\eqref{eq:FOM}. For time-critical or many-query applications,
	it is essential to relplace the FOM ODE~\eqref{eq:FOM} with a strategy that
	enables an approximate trajectory to be computed at lower computational
	cost. Projection-based ROMs constitute one such promising approach. 

\subsection{Reduced-order models}
Projection-based ROMs generate approximate solutions to the FOM
	ODE~\eqref{eq:FOM} by approximating the state in a low-dimensional trial
	subspace. Two types of space--time trial subspaces are commonly used for
	this purpose: 
\begin{enumerate} 
	\item \textit{Subspaces that reduce only the spatial dimension of the full-order
		model (\spatialAcronym)}. These 
	trial subspaces are commonly used
		in model-reduction methods based on spatial projection followed by time
		integration (e.g., Galerkin projection, LSPG projection).
	\item \textit{Subspaces that reduce both the spatial and temporal dimensions of the full-order
		model (\spaceTimeAcronym)}. These trial subspaces are commonly employed in
		model-reduction methods based on space--time projection (e.g., space--time
		LSPG projection).
\end{enumerate}
 We now describe these two types of space--time trial subspaces and their
	application to Galerkin, LSPG, and space--time LSPG projection. 

\subsection{\spatialAcronym\ trial subspaces}
At a given time instance 
$t\in[0,T]$,
\spatialAcronym\ trial subspaces approximate the FOM ODE solution
	as $\approxstate(t)\approx\stateFOM(t)$, which is enforced to reside in an
	affine spatial trial subspace of dimension $K\ll\fomdim$ such that
	$\approxstate(t)\in
	\stateIntercept+\trialspace
\subseteq\RR{\fomdim}$, where $\dim(\trialspace) = K$
and $\stateIntercept \in \mathbb{R}^{\fomdim}$ denotes the reference state, which
	is often taken to be the initial condition (i.e., $\stateIntercept = \stateFOMIC$).
Here, the trial subspace
$\trialspace$ 
is spanned by an orthogonal basis such that
$ \trialspace= \Range{\basismat}$
with 
$ \basismat \equiv \begin{bmatrix}  \basisvec_1  & \cdots &  \basisvec_K \end{bmatrix}
	\in \RRStar{\romdim}{\fomdim}$, where $\RRStar{\romdim}{\fomdim}$ denotes the compact Stiefel manifold (i.e.,  $
	\RRStar{\romdim}{\fomdim}\defeq
	\{ \mathbf{X} \in \RR{\fomdim
	\times \romdim}\, \big|\, \mathbf{X}^T \mathbf{X} = \mathbf{I} \}$).
The basis vectors $\basisvec_i$, $i=1,\ldots,K$ are typically constructed
using state snapshots, e.g., via
proper orthogonal decomposition (POD)~\cite{berkooz_turbulence_pod}, the reduced-basis method~\cite{rb_1,rb_2,rb_3,NgocCuong2005,Rozza2008}. 
Thus, at any time instance $t\in[0,T]$, ROMs that employ the  \spatialAcronym\ trial subspace approximate the state as
\begin{equation}\label{eq:affine_trialspace}
\stateFOMArg{}{t} \approx \approxstate(t) = \basisspace \genstateArg{}{t} + \stateIntercept,
\end{equation}
where $\genstate \in \RR{\romdim} \otimes \timeSpace$ with
$\genstate:\timeDummy\mapsto\genstate(\timeDummy)$
denotes the generalized
coordinates. 

	From the space--time perspective, this is equivalent to approximating the
	FOM ODE solution trajectory $\stateFOM\in\RR{N}\otimes\timeSpace$ with 
	$\approxstate\in \stspaceS$, where
\begin{equation}\label{eq:spatial_subspace}
\begin{split}
& \stspaceS \defeq \trialspace \otimes \timeSpace +
	\stateIntercept\otimes\onesFunction\subseteq\RR{N}\otimes\timeSpace
\end{split}
\end{equation}
with
	$\dim(\stspaceS) = K\ntimeSteps$
and $\onesFunction\in\timeSpace$ defined as
$\onesFunction:\timeDummy\mapsto 1$.
	 
Substituting the approximation~\eqref{eq:affine_trialspace} into the FOM ODE~\eqref{eq:FOM} and performing orthogonal
$\elltwo$-projection of the initial condition onto the trial subspace yields
the overdetermined system of ODEs
\begin{equation}\label{eq:g_truncation}
\basisspace \genstateDotArg{}{t} = \velocity(\basisspace
\genstateArg{}{t} + \stateIntercept,t ), \qquad \genstate(0) = \genstateIC,
	\qquad t \in [0,T],
\end{equation}
where $\genstateDot\equiv {d \genstate}/{d\tau}$.
Because Eq.~\eqref{eq:g_truncation} is overdetermined, a solution may not
exist. Typically, either \textit{Galerkin projection} or \textit{least-squares
Petrov--Galerkin projection} is employed to reduce the number of equations
such that a unique solution exists. We now describe these two methods.

\subsubsection{Galerkin projection}
Galerkin projection reduces the number of equations in
Eq.~\eqref{eq:g_truncation} by enforcing orthogonality of the 
residual to the spatial trial subspace in the (semi-)inner product induced by the 
positive (semi-)definite $\fomdim\times\fomdim$ matrix $\stweightingMatArg{}\equiv \stweightingMatOneArg{ }^T
\stweightingMatOneArg{ }$ (commonly set to
$\stweightingMatArg{}=\mathbf{I}$), i.e.,
\begin{equation}\label{eq:g_truncation2}
\genstateGalerkinDotArg{}{t} = \massArgnt{n}^{-1} \basisspace^T \stweightingMatArg{n} \velocity(\basisspace
\genstateGalerkinArg{}{t} + \stateIntercept,t), \qquad \genstateGalerkin(0) = \genstateIC, \qquad t \in [0,T],
\end{equation}
where $\massArgnt{ } \equiv \basisspace^T \stweightingMatArg{ } \basisspace$
denotes the $K\times K$ positive definite mass matrix.
As demonstrated in Ref.~\cite{carlberg_lspg_v_galerkin}, Galerkin projection
can be viewed alternatively as a residual-minimization
method, as the Galerkin ODE~\eqref{eq:g_truncation2} is equivalent
to
\begin{equation}\label{eq:GalOptimal}
\genstateGalerkinDotArg{}{t} = \underset{ \genstateyDiscrete \in \RR{\romdim}
	}{\text{arg\,min}} \| \basisspace \hat{\stateyDiscrete} -
	\velocity(\basisspace \genstateGalerkinArg{}{t} + \stateIntercept,t)
	\|_{\stweightingMatArg{n}}^2, \qquad \genstate(0) = \genstateIC,
	\qquad t \in [0,T],
%\hat{\mathbf{v}}, %\qquad \genstateGalerkin(0) = \basisspace^T(\stateFOMIC -\stateIntercept), \qquad t \in [0,T].
\end{equation}
where $\|\mathbf{x}\|_{\stweightingMatArg{n}}\equiv
\sqrt{\mathbf{x}^T\stweightingMatArg{n}\mathbf{x}}$.
%where
%$$\hat{ \mathbf{v}} = \underset{\hat{ \stateyDiscrete}  }{\text{arg min}} || \basisspace \hat{\stateyDiscrete} -  \velocity(\basisspace \genstateGalerkinArg{}{t} + \stateIntercept) ||_{\stweightingMatArg{n}}^2.$$
%The minimization problem has the solution,
%$$\hat{\mathbf{v}} = \massArgnt{n}^{-1}\basismat^T \stweightingMatArg{n} \velocity(\basismat \genstateGalerkinArg{}{t} + \stateIntercept ).$$
%Galerkin method is interpreted as finding the velocity that minimizes the FOM ODE residual~\cite{carlberg_lspg_v_galerkin},
% The velocity that minimize the FOM ODE residual is given as the solution to the optimization problem, %To demonstrate this, we define the FOM residual, %The LSPG residual corresponds to minimizing the FOM O$\Delta$ E residual at each time-instance. obtained either by projecting Eq.~\ref{eq:g_truncation} via a test basis or by residual minimization. Equivalence conditions for these two approaches are established in Ref.~\cite{carlberg_lspg_v_galerkin}. This work considers dimension reduction by residual minimization.
%The most common way to perform dimension reduction is by projecting Eq.~\ref{eq:g_truncation} via a test basis. An alternative philosophy that is pursued here is to perform dimension reduction through residual minimization. 
%Define the residual,
%\begin{align*}
%\resid & \vcentcolon \state \mapsto \frac{d \state}{dt} - \velocity(\state), \\
%& \vcentcolon \RR{\fomdim} \times [0,T] \rightarrow \RR{\fomdim} \times [0,T].
%\end{align*}
%The Galerkin method can be interpreted as finding the velocity such that the instantaneous time residual is minimized,%Minimization of the residual over the trial space leads to what is known as the Galerkin method,
%$$\mathbf{v} = \underset{\stateyDiscrete \in \text{Range}(\basismat)  }{\text{arg min}}|| \stateyDiscrete -  \velocity(\state(t)) ||_2^2.$$
%As $\mathbf{v} \in \text{Range}(\basismat)$, the minimization problem can be written as,%and hence $\boldsymbol v = \basismat \hat{\boldsymbol v} $,
%$$\hat{ \mathbf{v}} = \underset{\hat{ \stateyDiscrete}  }{\text{arg min}} || \basisspace \hat{\stateyDiscrete} -  \velocity(\state(t)) ||_2^2,$$
%where $\hat{\mathbf{v}} \in \RR{K}$ are the generalized coordinates of $\boldsymbol v$ and $\boldsymbol v = \basisspace \hat{\boldsymbol v}$. 
%The minimization problem has the solution~\cite{carlberg_lspg_v_galerkin},
%$$\hat{\mathbf{v}} = \basismat^T \velocity(\basismat \genstate(t) + \stateIntercept ).$$
%The Galerkin ROM is thus given by,
%\begin{equation*}\label{eq:g_truncation2}
%\frac{d}{dt}\genstateGalerkinArg{}{t} = \mass^{-1}\basisspace^T \stweightingMatArg{n} \velocity(\basisspace
%\genstateGalerkinArg{}{t} + \stateIntercept).
%\end{equation*}
Thus, the computed velocity $\genstateGalerkinDotArg{}{t}$ minimizes the
FOM ODE residual evaluated at the state $\basisspace
\genstateGalerkin(t) + \stateIntercept$ and time instance $t$ over the spatial trial
subspace $\trialspace$.
%To perform dimension reduction, 
%To perform dimension reduction, the governing equations are projected by a ``test" basis,
%\begin{equation}\label{eq:g_truncation}
%\testbasis^T \basisspace \frac{d \genstate}{dt} = \testbasis^T \velocity(\basisspace \genstate + \state_0,t; \param ), \qquad t \in [0,T],
%\end{equation}
%The most common choice is to set the test space and trial space to be equivalent, which results in the Galerkin reduced-order model,
%\begin{equation}\label{eq:g_truncation}
%\frac{d \genstate}{dt} = \basisspace^T \velocity(\basisspace \genstate + \state_0,t; \param ), \qquad t \in [0,T],
%\end{equation}
\subsubsection{Least-squares Petrov--Galerkin projection}
Despite its time-instantaneous residual-minimization optimality property 
\eqref{eq:GalOptimal}, Galerkin projection can yield inaccurate solutions,
particularly if the velocity is not self-adjoint or
is nonlinear. 
Least-squares Petrov--Galerkin (LSPG)
projection~\cite{carlberg_lspg_v_galerkin,carlberg_thesis,carlberg_gnat,bui_unsteady,bui_thesis}
was developed
as an alternative projection method that exhibits several advantages over Galerkin
projection.
Rather than minimize the (time-continuous) FOM ODE residual at a time instance as in Galerkin
projection, LSPG minimizes
the (time-discrete) FOM O$\Delta$E residual 
(i.e., the residual arising after applying time discretization to the FOM ODE)
over a time step.
We now describe LSPG projection in the case of linear multistep methods;
Ref.~\cite{carlberg_lspg_v_galerkin} also presents LSPG projection for Runge--Kutta
schemes. 

Without loss of generality, we introduce a uniform time
grid characterized by time step $\Delta t$ and time instances
$t^n = n\Delta
t$, $n=0,\ldots,N_t$.
Applying a linear multistep method to discretize the FOM ODE \eqref{eq:FOM}
with this time grid
yields the FOM O$\Delta$E, which computes the sequence of discrete
solutions
$\stateFOMDiscreteArg{n}( \approx \stateFOM(t^n))$, $n=1,\ldots,N_t$
as the implicit solution to the system of algebraic equations
\begin{equation}\label{eq:lms}
\residLMSArg{n}
	(\stateFOMDiscreteArg{n};\stateFOMDiscreteArg{n-1},\ldots,\stateFOMDiscreteArg{n-k^n})
	= \bz,\qquad n=1,\ldots,N_t,
\end{equation}
where  $k^n$ denotes the number of steps employed by the scheme at the $n$th
time instance and 
$\residLMSArg{n}$ denotes the FOM O$\Delta$E residual defined as
\begin{align*}
\residLMSArg{n} &: (\stateyDiscreteArgnt{n};\stateyDiscreteArgnt{n-1},\ldots,\stateyDiscreteArgnt{n-k^n}) \mapsto  \frac{1}{\Delta t} \sum_{j=0}^{k^n} \alpha^n_j \stateyDiscreteArgnt{n-j} -  \sum_{j=0}^{k^n} \beta^n_j \velocity(\stateyDiscreteArgnt{n-j},t^{n-j}),
\\
&: \RR{\fomdim} \otimes \RR{k^n + 1} \rightarrow \RR{\fomdim},
\end{align*} 
Here, $\alpha^n_j,\beta^n_j\in\RR{}$, $j=0,\ldots,k^n$ are coefficients
that define the linear multistep method at the $n$th time instance.
%\begin{equation}\label{eq:coarse_implicit_euler_0}
%\residLMS \frac{\stateFOMDiscreteArg{n} - \stateFOMDiscreteArg{n-1}  }{\Delta t} - \velocity(\stateFOMDiscreteArg{n}) = \mathbf{0}, \qquad n = 1,2,\ldots,N_t,
%\end{equation}
%the algebraic FOM O$\Delta$E obtained after an implicit Euler temporal discretization is given by,
%\begin{equation}\label{eq:coarse_implicit_euler_0}
%\frac{\stateFOMDiscreteArg{n} - \stateFOMDiscreteArg{n-1}  }{\Delta t} - \velocity(\stateFOMDiscreteArg{n}) = \mathbf{0}, \qquad n = 1,2,\ldots,N_t,
%\end{equation}
%where $\stateFOMDiscreteArg{n} \approx \stateFOM(t^n)$ denotes the discrete approximation to the state at the $n${th} time instance.

At each time instance on the time grid, the LSPG method substitutes the \spatialAcronym\ trial subspace approximation
\eqref{eq:affine_trialspace} into the FOM O$\Delta$E~\eqref{eq:lms} and
minimizes the residual, i.e., LSPG sequentially computes the solutions
$\approxstateLSPG^n \approx \stateFOMDiscreteArg{n}$, $n=1,\ldots,
\ntimeSteps$ that satisfy
\begin{equation*}
\approxstateLSPG^n = \underset{\stateyDiscrete \in \trialspace  }{\text{arg\,min}}|| \lspgWeightingArg{\stateyDiscrete}\residLMSArg{n} (\stateyDiscrete; \approxstateLSPG^{n-1},\ldots,\approxstateLSPG^{n-k^n}) ||_2, \qquad n = 1,\ldots,\ntimeSteps,
\end{equation*}
where 
$\lspgWeightingArg{\cdot} \in \RR{\nsamples \times \fomdim}$, with $\romdim
\le \nsamples \le \fomdim$, is a weighting matrix that can be used, e.g., to
enable hyper-reduction by requiring it to have a small number of nonzero
columns. 

As described in the introduction, although numerical experiments have
demonstrated that LSPG projection often yields more accurate and stable
solutions than Galerkin
projection~\cite{bui_thesis,carlberg_lspg_v_galerkin,carlberg_gnat,carlberg_thesis,parish_apg},
LSPG still suffers from several shortcomings, in particular its complex
dependence on the time discretization, exponentially growing error bounds, and
a lack of optimality for the trajectory defined over the entire time domain.

\subsection{\spaceTimeAcronym\ trial spaces and space--time ROMs}

Space--time projection methods that employ \spaceTimeAcronym\ trial
spaces~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC,benner_st,bui_thesis}
aim to overcome the latter two shortcomings of LSPG projection, which are
shared by all model-reduction methods based on spatial projection followed by
time integration.  Because these methods employ \spaceTimeAcronym\ trial
spaces, they reduce both the spatial and temporal dimensions of the full-order
model; further, they yield error bounds that grow more slowly in time and
their trajectories exhibit an optimality property over the entire time domain. 

\spaceTimeAcronym\ trial subspaces approximate the FOM ODE solution
trajectory
	$\stateFOM\in\RR{N}\otimes \timeSpace$ with an approximation that resides in an
	affine space--time trial subspace of dimension $\stdim\ll\fomdim$, i.e., 
	$\approxstate\in \stspaceST$ with $\dim(\stspaceST) =\stdim $, where
\begin{equation}\label{eq:sttrialspace_def}
 \stspaceST \defeq 
	\Range{\stbasis} + 
	\stateIntercept\otimes\onesFunction
	\subseteq \RR{N} \otimes \timeSpace
\end{equation}
and $\stbasis\in\RR{N \times \stdim}\otimes \timeSpace$ with 
$\stbasis:\timeDummy\mapsto\stbasis(\timeDummy)$
denotes the space--time trial basis.

Thus, at any time instance $t\in[0,T]$, ROMs that employ the
\spaceTimeAcronym\ trial subspace approximate the state as
\begin{equation}\label{eq:stapprox1}
 \stateFOMArg{}{t} \approx \approxstateArg{}{t}  = \stbasisArg{}{t} \stgenstate + \stateInterceptArg{},
\end{equation}
where $\stgenstateArg{} \in \RR{\stdim}$ denote the space--time generalized coordinates. 
Critically, comparing the approximations arising from \spatialAcronym\ and
\spaceTimeAcronym\ subspaces in Eqs.~\eqref{eq:affine_trialspace} and \eqref{eq:stapprox1}, respectively,
highlights that the former approximation associates with a time-dependent
generalized coordinates, while the latter approximation associates with a
time-dependent basis matrix.

Substituting the approximation~\eqref{eq:stapprox1} into the FOM
ODE~\eqref{eq:FOM} yields
\begin{equation}\label{eq:st_fom}
\stbasisDotArg{}{t} \stgenstate =  \velocity(\stbasisArg{}{t} \stgenstate +
	\stateIntercept,t) , \qquad \genstate(0) =\mathbb{P}(\stateFOM(0)\otimes
	\timeSpace)(0), \qquad t \in [0,T],
\end{equation}
where 
$\stbasisDot\equiv d\stbasis/d\timeDummy$,
$\mathbb{P} : \RR{N} \otimes \timeSpace \rightarrow \stspaceST$ is an orthogonal projection onto the trial subspace (e.g., $\elltwo$).
Equation~\eqref{eq:st_fom} is an over-determined algebraic system for the $\stdim$ unknowns. We now outline the Space--Time Least-Squares Petrov--Galerkin method~\cite{choi_stlspg}. 
The space--time Galerkin method is noted to additionally be a possibility for model reduction. As the space--time Galerkin method does not associate with any residual minimization 
principle, however, we do not outline it here.
\subsubsection{Space--Time LSPG projection}  
As in LSPG, Space--Time LSPG (ST-LSPG)~\cite{choi_stlspg} minimizes the fully discrete residual, with the key difference being that ST-LSPG minimizes the residual over the entire time domain. For brevity, ST-LSPG is outlined only for linear multistep methods.
%As ST-LSPG acts on the discrete solution, we first note that the discrete FOM solution lives in $\RR{N} \otimes \RR{N_t}$. ST-LSPG then seeks solutions in the trial space,
%$$\stspaceDiscrete \defeq \spaceTrialSpace \otimes \timeTrialSpaceDiscrete \subseteq \RR{N} \otimes \RR{N_t},$$
%where $\timeTrialSpaceDiscrete$ is the discrete temporal trial space. It is noted that this trial space is a specific instance of the trial space~\ref{eq:sttrialspace_def} 
%As before, we define a discretization of the continuous time-domain $[0,T]$ into $N_t$ time instances $t^n = n\Delta t, n=1,2,\ldots,N_t$ with a constant time-step $\Delta t = T/N_t$. We define now the space--time discrete residual for the implicit Euler method as,
We first define the space--time discrete residual for linear multistep methods,
\begin{align*}
\residST_{} & \vcentcolon (\stateyDiscreteArg{N_t}, \ldots ,\stateyDiscreteArg{1};\stateFOMIC) \mapsto \begin{bmatrix}
%\frac{\stateyDiscreteArg{N_t} - \stateyDiscreteArg{N_t-1}  }{\Delta t} -  \velocity(\stateyDiscreteArg{N_t}) \\
\residLMSArg{N_t}(\stateyDiscreteArg{N_t};\stateyDiscreteArg{N_t-1},\ldots,\stateyDiscreteArg{N_t - k^{N_t}}) \\ 
%\frac{\stateyDiscreteArg{N_t-1} - \stateFOMDiscreteArg{N_t-2}  }{\Delta t} -  \velocity(\stateyDiscreteArg{N_t-1}) \\
\vdots \\
%\frac{\stateyDiscreteArg{1} - \stateFOMIC }{\Delta t} -  \velocity(\stateyDiscreteArg{1}) \\
\residLMSArg{1}(\stateyDiscreteArg{1}; \stateFOMIC) 
\end{bmatrix}  \\
& \vcentcolon \RR{\fomdim} \otimes \RR{N_t+1} \rightarrow \RR{\fomdim N_t}. 
\end{align*}
For convenience we define the same residual with the generalized coordinates as the inputs,
\begin{align*}
\genresidST_{\text{}} & \vcentcolon (\stgenstatey ;\stateFOMIC) \mapsto \begin{bmatrix}
%\frac{\stateyDiscreteArg{N_t} - \stateyDiscreteArg{N_t-1}  }{\Delta t} -  \velocity(\stateyDiscreteArg{N_t}) \\
\residLMSArg{n}\bigg( \stbasisArg{}{t^{N_t}}\stgenstatey + \stateIntercept; \stbasisArg{}{t^{N_t-1}}\stgenstatey + \stateIntercept ,\ldots,  \stbasisArg{}{t^{N_t-k^{N_t}}}\stgenstatey + \stateIntercept \bigg) \\ 
\vdots \\
\residLMSArg{n}\bigg( \stbasisArg{}{t^1}\stgenstatey + \stateIntercept; \stateFOMIC \bigg) \\ 
\end{bmatrix}  \\
& \vcentcolon \RR{\stdim} \times \RR{N} \rightarrow \RR{\fomdim N_t}. 
\end{align*}
ST-LSPG is defined as,
\begin{equation*}
\stgenstate = \underset{ \stgenstatey }{\text{arg\,min}}|| \lspgWeightingSTArg{\stgenstatey}  \genresidST_{\text{}}(\stgenstatey;\stateFOMIC) ||_2^2, 
\end{equation*}
where $\lspgWeightingSTArg{\cdot} \in \RR{n_{st} \times N N_t}$, with $\stdim \le n_{st} \le N N_t$, is a space--time weighting matrix.


ST-LSPG is appealing in that it (1) allows for both spatial and temporal dimension reduction and (2) minimizes the residual over the \textit{entire} space--time domain (as opposed to a single time-step instance as in LSPG). As a result, ST-LSPG can potentially be more efficient than standard LSPG and is equipped with more favorable error bounds~\cite{choi_stlspg}. ST-LSPG, however, is subject to several practical challenges.  Most importantly, ST-LSPG can be computationally 
intensive as it involves (1.) computing space--time basis vectors and (2.) minimizing the entire space--time residual. For high-dimensional systems, these tasks may be intractable. Another challenge associated with ST-LSPG is that, as it requires a pre-defined temporal basis, it is unclear how it can be used, e.g., to perform future state prediction.
  
\subsection{Outstanding challenges}
This work seeks to overcome the limitations of existing reduced-order modeling approaches. Specifically, 
we look to overcome LSPG's time-step and time-scheme dependence, the stability challenges of both Galerkin and LSPG, and the potential computational cost of ST-LSPG. To this end, we now outline the \methodNameLower\ formulation. 
%\end{comment}
%\input{formulation_3}

%\input{space_time}
%\input{timecont_lspg}
\input{section3/tclsrm_formulation}
\input{section4/numerical_methods}
\input{analysis}
\input{numerical_experiments}


\section{Conclusions}\label{sec:conclude}
This paper introduced a windowed least-squares reduced-order modeling (\methodAcronym) formulation for dynamical systems. The approach sequentially minimizes the time-continuous full-order model residual within a low-dimensional trial subspace over a series of time windows. The formulation is applicable to both spatial and space--time dimension reduction trial subspaces. The Galerkin, Least-Squares Petrov--Galerkin (LSPG) and space--time LSPG methods were shown to be specific instances of the proposed method 


Two differing techniques to solve the least-squares minimization problems were considered: direct (discretize then minimize) and indirect (minimize then discretize). 
The case of \spatialAcronym\ trial subspaces proved particularly interesting: it was shown that indirect methods comprise solving a coupled two-point 
Hamiltonian boundary value problem. The forward problem evolves the standard Galerkin ROM forced by a ``costate" variable, while a backward problem evolves the costate and is forced by the ROM residual.

Numerical experiments of the compressible Euler and Navier-Stokes equations demonstrated the utility in the proposed approach. The first numerical experiment, in where the Sod shock tube was examined, demonstrated that \methodAcronymROMs\ minimizing the residual over larger time windows yielded smoother and more physically relevant solutions. Further, increasing the window size over which the residual was minimized led to a monotonic decrease in the total space--time residual. Increasing the window size over which the residual was minimized, however, did not necessarily decrease the $\elltwo$ solution error; this was observed to occur over an intermediary window size. It was 
additionally shown that the \methodAcronymROMs\ overcome the time-step sensitivity challenges that LSPG is subject to. The second numerical experiment, which examined hyper-reduced ROMs of a compressible cavity flow, demonstrated the utility of the \methodAcronym\ formulation on a more complex flow. In this experiment, \methodAcronymROMs\ yielded predictions with relative errors less than $5-10\%$, while the Galerkin and LSPG ROMs failed to converge/blew up. Increasing the window size over which the residual was minimized again led to a lower space--time residual, but not necessarily a lower $\elltwo$ error.

%In the case of spatial dimension-reduction-only, \methodAcronym\ displays improved performance over LSPG at the expense of an increased computational cost. This increase in computational cost is due to the fact that forming and/or solving the nonlinear minimization problem becomes more difficult as the time-slab size grows. To minimize this increase in cost, we intend to pursue the development of new solution techniques tailored to \methodAcronym.
 
The principle challenge encountered in the \methodAcronym\ formulation is the computational cost: increasing the window size over which the residual is minimized leads to, in general, a higher computational cost. This increased cost is due to the fact that forming and solving the nonlinear least-squares problem becomes more expensive and difficult as the window size grows. While numerical experiments demonstrated that this increase in computational cost is mild, future work will focus on the development of solution techniques to improve the scaling of the method.

 

%The second subspace we consider arrises when the decoder, $\decoder$, is linear and time-varying, while the generalized coordinates are time-stationary. This is the case in space-time discretizations. In this case one has,
%$$\state(t) \approx \approxstate(t)  =  \decoder(\genstate(t),t) = \basisst(t) \genstate + \state_0 $$
%where $\genstate \in \mathbb{R}^{\romstdim}$ are the time-stationary (space-time) generalized coordinates and the time-continuous basis is given by $\basisst \in \mathbb{R}^{romdim \times \romstdim} \times [0,T]$. The Euler-Lagrange equations are given by,
%\begin{equation}\label{eq:euler_lagrange_st}
% \bigg[ \dot{\basisst}^T \stweightingMat^T  - \basisst^T \big[\frac{\partial \velocity}{\partial \state} \big]^T \stweightingMat^T \bigg]  \bigg( \dot{\basisst} \genstate -    \velocity \bigg) = \mathbf{0}, \qquad \state(t=0) = \state_0\end{equation}
%Note that the end-point boundary condition in the Euler-Lagrange equations is automatically satisfied.

%\subsubsection{Solution techniques}
%Equation~\ref{eq:euler_lagrange_st} consists of a nonlinear algebraic system to be solved at each time-instance, $t$. To obtain solutions, we must employ a temporal discretization. Here, we propose using a variational discretization with tensor products, e.g. finite elements. Let,
%$$\basisst \equiv \basisspace \otimes \basistime \subseteq \basisspace \otimes \mathcal{T},$$
%where $\mathcal{T}$ is the finite elements space. Similarly, let
%$$\genstate = \genstate_s \otimes \genstate_t$$

\section{Acknowledgments}
The authors thank Yukiko Shimizo and Patrick Blonigan for numerous conversations from which this work benefited. 
E.\ Parish acknowledges an appointment to the Sandia National Laboratories'
John von Neumann Postdoctoral Research Fellowship in Computational Science. 
This work was partially sponsored by Sandia's Advanced Simulation and
Computing (ASC) Verification and Validation (V\&V) Project/Task
\#103723/05.30.02.  This paper describes objective technical results and
analysis. Any subjective views or opinions that might be expressed in the
paper do not necessarily represent the views of the U.S.\ Department of Energy
or the United States Government.  Sandia National Laboratories is a
multimission laboratory managed and operated by National Technology and
Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell
International, Inc., for the U.S.\  Department of Energy's National Nuclear
Security Administration under contract DE-NA-0003525.

\begin{appendices}
\section{Proper orthogonal decomposition}
Algorithm~\ref{alg:pod} presents the algorithm for computing the trial basis via proper orthogonal decomposition.
\begin{algorithm}
\caption{Algorithm for generating POD Basis.}
\label{alg:pod}
Input: Number of time-steps between snapshots $N_{\text{skip}}$; intercept $\stateInterceptArg{}$; basis dimension $K$ \; 
%\newline
Output: POD Basis $\basisspace \in \mathbb{V}_{K}(\RR{N})$ \;
%\newline
Steps:
\begin{enumerate}
    \item Solve FOM O$\Delta$E and collect solutions into snapshot matrix
$$\mathbf{S}(N_{\text{skip}}) \defeq \begin{bmatrix} \stateFOMDiscreteArg{0} - \stateInterceptArg{n} & \stateFOMDiscreteArg{N_{\text{skip}}} - \stateInterceptArg{n} & \cdots & \stateFOMDiscreteArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}} - \stateInterceptArg{n} \end{bmatrix}$$
    \item Compute the thin singular value decomposition, $$\mathbf{S} (N_{\text{skip}}) = \mathbf{U \Sigma Z}^T,$$
    where $\mathbf{U} \equiv \begin{bmatrix} \mathbf{u}_1 & \cdots & \mathbf{u}_{\text{floor}(N_t/N_{\text{skip}})}\end{bmatrix}$
    \item Truncate left singular vectors and to form the basis, $\basisspace \equiv \begin{bmatrix} \mathbf{u}_1 & \cdots & \mathbf{u}_K \end{bmatrix}$
\end{enumerate}


\end{algorithm}


\section{Selection of Sampling Points}
To construct the sampling point matrix used for hyper-reduction in the second numerical experiment, we employ q-sampling~\cite{qdeim_drmac} and the sample 
mesh concept~\cite{carlberg_gnat}. Algorithm~\ref{alg:qdeim} outlines the steps used in the second numerical experiment to compute the sampling points.

\begin{algorithm}
\caption{Algorithm for generating the sampling matrix through q-sampling.}
\label{alg:qdeim}
Input: Number of time-steps between snapshots $N_{\text{skip}}$, number of primal sampling points, $n_s$ \; 
%\newline
Output: Weighting matrix $\stweightingMatArg{} \equiv \stweightingMatOneArg{}^T \stweightingMatOneArg{} \in \{0,1\}^{N \times N}$ \;
%\newline
Steps:
\begin{enumerate}
    \item Solve FOM O$\Delta$E and collect velocity snapshots 
$$\mathbf{F}(N_{\text{skip}}) \defeq \begin{bmatrix} \velocity (\stateFOMDiscreteArg{0})  & \velocity( \stateFOMDiscreteArg{N_{\text{skip}}}) & \cdots & \velocity( \stateFOMDiscreteArg{\text{floor}(N_t/N_{\text{skip}})N_{\text{skip}}} ) \end{bmatrix}$$
    \item Compute the thin singular value decomposition, $$\mathbf{F} (N_{\text{skip}}) = \mathbf{U \Sigma Z}^T,$$
    where $\mathbf{U} \equiv \begin{bmatrix} \mathbf{u}_1 & \cdots & \mathbf{u}_{\text{floor}(N_t/N_{\text{skip}})}\end{bmatrix}$
    \item Compute the QR factorization of  $\mathbf{U}^T$ with column pivoting,
    \begin{equation*}
        \mathbf{U}^T \mathbf{P}^* = \mathbf{QR}
    \end{equation*}
    with $\mathbf{P}^* \equiv \begin{bmatrix} \mathbf{p}_1 & \cdots & \mathbf{p}_{\text{floor}(N_t/N_{\text{skip}})} \end{bmatrix}$, $\mathbf{p}_i \in \{0,1\}^N$. 
    \item Select the first $n_s$ columns of $\mathbf{P}^*$ to form the sampling point matrix $\stweightingMatOneArg{}^T \in \{0,1\}^{N \times n_s}$. 
    \item Augment the sampling point matrix, $\stweightingMatOneArg{}$, with additional columns such that all unknowns are computed at the mesh cells selected by Step 3. For the second numerical experiment, these additional unknowns correspond to each conserved variable and quadrature point in the selected cells. 
\end{enumerate}


\end{algorithm}



\section{Derivation of the Euler--Lagrange Equations}\label{appendix:eulerlagrange}
This section details the derivation of the Euler Lagrange equations. To this end, we consider the generic functional of the form,
\begin{equation}\label{eq:el_gen}
\mathcal{J}: (\statey,\stateyDot) \mapsto  \int_a^b \integrand(\statey(t),\stateyDot(t),t)  dt,
\end{equation}
where $\integrand: \RR{M} \times \RR{M} \times[a,b] \rightarrow  \RR{}$ (for arbitrary $M$) with $\integrand: (\stateyDiscrete,\stateyDotDiscrete,t) \mapsto \integrand(\stateyDiscrete,\stateyDotDiscrete,t)$. We now introduce the function $\statez: [a,b] \rightarrow \RR{M}$ that is implicitly defined as an extremum of~\eqref{eq:el_gen} subject to the boundary condition $\statez(a) = \statez_a$. 
We introduce an arbitrary function $\variation : [a,b] \rightarrow \RR{M}$ with the boundary condition $\variationArg{a} = \bz$ and define a variation from the extremal by,
\begin{align*}
\statezBar  &: (\tau;\delta) \mapsto \statez(\tau) + \delta \variation(\tau),\\
&: [a,b] \times \RR{} \rightarrow \RR{M}.
\end{align*}
We note that $\statezBar$ obeys the same boundary conditions as $\statez$ since $\variationArg{a} = \bz$.
Evaluating the functional~\eqref{eq:el_gen} at $\statezBar(t,\epsilon)$,
$$
\mathcal{J}(\epsilon) = \int_a^b \integrand(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t)  dt.
$$
The objective is to now find $\overline{\statez}$ that makes $\mathcal{J}(\epsilon)$ stationary. This can be done by differentiating with respect to $\epsilon$, and setting the result to zero,% Noting that the extermum of Eq.~\ref{eq:el_gen} occurs when $\state = \state^*$, and thus when $\epsilon = 0$, we have,
$$
\frac{d}{d\epsilon} \bigg[ \mathcal{J}\big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) \big)\bigg] =  0,
$$
or equivalently,
$$
\int_a^b \frac{d}{d\epsilon} \bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon),t \big)\bigg] dt=  0.
$$
%As $\statez$ is a stationary point of $\mathcal{J}$, it then directly follows that,
%\begin{equation}\label{eq:el1_appendix}
%\bigg[ \frac{d \mathcal{J}}{d \epsilon} (\statezBar(t;0),\statezDotBar(t;0) \bigg]= 0.
%\end{equation}
%This fact can be used to derive the Euler--Lagrange equations. Evaluating Eq.~\eqref{eq:el1_appendix},
%$$ \int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) \big) \bigg] = 0. $$
We first evaluate the derivative. First using the chain rule,
$$ \int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t\big) \bigg] dt = \int_a^b \bigg[ \frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t\big) \frac{\partial \statezBar }{\partial \delta}(t;\epsilon)  + \frac{\partial \integrand}{\partial \stateyDotDiscrete } \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \frac{\partial \statezDotBar}{\partial \delta} (t;\epsilon)\bigg]dt . $$
%$$\int_a^b \bigg[ \frac{\partial \integrand  }{\partial \statezBar } \frac{\partial \statezBar }{\partial \epsilon}\big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) \big)  + \frac{\partial \integrand}{\partial \statezDotBar }\frac{\partial \statezDotBar}{\partial \epsilon} \big(\statezBar(t,\epsilon),\statezDotBar(t,\epsilon) \big)\bigg]dt = 0. $$
%Setting $\epsilon = 0$,
%$$\int_a^b \bigg[ \frac{\partial \integrand  }{\partial \statezBar } \frac{\partial \statezBar }{\partial \epsilon}\big(\statezBar(t,0),\statezDotBar(t,0) \big)  + \frac{\partial \integrand}{\partial \statezDotBar }\frac{\partial \statezDotBar}{\partial \epsilon} \big(\statezBar(t,0),\statezDotBar(t,0) \big)\bigg]dt = 0. $$
Noting that,
$$\frac{\partial \statezBar}{\partial \delta} (\cdot,t)= \variation(t), \qquad \frac{\partial \statezDotBar }{\partial \delta}(\cdot,t)= \variationDot(t),$$
we have,
$$\int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \bigg] dt = \int_a^b \bigg[ \frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon),t \big) \variation(t)  + \frac{\partial \integrand}{\partial \stateyDotDiscrete } \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \variationDot(t) \bigg]dt. $$
%$$\int_a^b \bigg[\frac{\partial \integrand  }{\partial \overline{\genstate}} \variation + \frac{\partial \integrand}{\partial {\overline{\genstateDot}}}\variationDot \bigg]_{\epsilon = 0}dt = 0.$$
We integrate the second term by parts,
\begin{multline*}
\int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \bigg] dt =  \int_a^b  \bigg[\frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \variation(t) - \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial \stateyDotDiscrete }  \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \bigg) \variation(t) \bigg]dt + \\ \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statezBar(b;\epsilon),\statezDotBar(b;\epsilon) ,b \big) \variation(b)  - \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statezBar(a;\epsilon),\statezDotBar(a;\epsilon) ,a\big) \variation(a) .
\end{multline*}
Noting that $\variationArg{a} = \bz$,
\begin{multline*}
\int_a^b \frac{d}{d\epsilon}\bigg[ \integrand \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon),t \big) \bigg] dt =  \int_a^b  \bigg[\frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) , \big) \variation(t) - \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial \stateyDotDiscrete }  \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon) ,t \big) \bigg) \variation(t) \bigg]dt + \\ \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statezBar(b;\epsilon),\statezDotBar(b;\epsilon) ,b \big) \variation(b) . 
\end{multline*}
Setting equal to zero to find the stationary points,
$$
  \int_a^b  \bigg[\frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon),t \big) \variation(t) - \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial \stateyDotDiscrete }  \big(\statezBar(t;\epsilon),\statezDotBar(t;\epsilon),t \big) \bigg) \variation(t) \bigg]dt + \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statezBar(b;\epsilon),\statezDotBar(b;\epsilon),b \big) \variation(b)  = 0. 
$$
By definition, we know $\statez(\cdot) \equiv \statezBar(\cdot;0)$ is a stationary point. Thus, setting $\epsilon = 0$,
 $$
  \int_a^b  \bigg[\frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statez(t),\statezDot(t),t \big) \variation(t) - \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial \stateyDotDiscrete }  \big(\statez(t),\statezDot(t),t \big) \bigg) \variation(t) \bigg]dt + \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statez(b),\statezDot(b) ,b\big) \variation(b)  = 0. 
$$
%$$  \int_a^b \bigg[\frac{\partial \integrand  }{\partial \overline{\genstate}} \variation  - \frac{d}{dt}\frac{\partial \integrand}{\partial {\overline{\genstateDot}}}\variation \bigg]_{\epsilon = 0}dt + \bigg[\bigg[ \frac{\partial \integrand}{\partial {\overline{\genstateDot}}} \variation \bigg]_a^b\bigg]_{\epsilon = 0} = 0. $$
%At $\epsilon = 0$ we have $\overline{\genstate} = \genstate$ and, as $\variationArg{a} = 0$, we obtain,
%$$ \int_a^b \bigg(\frac{\partial \integrand  }{\partial \genstate} \variation  - \frac{\partial \integrand}{\partial {\genstateDot}}\variation \bigg)dt +  \frac{\partial \integrand}{\partial {\genstateDot}} \variation \bigg|_{t=b} = 0.$$
As $\variation$ is an arbitrary function, in order for the equality to hold we must have,
%\begin{equation}\label{eq:euler_lagrange_appendix}
%\bigg[ \frac{\partial \integrand  }{\partial \genstate} \bigg]^T  - \bigg[\frac{\partial \integrand}{\partial {\genstateDot}}\bigg]^T = \boldsymbol 0, 
%\end{equation}
\begin{equation}\label{eq:euler_lagrange_appendix}
\bigg[ \frac{\partial \integrand  }{\partial \stateyDiscrete } \big(\statez(t),\statezDot(t) ,t\big) \bigg]^T - \bigg[ \frac{d}{dt}\bigg( \frac{\partial \integrand}{\partial \stateyDotDiscrete }  \big(\statez(t),\statezDot(t) \big) \bigg) \bigg]^T = \bz  
\end{equation}
subject to,
$$
\statez(a) = \statez_a, \qquad \bigg[ \frac{\partial \integrand}{\partial \stateyDotDiscrete} \big(\statez(b),\statezDot(b),b \big) \variation(b) \bigg]^T = \bz.
%\genstate(a) = \genstate_a, \qquad \bigg[ \frac{\partial \integrand}{\partial {\genstateDot}} \bigg]^T_{t=b} = \bz.
$$
Equation~\eqref{eq:euler_lagrange_appendix} is known as the Euler--Lagrange equation. It states that, if $\statez$ is an extremal of $\mathcal{J}$, then it must satisfy~\eqref{eq:euler_lagrange_appendix}. It is emphasized that~\eqref{eq:euler_lagrange_appendix} is a necessary condition on $\statez$ to make $\mathcal{J}$ stationary, but it is not a sufficient condition. It is additionally noted that~\eqref{eq:euler_lagrange_appendix} provides a stationary point of $\mathcal{J}$, but the resulting stationary point can be either a maximum or a minimum.

\section{Evaluation of Gradients in the Euler--Lagrange Equations for \methodAcronym}\label{appendix:vector_calc} 
To obtain the specific form of the Euler--Lagrange equations for the \methodAcronym\ formulation, we need to evaluate the gradients in~\eqref{eq:el1} for the integrand,
\begin{align*}\label{eq:integrand_apx}
 \minintegrand & \vcentcolon
(\genstateyDiscreteArgnt{}, \genstateyDiscreteDotArgnt{} ,t) \mapsto \frac{1}{2} \big[
\basisspace \genstateyDiscreteDotArgnt{} - \velocity(\basisspace \genstateyDiscreteArgnt{}
+ \stateIntercept,t) \big]^T \stweightingMatArg{} \big[
\basisspace \genstateyDiscreteDotArgnt{}  - \velocity(\basisspace \genstateyDiscreteArgnt{} +
\stateIntercept,t) \big], \\ & 
\vcentcolon \RR{\romdim} \times \RR{\romdim} \times [0,T]
 \rightarrow \RR{} .  
\end{align*}
To evaluate the gradients, we first expand $\minintegrand$:
\begin{align*}
 \minintegrand(\genstateyDiscrete,\dot{\genstateyDiscrete},t)  &= \frac{1}{2} \big[ \basisspace \genstateyDiscreteDotArgnt{}  - \velocity(\basisspace \genstateyDiscreteArgnt{} + \stateIntercept,t) \big]^T \stweightingMatArg{n} \big[ \basisspace \genstateyDiscreteDotArgnt{} - \velocity(\veloargsromy) \big] \\ 
 &= \frac{1}{2}\big[\basisspace \genstateyDiscreteDotArgnt{}  \big]^T  \stweightingMatArg{n}  \big[\basisspace \genstateyDiscreteDotArgnt{} \big]  - \frac{1}{2}\big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{n}  \big[\basisspace \genstateyDiscreteDotArgnt{}\big] - \frac{1}{2} \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  \\ &+ \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big].
\end{align*}
Since $\stweightingMat$ is symmetric,
\begin{equation}\label{eq:int_expand}
 \minintegrand(\genstateyDiscrete,\genstateyDiscreteDot,t)  = \frac{1}{2}\big[\basisspace \genstateyDiscreteDotArgnt{}  \big]^T  \stweightingMatArg{n}  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]  -  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  + \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big].
\end{equation}
For notational purposes, we write the above as,
$$  \minintegrand(\genstateyDiscrete,\dot{\genstateyDiscrete},t) =  \minintegrand_1(\genstateyDiscrete,\genstateyDiscreteDot,t) +  \minintegrand_2(\genstateyDiscrete,\genstateyDiscreteDot,t) +  \minintegrand_3(\genstateyDiscrete,\genstateyDiscreteDot,t),$$
where
\begin{align*}
&\minintegrand_1(\genstateyDiscrete,\genstateyDiscreteDot,t) \equiv  \frac{1}{2}\big[\basisspace \genstateyDiscreteDotArgnt{}  \big]^T  \stweightingMatArg{n}  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big], \\
&   \minintegrand_2(\genstateyDiscrete,\genstateyDiscreteDot,t) \equiv   -  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big], \\
&  \minintegrand_3(\genstateyDiscrete,\genstateyDiscreteDot,t) \equiv  \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big] . 
\end{align*}
Constructing the Euler--Lagrange equations for this functional $\minintegrand$ requires evaluating the derivatives $\frac{\partial \minintegrand}{\partial \genstateyDiscrete}$ and $\frac{\partial \minintegrand}{\partial \genstateyDiscreteDot}$. We start by evaluating $\frac{\partial \minintegrand}{\partial \genstateyDiscrete}$ and go term by term.

Starting with $\minintegrand_1(\genstateyDiscrete,\genstateyDiscreteDot)$, we see,
$$\frac{\partial \minintegrand_1 }{\partial \genstateyDiscrete} = \boldsymbol 0,$$
where it is noted that $\mathcal{I}_1$ only depends on $\genstateyDiscreteDot$. Working with the second term:
\begin{align*}
\frac{\partial \minintegrand_2}{\partial \genstateyDiscrete}  &= -\frac{\partial}{\partial \genstateyDiscrete} \bigg( \big[ \basisspace \genstateyDiscreteDot \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big] \bigg) \\ 
&= - \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n} \frac{\partial}{\partial \genstateyDiscreteArgnt{}} \bigg( \big[ \velocity(\veloargsromy) \big]  \bigg)\\
 &= -\big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n}   \frac{\partial \velocity}{\partial \stateyDiscrete}\frac{\partial \stateyDiscrete}{\partial \genstateyDiscrete} \\
& = - \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace \\
&= -[\genstateyDiscreteDotArgnt{}]^T \basisspace^T \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete}  \basisspace.
\end{align*}
For $\minintegrand_3$,
\begin{align}\label{eq:i3sol}
\frac{\partial  \minintegrand_3}{\partial \genstateyDiscrete}  &= \frac{1}{2} \frac{\partial }{\partial \genstateyDiscrete} \bigg( \big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{n}\big[ \velocity(\veloargsromy) \big] \bigg) \\  
&=  [\velocity(\veloargsromy) ]^T \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \frac{\partial \stateyDiscrete}{\partial \genstateyDiscrete} \\
 &=   [\velocity(\veloargsromy) ]^T  \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
\end{align}
This gives the final expression,
$$
 \frac{\partial \minintegrand}{\partial \boldsymbol \genstateyDiscrete} = - \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace +  [\velocity(\veloargsromy) ]^T \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
$$%
We now evaluate $\frac{\partial \minintegrand}{\partial \genstateyDiscreteDotArgnt{}}$ and again go term by term. Starting with $\minintegrand_1$,
\begin{align*}
\frac{\partial \minintegrand_1}{\partial \genstateyDiscreteDotArgnt{}} &=
\frac{1}{2} \frac{\partial}{\partial \genstateyDiscreteDotArgnt{} }\bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]  \stweightingMatArg{n}  \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]  \bigg) \\
&= [\genstateyDiscreteDotArgnt{}]^T \basisspace^T \stweightingMatArg{n} \basisspace.
\end{align*}
Now working with the second term:
\begin{align*}
\frac{\partial \minintegrand_2}{\partial \genstateyDiscreteDotArgnt{} } &=
\frac{\partial}{\partial \genstateyDiscreteDotArgnt{}  } \bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  \bigg)\\
 &= \frac{\partial}{\partial \genstateyDiscreteDotArgnt{} } \bigg( [\genstateyDiscreteDotArgnt{}]^T \bigg) \basisspace^T  \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big] \\ 
 &= \bigg[  \basisspace^T  \stweightingMatArg{n}  \velocity(\veloargsromy )   \bigg]^T \\
 &= [\velocity(\veloargsromy) ]^T \stweightingMatArg{n} \basisspace.
\end{align*}
Finally, for the last term,
\begin{align*}
\frac{\partial \minintegrand_3}{\partial \genstateyDiscreteDotArgnt{} } &= \boldsymbol 0,
\end{align*}
where it is noted that $\mathcal{I}_3$ only depends on $\genstateyDiscrete$. We thus have,
$$\frac{\partial \minintegrand}{\partial \dot{ \genstateyDiscrete} } =   [\genstateyDiscreteDotArgnt{}]^T \basisspace^T \stweightingMatArg{n} \basisspace -  [\velocity(\veloargsromy ) ]^T \stweightingMatArg{n} \basisspace.$$
Combining all terms, the Euler--Lagrange equation reads,
\begin{multline*}
 \frac{\partial \minintegrand}{\partial \genstateyDiscrete}(\genstateArg{n}{t},\genstateDotArg{n}{t},t)  - \frac{d}{dt} \bigg[ \frac{\partial \minintegrand}{\partial \genstateyDiscreteDotArgnt{}} (\genstateArg{n}{t},\genstateDotArg{n}{t},t) \bigg] =  - \big[ \basisspace \genstateDotArg{n}{t}  \big]^T \stweightingMatArg{n} \big[  \frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsromn) \big] \basisspace \\ + [\velocity(\veloargsromn) ]^T \stweightingMatArg{n} \big[ \frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsromn) \big] \basisspace - \frac{d}{dt} \bigg[  [\genstateDotArg{n}{t} ]^T  \basisspace^T \stweightingMatArg{n} \basisspace - [\velocity(\veloargsromn) ]^T \stweightingMatArg{n} \basisspace \bigg]  = \boldsymbol 0.
\end{multline*}
To put this in a more recognizable form, we can pull out the common factor in the first two terms,
$$ - \bigg( \big[ \basisspace \genstateDotArg{n}{t} \big]^T  -  [\velocity(\veloargsromn)] ^T \bigg) \stweightingMatArg{n}
 \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \basisspace - \frac{d}{dt} \bigg[  [\genstateDotArg{n}{t}]^T \basisspace^T \stweightingMatArg{n} \basisspace -  
[\velocity(\veloargsromn) ]^T \stweightingMatArg{n} \basisspace  \bigg] = \boldsymbol 0.$$
Taking the transpose to put into the common column major format,
$$ -  \basisspace^T \big[ \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn)  \big]^T \stweightingMatArg{n} \bigg( \big[ \basisspace \genstateDotArg{n}{t}\big]  -  \velocity(\veloargsromn)  \bigg) -  \frac{d}{dt} \bigg[  \basisspace^T \stweightingMatArg{n} \basisspace \genstateDotArg{n}{t}  - \basisspace^T \stweightingMatArg{n} \velocity(\veloargsromn)   \bigg] = \bz.
 $$
We now factor the second term,
$$ -  \basisspace^T \big[ \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn)  \big]^T \stweightingMatArg{n} \bigg(  \basisspace \genstateDotArg{n}{t}  -  \velocity(\veloargsromn) \bigg) -  \basisspace^T \stweightingMatArg{n} \frac{d}{dt} \bigg[   \basisspace \genstateDotArg{n}{t}  - \velocity(\veloargsromn) \bigg] = \bz. $$
Gathering terms and multiplying by negative one, the final form of the Euler Lagrange equations are obtained,
$$ \bigg[\basisspace^T \big[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \big]^T \stweightingMatArg{n} + \basisspace^T \stweightingMatArg{n} \frac{d}{dt} \bigg] \bigg(  \basisspace \genstateDotArg{n}{t} -  \velocity(\veloargsromn) \bigg) = \bz. $$
This is the \methodAcronym-ROM. Note that this is a second order equation and can be written as two separate first order equations. Equation~\ref{eq:clspg_2ord} can be written equivalently as two first order equations by introducing an auxiliary ``costate" variable. Defining the costate as,
\begin{equation*}
%\adjoint^n(t) \defeq \basisspace^T \stweightingMatArg{n} \basisspace \genstateDotArg{n}{t}  -  \basisspace^T \stweightingMatArg{n} \velocity(\veloargsromn) ,
\adjoint^n(t) \defeq  \genstateDotArg{n}{t}  -  \mass^{-1} \basisspace^T \stweightingMatArg{n} \velocity(\veloargsromn) ,
\end{equation*}
we can manipulate Eq.~\ref{eq:clspg_2ord} as follows: %First, to enable hyper-reduction later on, we leverage the fact that $\stweightingMat^2 = \stweightingMat$,
%$$
% \bigg[\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn)\bigg]^T \stweightingMat^n \stweightingMat^n + \basisspace^T \stweightingMat^n \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}^n   -  \velocity(\veloargsromn) \bigg) = \bz.
%$$
First, we add and subtract the first term multiplied by $\basisspace [\massArgnt{n}]^{-1}\basisspace^T \stweightingMat$, 
$$
 \bigg[\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \bigg]^T \stweightingMatArg{n} \bigg( \mathbf{I} - \basisspace [\massArgnt{n}]^{-1} \basisspace^T\stweightingMatArg{n} + \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \bigg)  + \basisspace^T \stweightingMatArg{n}  \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}^n   -  \velocity(\veloargsromn) \bigg) = \bz.
$$
Pulling out the term multiplied by the positive portion of $\basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}$,
\begin{multline*} 
\bigg[\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} ( \veloargsromn) \bigg]^T \stweightingMatArg{n}\bigg( \mathbf{I} - \basisspace [\massArgnt{n}]^{-1}  \basisspace^T  \stweightingMatArg{n} \bigg)  + \\ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsromn) \bigg]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}  \basisspace^T \stweightingMatArg{n} +   \basisspace^T \stweightingMatArg{n} \frac{d}{dt} \bigg] \bigg(  \basisspace \genstateDotArg{n}{t}   -  \velocity(\veloargsromn) \bigg) = \bz.
\end{multline*}
Splitting into two separate terms, 
\begin{multline*}
\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \bigg]^T \stweightingMatArg{n}\bigg( \mathbf{I} - \basisspace [\massArgnt{n}]^{-1}  \basisspace^T \stweightingMatArg{n} \bigg)  \bigg(  \basisspace \genstateDotArg{n}{t}   -  \velocity(\veloargsromn) \bigg)  + \\  
\bigg[ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \bigg]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} +   \basisspace^T \stweightingMatArg{n} \frac{d}{dt} \bigg] \bigg(  \basisspace \genstateDotArg{n}{t}  -  \velocity(\veloargsromn) \bigg) = \bz.
\end{multline*}
Pulling $\mass^{-1} \basisspace^T \stweightingMatArg{n}$ inside the parenthesis on the second term,
\begin{multline*}
\basisspace^T \bigg[\frac{\partial \velocity}{\partial \state^n}(\veloargsromn) \bigg]^T \stweightingMatArg{n}\bigg( \mathbf{I} - \basisspace[\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \bigg)  \bigg(  \basisspace \genstateDotArg{n}{t}   -  \velocity(\veloargsromn) \bigg)  + \\  
\bigg[ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \bigg]^T \stweightingMatArg{n} \basisspace  +   \mass \frac{d}{dt} \bigg] \bigg( \genstateDotArg{n}{t}  - \mass^{-1} \basisspace^T \stweightingMatArg{n}  \velocity(\veloargsromn) \bigg) = \bz.
\end{multline*}
By definition, the term inside the parenthesis of the second term is $\adjointArg{n}{t}$,
\begin{multline*}
\basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \bigg]^T \stweightingMatArg{n}\bigg( \mathbf{I} - \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \bigg)  \bigg(  \basisspace \genstateDotArg{n}{t}    -  \velocity(\veloargsromn) \bigg)  +   \\ \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \bigg]^T \stweightingMatArg{n} \basisspace \adjointArg{n}{t}  +  \massArgnt{n} \frac{d}{dt} \adjointArg{n}{t}= \bz.
\end{multline*}
Re-arranging,
\begin{multline*}
\mass \frac{d }{dt}\adjointArg{n}{t} + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} ( \veloargsromn ) \bigg]^T \stweightingMatArg{n} \basisspace  \adjointArg{n}{t}  \\
= - \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn)\bigg]^T \stweightingMatArg{n}\bigg( \mathbf{I} - \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \bigg)  \bigg(  \basisspace \genstateDotArg{n}{t}   -  \velocity(\veloargsromn) \bigg).
\end{multline*}
We thus get the splitting,
%Eq.~\ref{eq:clspg_2ord} can be split as,
%\begin{equation}
\begin{align*}\label{eq:lspg_continuous_appendix}
& \mass \frac{d}{dt}\genstateArg{n}{t}  -  \basisspace^T \stweightingMatArg{n} \velocity(\veloargsromn) =  \mass \adjointArg{n}{t},\\
&\mass \frac{d}{dt} \adjointArg{n}{t}  + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \stateyDiscrete} (\veloargsromn) \bigg]^T \stweightingMatArg{n} \basisspace \adjointArg{n}{t} = \\
& -\basisspace^T \big[ \frac{\partial \velocity}{\partial \stateyDiscrete}(\veloargsromn) \big] ^T \stweightingMatArg{n}  \bigg( \mathbf{I} -   \basisspace [\massArgnt{n}]^{-1} \basisspace^T   \stweightingMatArg{n} \bigg)\bigg( \basisspace \genstateDotArg{n}{t}   -   \velocity(\veloargsromn) \bigg) . 
\end{align*}
\section{Evaluation of Gradients for Optimal Control Formulation}\label{appendix:optimal_control}
When formulated as an optimal control problem of Lagrange type, the gradients of the Hamiltonian with respect to the state, controller, and costate 
need to be evaluated. This section details this evaluation.

The Pontryagin Maximum Principle leverages the following Hamiltonian,
\begin{align*}
\hamiltonian^n \; &: \;  (\genstateyDiscreteArgnt{},\adjointDiscreteDumArgnt{},\controllerDiscreteDumArgnt{},t) \mapsto 
 \adjointDiscreteDumArgnt{T} \bigg[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArgnt{} \bigg] +  \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},t) \\
&: \; \RR{\romdim} \times \RR{\romdim} \times \RR{\romdim} \times [0,T] \rightarrow \RR{},
\end{align*} 
where,
\begin{align*}
 \objectiveControlArg{} &:  (\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},t)
\mapsto \frac{1}{2} \bigg[ \basisspace \bigg(  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArgnt{} \bigg) -
\velocity(\veloargsromy) \bigg]^T
\stweightingMatArg{n}  \\ & \hspace{1.5 in}\bigg[ \basisspace \bigg(
[\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArgnt{}
\bigg) - \velocity( \veloargsromy ) \bigg]
, \nonumber \\ & : \RR{\romdim} \times \RR{\romdim} \times [0,T] \rightarrow \RR{}.
\end{align*} 
As described in Section~\ref{sec:optimal_control}, to derive the stationary conditions of the \methodAcronym\ objective function, we require evaluating the following gradients,
$ \frac{\partial \hamiltonianArg{n}}{\partial \adjointDiscreteDumArgnt{}{}}, \;   \frac{\partial \hamiltonianArg{n}}{\partial \genstateyDiscreteArgnt{}{}}$, and $\frac{\partial \hamiltonianArg{n}}{\partial \controllerDiscreteDumArgnt{}{}}.$  Starting with $\frac{\partial \hamiltonianArg{n}}{\partial \adjointDiscreteDumArgnt{}{}}$, we have,
$$
\bigg[\frac{\partial \hamiltonianArg{n}}{\partial \adjointDiscreteDumArgnt{}{}}\bigg]^T = [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArgnt{} .
$$
Next, we address $ \frac{\partial \hamiltonianArg{n}}{\partial \genstateyArgnt{}{}}$. 
\begin{align*}
  \frac{\partial \hamiltonianArg{n}}{\partial \genstateyDiscreteArgnt{}{}} &= \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}} \bigg[  \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArgnt{} \big] +  \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg] \\
 &= \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}} \bigg[  \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromy
) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArgnt{} \big] \bigg]+  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg] \\
 &=   \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}} \bigg( \velocity(\veloargsromy)\bigg) \big] +  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg] \\
 &=   \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}  \frac{\partial \velocity}{\partial \stateyDiscrete} \frac{\partial \stateyDiscreteArgnt{}{}}{\partial  \genstateyDiscreteArgnt{}{} } \big] +  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg] \\
 &=   \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace  \big] +  \frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg]. 
\end{align*}
To evaluate $\frac{\partial}{\partial \genstateyDiscreteArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg]$, we leverage the previous result~\eqref{eq:int_expand} and insert $\genstateyDiscreteDotArgnt{} =  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArg{}{t} .$ This leads to the expression for the expanded Lagrangian,
\begin{multline*}
 \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) = \frac{1}{2}\big[\basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy)   + \basisspace [\massArgnt{n}]^{-1}\controllerDiscreteDumArg{}{t} \big]^T  \stweightingMatArg{n}  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy)   + \basisspace [\massArgnt{n}]^{-1}\controllerDiscreteDumArg{}{t}\big] \\  -  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy)  + \basisspace [\massArgnt{n}]^{-1}\controllerDiscreteDumArg{}{t} \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  + \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big].
\end{multline*}
Again for notational purposes, we split this into three terms,
$$
\objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg)  = 
\objectiveControlArg{}_1(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg)  + 
\objectiveControlArg{}_2(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg)  + 
\objectiveControlArg{}_3(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) ,$$
where
\begin{align*}
& \objectiveControlArg{}_1(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \equiv  \frac{1}{2}\big[\basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy)  + \basisspace [\massArgnt{n}]^{-1}\controllerDiscreteDumArg{}{t}  \big]^T  \stweightingMatArg{n}  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy)   +  \basisspace [\massArgnt{n}]^{-1}\controllerDiscreteDumArg{}{t} \big],\\
& \objectiveControlArg{}_2(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \defeq -  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy)  + \basisspace [\massArgnt{n}]^{-1} \controllerDiscreteDumArgnt{n}\big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big] ,\\
& \objectiveControlArg{}_3(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \defeq \frac{1}{2}\big[\velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big] .
\end{align*}
Evaluating the first term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_1 }{\partial \genstateyDiscreteArgnt{}{}} &= 
\frac{1}{2} \frac{\partial }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\velocity(\veloargsromy)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) \big] \bigg) +  \\
& \qquad \frac{\partial }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{n}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) \big] \bigg) + \\
& \qquad \frac{1}{2} \frac{\partial }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \controllerDiscreteDumArgnt{}\bigg) 
,\\ 
&= 
[\velocity(\veloargsromy)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace + 
[\controllerDumArgnt{n}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \statey} \basisspace , \\ 
%&= 
%[\velocity(\basisspace \genstateyArgnt{} + \stateIntercept)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  
% \frac{\partial \velocity}{\partial \statey} \basisspace + 
%[\controllerArgnt{n}]^T [\massArgnt{n}]^{-1} \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
%\stweightingMatArg{n} \frac{\partial \velocity}{\partial \statey} \basisspace , \\  
&= 
\bigg( [\velocity(\veloargsromy)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}  + [\controllerDiscreteDumArgnt{}]^T [\massArgnt{}]^{-1} \bigg)\basisspace^T  \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace ,\\
&= 
[\genstateyDiscreteDotArgnt{} ]^T \basisspace^T  \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace , \\
&= 
[\basisspace \genstateyDiscreteDotArgnt{} ]^T   \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace .
\end{align*}
Now evaluating the second term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_2 }{\partial \genstateyDiscreteArgnt{}{}} &= 
-\frac{\partial  }{\partial \genstateyDiscreteArgnt{}{}}   \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  - \frac{\partial  }{\partial \genstateyDiscreteArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T[\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  \bigg)  \\&= 
-\frac{\partial  }{\partial \genstateyDiscreteArgnt{}{}}    [\velocity(\veloargsromy)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T 
 \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  - 
  [\controllerDiscreteDumArgnt{}]^T[\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace 
\\
&= 
- 2[\velocity(\veloargsromy)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  
 \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace - 
 [\controllerDiscreteDumArgnt{}]^T[\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace, \\
&= - \bigg[ 2[\velocity(\veloargsromy)]^T  \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}  + [\controllerDiscreteDumArgnt{}]^T  [\massArgnt{n}]^{-1} \bigg] \basisspace^T \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace ,\\
&= -\bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} ]^T  + \velocity(\veloargsromy)^T  \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \bigg) \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace. 
\end{align*}
Next, we can use the result~\eqref{eq:i3sol} to have,
$$ \frac{\partial  \objectiveControlArg{}_3 }{\partial \genstateyDiscreteArgnt{}{}} =
  [\velocity(\veloargsromy) ]^T  \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
$$
Thus we have,
\begin{align*}
\frac{\partial \objectiveControlArg{}}{\partial \genstateyDiscreteArgnt{}} &= 
[\basisspace \genstateyDiscreteDotArgnt{} ]^T   \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace -
\bigg( \big[ \basisspace \genstateyDiscreteDotArgnt{} ]^T  + \velocity(\veloargsromy)]^T  \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T  \bigg) \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace + 
 [\velocity(\veloargsromy) ]^T  \stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace, \\
&= 
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)  \stweightingMatArg{n}   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace -
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)  \stweightingMatArg{n}
\frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace \\ 
&= 
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)   \bigg( \stweightingMatArg{n} \basisspace  [\massArgnt{n}]^{-1}\basisspace^T - \mathbf{I} \bigg)
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace, 
\end{align*}
such that,
\begin{equation*}
\frac{\partial \hamiltonianArg{n}}{\partial \genstateyDiscreteArgnt{}{}} = 
  \adjointDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}  \frac{\partial \velocity}{\partial \stateyDiscrete}\basisspace  \big]  + 
\bigg( [\basisspace \genstateyDiscreteDotArgnt{} ]^T - \velocity(\veloargsromy)^T \bigg)   \bigg( \stweightingMatArg{n} \basisspace  [\massArgnt{n}]^{-1}\basisspace^T - \mathbf{I} \bigg)
\stweightingMatArg{n} \frac{\partial \velocity}{\partial \stateyDiscrete} \basisspace.
\end{equation*}
Or, equivalently,
\begin{equation*}
\bigg[ \frac{\partial \hamiltonianArg{n}}{\partial \genstateyDiscreteArgnt{}{}} \bigg]^T =  \basisspace^T \big[  \frac{\partial \velocity}{\partial \stateyDiscrete} \big]^T \stweightingMatArg{n}  \basisspace [\massArgnt{n}]^{-1}  \adjointDiscreteDumArgnt{} + 
 \basisspace^T \big[  \frac{\partial \velocity}{\partial \stateyDiscrete} \big]^T 
  \stweightingMatArg{n} \bigg(  \basisspace  [\massArgnt{n}]^{-1}\basisspace^T  \stweightingMatArg{n}- \mathbf{I} \bigg)
 \bigg( \basisspace \genstateyDiscreteDotArgnt{} - \velocity(\veloargsromy) \bigg) 
\end{equation*}
Next, we evaluate $\frac{\partial \hamiltonianArg{n}}{\partial \controllerDumArgnt{}}$:
\begin{align}
  \frac{\partial \hamiltonianArg{n}}{\partial \controllerDumArgnt{}{}} &= \frac{\partial}{\partial \controllerDumArgnt{}{}} \bigg[  \adjointDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDumArgnt{} \big] +  \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},t) \bigg] \\
 &= \frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}} \bigg[  \adjointDiscreteDumArgnt{T} \big[  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromy) + [\massArgnt{n}]^{-1}\controllerDiscreteDumArgnt{} \big] \bigg]+  \frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg] \\
 &= [\adjointDiscreteDumArgnt{}]^T[\massArgnt{n}]^{-1} + \frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},t) \bigg]. 
\end{align}
To evaluate $\frac{\partial}{\partial \controllerDiscreteDumArgnt{}{}}  \bigg[ \objectiveControlArg{}(\genstateyDiscreteArgnt{},\controllerDiscreteDumArgnt{},\timeDumArg) \bigg]$, we again 
go term by term. Starting with the first term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_1 }{\partial \genstateyDiscreteArgnt{}{}} &= 
\frac{1}{2} \frac{\partial }{\partial \controllerDiscreteDumArgnt{}} \bigg( [\velocity(\veloargsromy)]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) \big] \bigg) + \\
& \qquad \frac{\partial }{\partial \controllerDiscreteDumArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{n}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) \big] \bigg) + \\
& \qquad \frac{1}{2} \frac{\partial }{\partial \controllerDiscreteDumArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \controllerDiscreteDumArgnt{}\bigg) \\
&= \bigg(  [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n}  \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T                      
\stweightingMatArg{n}  \velocity(\veloargsromy) \big] \bigg)^T +   [\controllerDiscreteDumArgnt{}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} \\
&= \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}\basisspace^T  \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} +   [\controllerDiscreteDumArgnt{}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}. 
\end{align*}
Moving on to the second term,
\begin{align*}
\frac{\partial  \objectiveControlArg{}_2 }{\partial \controllerDiscreteDumArgnt{}{}} &= 
-\frac{\partial  }{\partial \controllerDiscreteDumArgnt{}{}}   \big[ \basisspace  [\massArgnt{n}]^{-1}\basisspace^T
\stweightingMatArg{n}  \velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  - \frac{\partial  }{\partial \controllerDiscreteDumArgnt{}{}} \bigg( [\controllerDiscreteDumArgnt{}]^T[\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \big[ \velocity(\veloargsromy) \big]  \bigg)  \\
&=- \bigg[ [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \velocity(\basisspace \genstateyDiscrete + \stateIntercept) \bigg]^T \\
&= - [\velocity(\veloargsromy) ]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}. 
\end{align*}
For the third term we have simply,
$$ \frac{\partial  \objectiveControlArg{}_3 }{\partial \controllerDiscreteDumArgnt{}{}} = \bz .$$
Thus,
\begin{align*}
\frac{\partial \hamiltonianArg{n}}{\partial \controllerDiscreteDumArgnt{}} &= 
[\adjointDiscreteDumArgnt{}]^T[\massArgnt{n}]^{-1}  +  \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}\basisspace^T  \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} + \\
& \qquad   [\controllerDiscreteDumArgnt{}]^T [\massArgnt{n}]^{-1} \basisspace^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1} - [\velocity(\veloargsromy) ]^T \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}, \\
& = \bigg( [\adjointDiscreteDumArgnt{}]^T - [\velocity(\veloargsromy) ]^T \stweightingMatArg{n} \basisspace \bigg) [\massArgnt{n}]^{-1}  + \bigg(  \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \basisspace + [\controllerDiscreteDumArgnt{}]^T   \bigg) [\massArgnt{n}]^{-1}\basisspace^T  \stweightingMatArg{n} \basisspace [\massArgnt{n}]^{-1}  \\
& = \bigg( [\adjointDiscreteDumArgnt{}]^T - [\velocity(\veloargsromy) ]^T \stweightingMatArg{n} \basisspace \bigg) [\massArgnt{n}]^{-1}  + \bigg(  \big[ \velocity(\veloargsromy) \big]^T \stweightingMatArg{n} \basisspace + [\controllerDiscreteDumArgnt{}]^T   \bigg) [\massArgnt{n}]^{-1}  \\
& =  [\adjointDiscreteDumArgnt{}]^T  [\massArgnt{n}]^{-1}  +  [\controllerDiscreteDumArgnt{}]^T   [\massArgnt{n}]^{-1}  
\end{align*}
Or, equivalently,
\begin{equation*}
\frac{\partial \hamiltonianArg{n}}{\partial \controllerDiscreteDumArgnt{}} = 
  [\massArgnt{n}]^{-1} [\adjointDiscreteDumArgnt{} +  \controllerDiscreteDumArgnt{}]. 
\end{equation*}
Thus, the conditions stated via the Pontryagin Maximum Principle are,
\begin{align*}%\label{eq:hamiltonian_sysa}
&\frac{d}{dt}\genstateArg{n}{t} =  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\veloargsromn) + [\massArgnt{n}]^{-1}\controllerArg{n}{t}\\ 
&\frac{d }{dt} \adjointArg{n}{t}  + \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{n}  \basisspace [\massArgnt{n}]^{-1}  \adjointArg{n}{t} = 
 - \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{n} 
\bigg(  \basisspace  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n} - \mathbf{I} \bigg)
 \bigg( \basisspace \genstateDotArg{n}{t} - \velocity(\basisspace \genstateArg{n}{t} + \stateIntercept) \bigg) \\ 
& \adjointArg{n}{t}= -\controllerArg{n}{t}.
\end{align*}
Equivalently,
\begin{align*}%\label{eq:hamiltonian_sys}
&\frac{d}{dt}\genstateArg{n}{t} =  [\massArgnt{n}]^{-1}\basisspace^T \stweightingMatArg{n}\velocity(\basisspace \genstateArg{n}{t} + \stateIntercept) + [\massArgnt{n}]^{-1}\controllerArg{n}{t} \\ 
&\frac{d}{dt} \controllerArg{n}{t}  + \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{n}  \basisspace [\massArgnt{n}]^{-1}  \controllerArg{n}{t} = 
 - \basisspace^T \big[  \frac{\partial \velocity}{\partial \statey} \big]^T \stweightingMatArg{n} 
\bigg( \mathbf{I} -   \basisspace  [\massArgnt{n}]^{-1}\basisspace^T \bigg)
\stweightingMatArg{n} \bigg( \basisspace \genstateDotArg{n}{t} - \velocity(\basisspace \genstateArg{n}{t} + \stateIntercept) \bigg) 
\end{align*}


\begin{comment}

====================
To do this, first write the above as,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T + \basisspace \basisspace^T \bigg)  \stweightingMat^T + \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
Re-arranging gives,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg]  +  \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T  \basisspace   +  \frac{d}{dt} \bigg] \bigg( \basisspace^T \stweightingMat \basisspace \dot{\genstate}   -  \basisspace^T \stweightingMat  \velocity \bigg) = 0. $$
Defining the adjoint as,
\begin{equation}
\basisspace^T \stweightingMat \basisspace \frac{d \genstate}{dt}  -  \basisspace^T \stweightingMat \velocity =  \boldsymbol \lambda , \qquad \genstate(t=0) = \genstate_0,
%\basisspace \frac{d \genstate}{dt}  -   \velocity =  \adjoint  , \qquad \genstate(t=0) = \genstate_0
\end{equation}
we then get the auxiliary adjoint equation,
\begin{equation}
%\basisspace^T \stweightingMat^T \frac{d}{dt} \adjoint  + \basisspace^T \frac{\partial \velocity}{\partial \genstate}^T \stweightingMat^T \adjoint = 0, \qquad \basisspace^T \stweightingMat \adjoint(t=T) = 0
 \frac{d}{dt} \adjoint  + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \state} \bigg]^T \basisspace \adjoint = -\bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg] , \qquad \adjoint(t=T) = 0.
\end{equation}
\end{comment}
\begin{comment}
=================================

%We start $ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate}$. We go term by term and note the useful link: \url{https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions}
%First term
\begin{enumerate}
\item First term: Only depends on $\dot{\genstate}$, term is zero
\item Second term: $$\frac{\partial}{\partial \genstate} \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)=  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \frac{\partial}{\partial \genstate} \bigg( \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg)$$
$$ = \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat   \frac{\partial \boldsymbol f(\state)}{\partial \state}\frac{\partial \state}{\partial \genstate} $$
$$ =  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace $$
%$${\color{red} \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big] \bigg) = \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \bigg) \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \basisspace.$$
%$${\color{red} ?  = \big[ \frac{\partial \velocity}{\partial \state}\frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
%$${\color{red} ?  = \big[ \basisspace \frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
\item Third term: 
$$\frac{1}{2} \frac{\partial }{\partial \genstate} \bigg( \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)  =  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ =   \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisspace$$
\end{enumerate}
This gives us,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} = - \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace +  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace $$


Now we work on $\frac{\partial \minintegrand}{\partial \dot{ \genstate } }$:
\begin{enumerate}
\item First term:
$$\frac{1}{2} \frac{\partial}{\partial \dot{\genstate}}\bigg( \big[\dot{\genstate}^T  \basisspace^T \big]  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  \bigg) =  \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace$$
\item Second term:
$$\frac{\partial}{\partial \dot{\genstate} } \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg) = \frac{\partial}{\partial \dot{\genstate} } \bigg( \dot{\genstate}^T \bigg) \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  $$
$$ = \bigg[  \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg]^T $$
$$ = \velocity^T \stweightingMat^T \basisspace$$
\end{enumerate}
This gives,
$$\frac{\partial \minintegrand}{\partial \dot{ \genstate } } =   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace -  \velocity^T \stweightingMat^T \basisspace.$$
%\end{comment}

\begin{comment}
Let $\stweightingMat = \mathbf{I}$,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} - \frac{d}{dt} \bigg[ \frac{\partial \boldsymbol F}{\partial \dot{\genstate}} \bigg] =   \bigg( -\big[ \basisspace \dot{\genstate} \big]^T  +  \velocity^T \bigg) \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T  - \velocity^T \basisspace \bigg] $$
Transposing,
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[   \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[ \basisspace^T \basisspace  \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ =    -\basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( \basisspace \dot{\genstate}  -  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ = -\bigg[ \basisspace^T \frac{d}{dt} + \basisspace^T \big[ \frac{\partial \velocity}{\partial \state} \big]^T  \bigg] \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$ 
Adjoint style
$$ =   \basisspace^T  \frac{d }{dt}\lambda  +   \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \lambda  $$
$$ \basisspace  \dot{\genstate}  -\velocity = \lambda$$
%Gathering terms



\end{comment}
%\begin{comment}

\begin{comment}

\subsection{Time-varying spacetime subspace}
We have $\state(t)= \basisst(t) \genstate + \state_0$. Note that, in this case, the generalized coordinates are stationary in time, $\dot{\genstate} = \mathbf{0}$. Thus we only need to compute $\frac{\partial \minintegrand}{\partial \genstate}$. Going term by term:

\begin{enumerate}
\item First term: 
$$\frac{1}{2} \frac{\partial}{\partial \genstate} \bigg(\big[\frac{\partial \decoder}{\partial t}  \big]^T \stweightingMat  \big[\frac{\partial \decoder}{\partial t} \big]\bigg) = 
\frac{\partial}{\partial \genstate} \bigg(\big[\frac{\partial \basisst}{\partial t} \genstate \big]^T \stweightingMat  \big[\frac{\partial \basisst}{\partial t} \genstate \big]\bigg) $$
$$
= \genstate^T \dot{\basisst}^T \stweightingMat \dot{ \basisst }
$$
\item Second term: $$\frac{\partial}{\partial \genstate} \bigg( \big[ \frac{\partial \decoder}{\partial t} \big]^T \stweightingMat \big[ \velocity(\decoder,t;\param) \big] \bigg)= \frac{\partial}{\partial \genstate} \bigg( \big[ \frac{\partial \basisst}{\partial t} \genstate  \big]^T \stweightingMat  \big[ \velocity(\decoder,t;\param) \big]  \bigg)$$
$$ = \big[ \frac{\partial \basisst}{\partial t} \genstate  \big]^T \stweightingMat \frac{\partial}{\partial \genstate}\bigg( \big[ \velocity(\decoder,t;\param) \big] \bigg) +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T \frac{\partial}{\partial \genstate} \bigg( \big[ \frac{\partial \basisst}{\partial t} \genstate  \big] \bigg)$$
$$ = \big[ \dot{\basisst} \genstate  \big]^T \stweightingMat \frac{\partial \velocity}{\partial \decoder} \frac{\partial \decoder}{\partial \genstate} +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T  \dot{ \basisst}$$
$$ = \big[ \dot{\basisst} \genstate  \big]^T \stweightingMat \frac{\partial \velocity}{\partial \decoder} \basisst +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T  \dot{ \basisst}$$
\item Third term: 
$$\frac{1}{2} \frac{\partial }{\partial \genstate} \bigg( \big[\velocity(\decoder,t;\param) \big]^T \stweightingMat \big[ \velocity(\decoder,t;\param) \big] \bigg)  =  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ =   \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisst$$
Putting it all together we get,
$$ \genstate^T \dot{\basisst}^T \stweightingMat \dot{\basisst} - \bigg(  \big[ \dot{\basisst} \genstate  \big]^T \stweightingMat \frac{\partial \velocity}{\partial \decoder} \basisst +  \big[ \velocity(\decoder,t;\param) \big]^T \stweightingMat^T  \dot{ \basisst} \bigg) + \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisst = \mathbf{0}$$
Transposing, we obtain the Euler--Lagrange equations for a space-time basis
$$ \basisst^T \stweightingMat^T \dot{\basisst} \genstate - \bigg( \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \dot{\basisst} \genstate + \dot{\basisst}^T \stweightingMat \velocity  \bigg) + \basisst^T  \big[ \frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \velocity  = \mathbf{0}, \qquad \state(t=0) = \state_0.$$
Note that the additional boundary condition is automatically satisfied.
$$ \dot{\basisst}^T \stweightingMat^T \bigg( \dot{\basisst} \genstate -    \velocity \bigg)   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \dot{\basisst} \genstate  + \basisst^T  \big[ \frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \velocity  = \mathbf{0}, \qquad \state(t=0) = \state_0.$$
$$ \dot{\basisst}^T \stweightingMat^T \bigg( \dot{\basisst} \genstate -    \velocity \bigg)   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \bigg( \dot{\basisst} \genstate  -  \velocity \bigg) = \mathbf{0} $$
$$ \bigg[ \dot{\basisst}^T \stweightingMat^T  - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \bigg]  \bigg( \dot{\basisst} \genstate -    \velocity \bigg) = \mathbf{0}$$

Note that we can write this as two systems. To put it in the best form, write the above as,
$$ \dot{\basisst}^T \bigg( \mathbf{I} -  \basisst \basisst^T  +  \basisst \basisst^T \bigg)  \stweightingMat^T \bigg( \dot{\basisst} \genstate -    \velocity \bigg)   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \bigg( \mathbf{I} -  \basisst \basisst^T  +  \basisst \basisst^T \bigg)\stweightingMat^T \bigg( \dot{\basisst} \genstate  -  \velocity \bigg) = \mathbf{0} $$
Defining,
$$  \basisst^T \stweightingMat^T \dot{\basisst} \genstate  -  \basisst^T \stweightingMat^T \velocity = \boldsymbol \lambda,$$
we have,
$$ \bigg[ \dot{\basisst}^T   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \bigg] \basisst \adjoint  = -\bigg[\dot{\basisst}^T   - \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \bigg]  \bigg( \mathbf{I} -  \basisst \basisst^T  \bigg)\stweightingMat^T \bigg( \dot{\basisst} \genstate  -  \velocity \bigg)$$

%The above can be written as two systems,
%$$  \dot{\basisst}^T \stweightingMat^T \dot{\basisst} \genstate - \dot{\basisst}^T \stweightingMat \velocity  = \boldsymbol \lambda $$
%$$  \basisst^T \big[\frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \dot{\basisst} \genstate  - \basisst^T  \big[ \frac{\partial \velocity}{\partial \decoder} \big]^T \stweightingMat^T \velocity  = \lambda$$


\end{enumerate}
\end{comment}

\begin{comment}
\textbf{Linear subspace with full temporal support}:
For a linear subspace with full temporal support, we have $\state = \basisspace \genstate + \state_0$
$ \frac{\partial \state}{\partial \genstate} = \basisspace.$
We have,
$$\minintegrand = \frac{1}{2} \big[ \basisspace \dot{\genstate} \big]^T  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  -  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] 
  + \frac{1}{2} \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]$$
Write this as,
$$\minintegrand = \minintegrand_1 + \minintegrand_2 + \minintegrand_3 $$

%other form
%$${\color{red} = \big[ \basisspace \dot{\genstate} \big]^T  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  - 2 \big[ \velocity(\state(\genstate),t;\param) \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big] + \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] }$$

First we compute $ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate}$. We go term by term and note the useful link: \url{https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions}
%First term
\begin{enumerate}
\item First term: Only depends on $\dot{\genstate}$, term is zero
\item Second term: $$\frac{\partial}{\partial \genstate} \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)=  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \frac{\partial}{\partial \genstate} \bigg( \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg)$$
$$ = \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat   \frac{\partial \boldsymbol f(\state)}{\partial \state}\frac{\partial \state}{\partial \genstate} $$
$$ =  \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace $$
%$${\color{red} \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big] \bigg) = \frac{\partial}{\partial \genstate} \bigg(\big[ \velocity(\state(\genstate),t;\param) \big]^T \bigg) \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ = \dot{\genstate}^T \basisspace^T \stweightingMat^T \frac{\partial \velocity}{\partial \state} \basisspace.$$
%$${\color{red} ?  = \big[ \frac{\partial \velocity}{\partial \state}\frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
%$${\color{red} ?  = \big[ \basisspace \frac{\partial \state}{\partial \genstate} \big]^T \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  }  $$
\item Third term: 
$$\frac{1}{2} \frac{\partial }{\partial \genstate} \bigg( \big[\velocity(\state(\genstate),t;\param) \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big] \bigg)  =  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \frac{\partial \state}{\partial \genstate}.$$
$$ =   \velocity^T \stweightingMat\frac{\partial \velocity}{\partial \state} \basisspace$$
\end{enumerate}
This gives us,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} = - \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace +  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace $$
Now we work on $\frac{\partial \minintegrand}{\partial \dot{ \genstate } }$:
\begin{enumerate}
\item First term:
$$\frac{1}{2} \frac{\partial}{\partial \dot{\genstate}}\bigg( \big[\dot{\genstate}^T  \basisspace^T \big]  \stweightingMat  \big[ \basisspace \dot{\genstate} \big]  \bigg) =  \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace$$
\item Second term:
$$\frac{\partial}{\partial \dot{\genstate} } \bigg( \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg) = \frac{\partial}{\partial \dot{\genstate} } \bigg( \dot{\genstate}^T \bigg) \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  $$
$$ = \bigg[  \basisspace^T  \stweightingMat \big[ \velocity(\state(\genstate),t;\param) \big]  \bigg]^T $$
$$ = \velocity^T \stweightingMat^T \basisspace$$
\end{enumerate}
This gives,
$$\frac{\partial \minintegrand}{\partial \dot{ \genstate } } =   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace -  \velocity^T \stweightingMat^T \basisspace.$$
Putting it all together, the Euler--Lagrange equation reads,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} - \frac{d}{dt} \bigg[ \frac{\partial \minintegrand}{\partial \dot{\genstate}} \bigg] =  - \big[ \basisspace \dot{\genstate} \big]^T \stweightingMat  \frac{\partial \boldsymbol f(\state)}{\partial \state}\basisspace +  \velocity^T \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace - \velocity^T \stweightingMat^T \basisspace \bigg]  = 0$$
Factoring the first term,
$$ - \bigg( \big[ \basisspace \dot{\genstate} \big]^T  -  \velocity^T \bigg) \stweightingMat \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T \basisspace^T \stweightingMat \basisspace -  \velocity^T \stweightingMat^T \basisspace  \bigg] = 0$$
Taking the transpose,
$$ -  \basisspace^T \frac{\partial \velocity}{\partial \state}^T \stweightingMat^T \bigg( \big[ \basisspace \dot{\genstate} \big]  -  \velocity \bigg) -  \frac{d}{dt} \bigg[  \basisspace^T \stweightingMat^T \basisspace \dot{\genstate} - \basisspace^T \stweightingMat \velocity  \bigg] = 0.
 $$
Noting that $\stweightingMat = \stweightingMat^T$, we factor the second term,
$$ -  \basisspace^T \frac{\partial \velocity}{\partial \state}^T \stweightingMat^T \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) -  \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg[   \basisspace \dot{\genstate} - \velocity  \bigg] = 0. $$
Gathering terms and multiplying by negative one,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \stweightingMat^T + \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
%Equivalently,
%$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T  \basisspace \basisspace^T \stweightingMat^T + \basisspace^T  \basisspace \basisspace^T \stweightingMat^T  \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
%Pulling in the $\stweightingMat^T \basisspace^T$,
%$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \basisspace  +  \frac{d}{dt} \bigg] \bigg(  \basisspace^T \stweightingMat \basisspace \dot{\genstate}   -  \basisspace^T \stweightingMat \velocity \bigg) = 0. $$

This is the time-continuous LSPG ROM. Note that this is a second order equation and can be written as two separate first order equations. To do this, first write the above as,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T + \basisspace \basisspace^T \bigg)  \stweightingMat^T + \basisspace^T \stweightingMat^T \frac{d}{dt} \bigg] \bigg(  \basisspace \dot{\genstate}   -  \velocity \bigg) = 0. $$
Re-arranging gives,
$$ \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg]  +  \bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T  \basisspace   +  \frac{d}{dt} \bigg] \bigg( \basisspace^T \stweightingMat \basisspace \dot{\genstate}   -  \basisspace^T \stweightingMat  \velocity \bigg) = 0. $$
Defining the adjoint as,
\begin{equation}
\basisspace^T \stweightingMat \basisspace \frac{d \genstate}{dt}  -  \basisspace^T \stweightingMat \velocity =  \boldsymbol \lambda , \qquad \genstate(t=0) = \genstate_0,
%\basisspace \frac{d \genstate}{dt}  -   \velocity =  \adjoint  , \qquad \genstate(t=0) = \genstate_0
\end{equation}
we then get the auxiliary adjoint equation,
\begin{equation}
%\basisspace^T \stweightingMat^T \frac{d}{dt} \adjoint  + \basisspace^T \frac{\partial \velocity}{\partial \genstate}^T \stweightingMat^T \adjoint = 0, \qquad \basisspace^T \stweightingMat \adjoint(t=T) = 0
 \frac{d}{dt} \adjoint  + \basisspace^T \bigg[\frac{\partial \velocity}{\partial \state} \bigg]^T \basisspace \adjoint = -\bigg[\basisspace^T \frac{\partial \velocity}{\partial \state}^T \bigg( \mathbf{I} -   \basisspace \basisspace^T \bigg)  \stweightingMat^T \bigg( \basisspace \dot{\genstate}   -   \velocity \bigg) \bigg] , \qquad \adjoint(t=T) = 0.
\end{equation}

\end{comment}

\begin{comment}
Let $\stweightingMat = \mathbf{I}$,
$$ \frac{\partial \minintegrand}{\partial \boldsymbol \genstate} - \frac{d}{dt} \bigg[ \frac{\partial \boldsymbol F}{\partial \dot{\genstate}} \bigg] =   \bigg( -\big[ \basisspace \dot{\genstate} \big]^T  +  \velocity^T \bigg) \frac{\partial \velocity}{\partial \state} \basisspace - \frac{d}{dt} \bigg[   \dot{\genstate}^T  - \velocity^T \basisspace \bigg] $$
Transposing,
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[   \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)- \frac{d}{dt} \bigg[ \basisspace^T \basisspace  \dot{\genstate}  -\basisspace^T  \velocity \bigg] $$
$$ =    \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( -\big[ \basisspace \dot{\genstate} \big]  +  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ =    -\basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \bigg( \basisspace \dot{\genstate}  -  \velocity \bigg)-  \basisspace^T  \frac{d}{dt} \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$
$$ = -\bigg[ \basisspace^T \frac{d}{dt} + \basisspace^T \big[ \frac{\partial \velocity}{\partial \state} \big]^T  \bigg] \bigg[\basisspace  \dot{\genstate}  -\velocity \bigg] $$ 
Adjoint style
$$ =   \basisspace^T  \frac{d }{dt}\lambda  +   \basisspace^T  \big[ \frac{\partial \velocity}{\partial \state} \big]^T   \lambda  $$
$$ \basisspace  \dot{\genstate}  -\velocity = \lambda$$
%Gathering terms
\end{comment}



\end{appendices}

%\section*{References}
\bibliographystyle{siam}
\bibliography{refs}
\end{document}


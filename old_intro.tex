%===ABSTRACT
% In contrast to (1.) the Galerkin method, which minimizes the time-instantaneous residual, (2.) the Least-Squares Petrov--Galerkin (LSPG) method, which sequentially minimizes the discrete residual over a single time step, and (3.) the Space--Time LSPG method, which minimizes the discrete space–time residual,
%In contrast to existing residual minimization model-reduction approaches, which generate solutions in a low-dimensional space--time trial space that either minimizes (1.) the time-continuous residual over an infinitesimal time-window (Galerkin), (2.) the time-discrete residual over a single time-step instance (Least-Squares Petrov--Galerkin), or (3.) the time-discrete residual over the entire time domain (Space--Time Least-Squares Petrov--Galerkin), 
%the proposed approach minimizes the
%	time-continuous full-order-model residual over a low-dimensional space--time
%	trial subspace across a sequence of time windows. Among other advantages, this process yields an approach that overcomes the 
%time-step and time-scheme dependence that the least-squares Petrov--Galerkin method is subject to. 
%that decompose the time
%	domain of interest.

%	domain of interest. The proposed approach overcomes the time-step and time-scheme dependence that the least-squares Petrov--Galerkin method is subject to.  
%	\KTC{Why are we saying the next two sentences? These seem kind of random.
%	The abstract should explain why the work is necessary; this more reads like
%	a list of interesting factoids.} %The forward system, which is forced by an auxiliary costate, evolves the generalized coordinates of the reduced-order model. The backwards system, which is forced by the FOM residual evaluated about the ROM state, governs the dynamics of the costate.
%For spatial trial spaces, the stationary conditions for the time-continuous minimization problems are obtained 
%by deriving the associated Euler--Lagrange equations. 
%The stationary conditions for the resulting minimization problems are derived. Of particular interest, the stationary conditions for trial spaces with spatial dimension reduction are obtained via the Euler–Lagrange equations and comprise a coupled two-point Hamiltonian boundary value problem containing a forward and backward system. 
%  We show that the proposed method
%	comprises a generalization of many existing techniques and provides a
%	unified framework to reason about existing model-redcution techniques. In
%	particular, we show that specific instances of the proposed method lead to
%	the existing reduced-order modeling techniques of Galerkin projection,
%	least-squares Petrov--Galerkin (LSPG) projection, and space--time
%	least-squares Petrov--Galerkin (ST-LSPG) projection. 
%By formulating the least--squares problem from the time-continuous level, the \methodAcronym\ formulation overcomes 
%the time-step sensitivity and time-scheme dependence that LSPG is subject to. 
%The stationary points of the minimization problems are obtained by solving the associated Euler-Lagrange equations. The Euler-Lagrange equations consist of a set of coupled two point boundary value problems defined on each time-slab containing a forward primal system and a backward adjoint system. %The approach is conceptually similar to the classic Least-Squares Petrov-Galerkin approach, but displays several advantages. First, by formulating the residual minimization problem at the continuous level, the sensitivity of the resulting method to the underlying time-discretization scheme is greatly reduced. 
%This work proposes a least-squares residual minimization approach for reduced-order models of dynamical systems. The proposed approach, which comprises a general least-squares reduced-order modeling (LSROM) formulation for dynamical systems, sequentially minimizes the time-continuous full-order model residual within a low-dimensional trial space over a series of time slabs. The stationary conditions for the time-continuous minimization problems are obtained 
%by deriving the associated Euler--Lagrange equations. The time-continuous formulation exposes two differing methods that can be used to solve the minimization problem, namely direct (discretize then minimize) and indirect (minimize and then discretize). The proposed approach displays commonalities with optimal control problems and is a generalization of the popular 
%Least-Squares Petrov--Galerkin (LSPG) method. By formulating the residual minimization problem from the time-continuous level, the \methodAcronym\ approach overcomes 
%the time-step sensitivity and time-scheme dependence that LSPG is subject to. 
%The stationary points of the minimization problems are obtained by solving the associated Euler-Lagrange equations. The Euler-Lagrange equations consist of a set of coupled two point boundary value problems defined on each time-slab containing a forward primal system and a backward adjoint system. %The approach is conceptually similar to the classic Least-Squares Petrov-Galerkin approach, but displays several advantages. First, by formulating the residual minimization problem at the continuous level, the sensitivity of the resulting method to the underlying time-discretization scheme is greatly reduced. 
%Numerical experiments on the compressible Euler equations demonstrate that the proposed approach leads to more accurate and physically relevant solutions than existing model reduction approaches. 
%===INTRO

%~\cite{legresley_1,legresley_2,legresley_3,bui_resmin_steady,rovas_thesis,carlberg_thesis,bui_thesis} and then extended to dynamical systems in Refs~\cite{bui_thesis,bui_unsteady,carlberg_thesis,carlberg_gnat,carlberg_lspg,carlberg_lspg_v_galerkin}. Specifically, Refs~\cite{bui_thesis,bui_unsteady,carlberg_thesis,carlberg_gnat,carlberg_lspg}

%\begin{itemize}
%\item Galerkin is inaccurate/unstable for non-symmetric static systems -- motivates residual minimization
%\item Residual minimization (stems from FEM): minimizes the residual of the full-order model over the range of the trial space -- originally developed for static systems
%\item Can guarantee stability for linear static systems 
%\item Extension to time-varying systems is challenging 
%\begin{itemize}
%\item What is the definition of the ODE residual?
%\item Over what time-interval is the residual being minmized?
%\end{itemize}
%\end{itemize} 


%\KTC{Why are we reviewing LSFEM? This isn't a ROM. Don't think it's relevant}
%Least-squares residual minimization as a numerical solution technique for
%partial differential equations was pioneered in the finite-element community
%and led to what is known as the least-squares finite-element method
%(LSFEM)~\cite{ls_review,bochev_leastsquares}. 
%\KTC{Specifically, LSFEM was designed to solve the inf--sup stability problem.
%Be more precise.}
%Motivated by the fact that the
%Galerkin method can be inaccurate and unstable for nonlinear and
%non-self-adjoint systems, the LSFEM involves (1.) forming a quadratic
%functional comprising the residual norm of the partial differential equation
%%of interest and (2.) leveraging variational principles to derive the
%stationary conditions of the quadratic functional; i.e, LSFEM can be thought
%of as finding the solution within a finite element trial space the minimizes
%the $2$-norm of the PDE residual. Least-squares methods often exhibit numerous
%advantages (e.g., least-squares system are symmetric and positive definite)
%\KTC{This is not an `advantage'. It is guaranteed to be inf--sup stable.}
%over the more standard Galerkin discretization. 
%The accuracy of LSPG additionally degrades if the time-step size becomes too large. It is unclear if this is due to the discretization error associated with enlarging the time-step size, or rather if it is due to the size of the window the residual is being minimized over. LSPG projection additionally is
%additionally sensitive to the time discretization scheme. When used with
%explicit temporal discretization, for instance, LSPG again reverts to
%Galerkin. To gain the enhanced stability associated with LSPG, the underlying
%time-discretization scheme must be implicit with an appropriate time-step.
%%This can increase the cost of the reduced-order model, especially in the case
%where the optimal time-step required by LSPG is small. 

%this by minimizing the full-order-model residual over the entire
%time domain, sothe resulting model does not because these methods minimize the full-order-model residual over the entire
%time domain, they are intractable without hyper-reduction
%\cite{choi_stlspg,constantine_strom}. Second, see (1) Several drawbacks of existing
%space--time ROMs are that they (1) require minimizing the entire space--time
%residual (which can be a computationally intensive process) and (2) require
%the specification of temporal basis (computed, e.g., via proper orthogonal
%decomposition applied to space--time snapshots). This latter issue precludes
%space--time projection methods from being used for, e.g., future state
%prediction. 

%the trial basis comprises space--time \textit{functions} that map a low-dimensional vector of generalized coordinates to the space--time solution. In spatial projection, the trial space comprises space--time functions that map a low-dimensional vector of 
%These space--time In these space--time ROMs, the temporal dependence of the state 
%is represented via a finite dimensional basis representation. 
%Space--time residual minimization methods can be prohibitively expensive, however, as they involve minimizing the entire space--time residual. Further, existing space--time approaches require a finite-dimensional space--time basis. 

%This work seeks to overcome the practical challenges associated with existing
%residual minimization approaches for reduced-order models. To this end, we
%propose the \methodName\ method for model reduction, which minimizes the FOM ODE residual
%over a sequence of time windows. The key novelties of the work are two fold.
%First, the residual minimization problem is formulated to minimize the
%\textit{time-continuous} residual. By working from a time-continuous viewpoint, the
%present formulation decouples the underlying temporal discretization scheme
%from the residual minimization problem; thus addressing a key deficiency of
%LSPG \KTC{This is also true for ST-LSPG and maybe other discretization-based
%methods? Have to check}. The time-continuous viewpoint additionally exposes
%two different methods that
%can be used to solve the residual-minimization problems, namely
%a \textit{discretize-then-optimize} (i.e., direct) method and
%an \textit{optimize-then-discretize} (i.e., indirect) method. The second novelty
%is that the residual is minimized over \textit{time windows} rather than over
%a single time step or the entire time domain. This equips the method with
%additionaly flexibility that enables it
%to explore more fine-grained tradeoffs in computational cost and error. We
%demonstrate that this method can be viewed as a generalization of existing
%methods, as particular instances of this formulation recover Galerkin, LSPG,
%and space--time LSPG projection. Figure~\ref{fig:flowchart} depicts how the
%proposed method is related to existing approaches. 


% \KTC{This is what ST-LSPG does. Why do we ignore it? the
%full dynamical system} Indeed, LSPG is not equipped with \textit{a priori}
%guarantees of stability or convergence, even for linear time-invariant
%systems~\cite{bui_thesis}. Further, the performance of LSPG depends on the
%time step in two distinct ways: changing the time-step size modifies both the
%time window over which LSPG minimizes the residual \textit{as well as} the
%time-discretization error of the full-order model on which LSPG is based. As a
%result, LSPG often yields the smallest error for an intermediate value of the
%time step (see, e.g., Ref.~\cite[Figure 9]{carlberg_lspg_v_galerkin}). In
%particular, as the time step decreases, the accuracy of LSPG approaches that
%of Galerkin (which is often poor)\footnote{This occurs because LSPG projection
%is equivalent to Galerkin projection in the limit $\Delta t \rightarrow 0$.};
%as the time step increases, the time discretization error of the full-order
%model (on which the LSPG ROM is based) increases.  LSPG projection additionally is
%additionally sensitive to the time discretization scheme. When used with
%explicit temporal discretization, for instance, LSPG again reverts to
%Galerkin. To gain the enhanced stability associated with LSPG, the underlying
%time-discretization scheme must be implicit with an appropriate time-step.
%This can increase the cost of the reduced-order model, especially in the case
%where the optimal time-step required by LSPG is small. 

 
%The thesis of least--squares residual minimization was extended to construct reduced-order models of static systems by several authors, with two such instances being~\cite{rovas_thesis} and~\cite{bui_resmin}. Similar to the finite-element case, numerous theoretical analyses and numerical experiments have demonstrated that residual minimization methods yield more accurate and stable solutions than the Galerkin method. The extension of least--squares residual minimization methods to dynamical systems, however, has proved challenging. The least-squares Petrov-Galerkin method is arguably the most commonly adapted residual minimization method for dynamical systems. LSPG extends the idea of least--squares residual minimization to dynamical systems by sequentially minimizing the \textit{fully discrete} residual (i.e., after temporal discretization) in a weighted norm (nominally $\elltwo$) over each time step. Numerous numerical experiments have demonstrated that the LSPG approach yields more stable (although not always more accurate) solutions than the Galerkin method~\cite{carlberg_lspg_v_galerkin,carlberg_gnat,carlberg_thesis,parish_apg}. The common explanation for this improved performance is that, by minimizing the fully discrete residual, LSPG minimizes the residual over a \textit{finite-sized time-window}, rather than instantaneously as is done in the Galerkin method. 

%The LSPG approach, however, is still subject to exponentially 
%growing error bounds and, in general, lacks \textit{a priori} guarantees of stability and accuracy. Further, as it is defined at the fully discrete level, LSPG contains several nuances. First, the LSPG approach is sensitive to the time-step. In Ref.~\cite{carlberg_lspg_v_galerkin}, LSPG was shown to provide optimal results at an intermediary time-step. For too small a time-step, the LSPG approach is prone to the same stability challenges as the Galerkin approach. In the limiting case of $\Delta t \rightarrow 0$, the LSPG approach reverts to a Galerkin approach. For too-large a time-step, on the other hand, the accuracy of the LSPG method degrades. It is unclear if this is due to the numerical error associated with enlarging the time-step size, or rather if it is due to the size of the window the residual is being minimized over. The LSPG approach is additionally sensitive to the time discretization scheme. When used with explicit temporal discretization, for instance, the LSPG approach reverts to a Galerkin approach. To gain the enhanced stability associated with the LSPG method, the underlying time-discretization scheme must be implicit with an appropriate time-step. This can increase the cost of the reduced-order model, especially in the case where the optimal time-step required by LSPG is small. 

 
%Residual minimization methods are the most relevant to the current work. Residual minimization methods seek to minimize the residual of the full-order model over the trial space defined by the POD basis. Two of the most popular reduced-order modeling approaches are (or can be interpreted as) residual minimization approaches. The Galerkin method, which is arguably the most common reduced-order modeling approach, minimizes the instantaneous FOM residual over the range of the trial basis~\cite{carlberg_lspg_v_galerkin}. %The Galerkin method is arguably the most common reduced-order modeling approach. In Ref~\cite{carlberg_lspg_v_galerkin}, Carlberg and Barone demonstrated that the Galerkin method corresponds to finding a velocity that minimizies the instananeous FOM residual over the range of the trial basis. 
%Numerical experiments and theoretical analyses have shown, however, that the Galerkin method can be inappropriate for model-order reduction of general nonlinear dynamical systems~\cite{carlberg_gnat,carlberg_thesis,huang_combustion_roms}; error analysis has demonstrated that the Galerkin method is subject to exponentially growing error bounds while a variety of numerical experiments have demonstrated the lack of stability and inaccuracy of the method in practice. Indeed, it is well known in numerical analysis that Galerkin method performs poorly for non-symmetric systems. To address these issues, Carlberg et al. developed the least-squares Petrov-Galerkin (LSPG) method. LSPG sequentially minimizes the \textit{fully discrete} residual (i.e., after temporal discretization) in a weighted norm over each time-step. In this sense, the LSPG method is typically referred to as being \textit{discretely-optimal}. Numerous numerical experiments have demonstrated that the LSPG approach yields more stable (although not always more accurate) solutions than the Galerkin method~\cite{carlberg_lspg_v_galerkin,carlberg_gnat,carlberg_thesis,parish_apg}. The common explanation for this improved performance is that, by minimizing the fully discrete residual, LSPG minimizes the residual over a \textit{finite-sized time-window}, rather than instantaneously as is done in the Galerkin method. The LSPG approach, however, is still subject to exponentially 
%growing error bounds and, in general, lacks \textit{a priori} guarantees of stability and accuracy. Further, as it is defined at the fully discrete level, LSPG contains several nuances. First, the LSPG approach is sensitive to the time-step. In Ref.~\cite{carlberg_lspg_v_galerkin}, LSPG was shown to provide optimal results at an intermediary time-step. For too small a time-step, the LSPG approach is prone to the same stability challenges as the Galerkin approach. In the limiting case of $\Delta t \rightarrow 0$, the LSPG approach reverts to a Galerkin approach. For too-large a time-step, on the other hand, the accuracy of the LSPG method degrades. It is unclear if this is due to the numerical error associated with enlarging the time-step size, or rather if it is due to the size of the window the residual is being minimized over. The LSPG approach is additionally sensitive to the time discretization scheme. When used with explicit temporal discretization, for instance, the LSPG approach reverts to a Galerkin approach. To gain the enhanced stability associated with the LSPG method, the underlying time-discretization scheme must be implicit with an appropriate time-step. This can increase the cost of the reduced-order model, especially in the case where the optimal time-step required by LSPG is small. 

%The nuances associated with the LSPG approach lead to practical challenges. For instance, minimal analysis on the optimal \textit{a priori} time-step exists, and as such specifying an appropriate time-step for LSPG \textit{a priori} is challenging. The limitation to implicit time-schemes is also restrictive, especially considering the fact that ROMs are often much less stiff than the corresponding FOM~\cite{Bach2018StabilityCF} and thus explicit time-stepping schemes with large time-steps can sometimes be used. It is emphasized that, despite these challenges, LSPG is arguably the most robust model-order reduction 
%formulation for general nonlinear dynamical systems and has gained wide-spread use~\cite{narmal,cheng,4,5,6}.\EP{Finish with good Refs using LSPG} 
%Time-discrete space-time residual minimization~\cite{choi_stlspg,constantine_strom} is the third approach that has been proposed. Reference~\cite{choi_stlspg} extends the LSPG method to space-time formulations and develops the ST-LSPG method. %ST-LPSG seeks a space-time solution within a space-time trial space that is a minimizer of the fully discrete residual. The space-time trial spaces
%ST-LSPG seeks to find the minimizer of the \textit{fully discrete} residual over the entire space-time domain within a given space-time trial space. These space-time trial spaces are constructed through higher-order singular value decompositions (SVD), such as the truncated high-order SVD.
%\item How can a finite-time residual minimization approach be developed that is independent of the underlying temporal discretization?
%\item Does minimizing the residual over a larger time-window improve the model prediction?
%\end{enumerate}

%Recently, LSROMs for dynamical systems have been extended to the space--time setting~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC}. These space--time LSROMs (ST-LSROMs), which are typically motivated from the need to reduce the spatial \textit{and} temporal dimension of a dynamical system model, comprise finding a space--time solution that minimizes the space--time residual within a low-dimensional space--time trial space. In contrast to standard ROMs for dynamical systems, in where the generalized coordinates are \textit{(discretized) time-dependent functions}, space--time methods ROMs represent the temporal dependence of the state via a finite dimensional temporal basis and thus the generalized coordinates are a stationary vector. ST-LSROMs thus differ from their standard LSROM counterparts in that they 1.) minimize the full space--time residual and 2.) represent the temporal dependence of the state via a prescribed trial space. ST-LSROMs thus comprise algebraic systems, while LSROMs comprise dynamical systems.  
 
%ST-LSROMs involve a monolithic space--time least--squares solve, and are typically equipped with more robust error bounds than their non-space--time counterparts. To best describe how these existing methods fit into the present work, they must be viewed in a two dimensional space: one dimension corresponding the space--time trial space and one dimension corresponding to the space--time residual minimization problem.  With the exception of Ref.~\cite{bui_thesis}, existing space--time methods leverage reduced spatial and temporal trial spaces. In Ref.~\ref{choi_stlspg}, these trial spaces are constructed through higher-order singular value decompositions (SVDs), such as the truncated high-order SVD. 
%Ref.~\cite{choi_stlspg} develops a space--time formulation of the least-squares Petrov-Galerkin approach. The formulation minimizes the \textit{fully discrete} residual over the entire space--time domain within a given space--time trial space. These space--time trial spaces are constructed through higher-order singular value decompositions (SVDs), such as the truncated high-order SVD. 
%Another piece of relevant work is that performed by Constantine and Wang~\cite{constantine_strom}, in which a space--time residual minimization formulation for model interpolation is pursued. The present work again distinguishes itself in that it seeks to minimize the time-continuous residual over space--time slabs;~\cite{constantine_strom} considers the solution of a fully discrete optimization problem over the entire space--time domain. It is noted that, similar to the current work, a time-continuous residual-minimization formulation is used as a starting point in~\cite{constantine_strom}, and as such thematic similarities exist between~\cite{constantine_strom} and the present work.
%Ref.~\cite{choi_stlspg} develops a space--time formulation of the least-squares Petrov-Galerkin approach. The formulation minimizes the \textit{fully discrete} residual over the entire space--time domain within a given space--time trial space. These space--time trial spaces are constructed through higher-order singular value decompositions (SVDs), such as the truncated high-order SVD. The present work distinguishes itself from Ref.~\cite{choi_stlspg} in that it seeks to minimize the time-continuous residual over time slabs, as opposed to the fully-discrete residual over the entire domain. Another point of departure from Ref.~\cite{choi_stlspg} is that the present work does not consider temporal reduction; this will be a subject of future work. Instead, the temporal basis is taken to have full support. Another piece of relevant work is that performed by Constantine and Wang~\cite{constantine_strom}, in which a space--time residual minimization formulation for model interpolation is pursued. The present work again distinguishes itself in that it seeks to minimize the time-continuous residual over space--time slabs;~\cite{constantine_strom} considers the solution of a fully discrete optimization problem over the entire space--time domain. It is noted that, similar to the current work, a time-continuous residual-minimization formulation is used as a starting point in~\cite{constantine_strom}, and as such thematic similarities exist between~\cite{constantine_strom} and the present work.
